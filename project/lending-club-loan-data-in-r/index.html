<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.27.1" />
  <meta name="author" content="Alexander Wagner">
  <meta name="description" content="Data Enthusiast">

  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="/css/highlight.min.css">
    
  
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <link rel="stylesheet" href="/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/academicons.min.css">
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700|Merriweather|Roboto+Mono">
  <link rel="stylesheet" href="/css/hugo-academic.css">
  

  <link rel="alternate" href="https://triamus.github.io/index.xml" type="application/rss+xml" title="suspiciously datalicious">
  <link rel="feed" href="https://triamus.github.io/index.xml" type="application/rss+xml" title="suspiciously datalicious">

  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/apple-touch-icon.png">

  <link rel="canonical" href="https://triamus.github.io/project/lending-club-loan-data-in-r/">

  

  <title>Lending Club Loan Data in R | suspiciously datalicious</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">suspiciously datalicious</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      <ul class="nav navbar-nav navbar-right">
        

        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
          </a>
        </li>

        
        
      </ul>

    </div>
  </div>
</nav>


<article class="article article-project" itemscope itemtype="http://schema.org/Article">

  

  <div class="article-container">

    <div class="pub-title">
      <h1 itemprop="name">Lending Club Loan Data in R</h1>
      <span class="pub-authors" itemprop="author">&nbsp;</span>
      <span class="pull-right">
        
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2ftriamus.github.io%2fproject%2flending-club-loan-data-in-r%2f"
         target="_blank">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Lending%20Club%20Loan%20Data%20in%20R&amp;url=https%3a%2f%2ftriamus.github.io%2fproject%2flending-club-loan-data-in-r%2f"
         target="_blank">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftriamus.github.io%2fproject%2flending-club-loan-data-in-r%2f&amp;title=Lending%20Club%20Loan%20Data%20in%20R"
         target="_blank">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2ftriamus.github.io%2fproject%2flending-club-loan-data-in-r%2f&amp;title=Lending%20Club%20Loan%20Data%20in%20R"
         target="_blank">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Lending%20Club%20Loan%20Data%20in%20R&amp;body=https%3a%2f%2ftriamus.github.io%2fproject%2flending-club-loan-data-in-r%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


      </span>
    </div>

    

    <div class="article-style" itemprop="articleBody">
      <div id="TOC">
<ul>
<li><a href="#overview"><span class="toc-section-number">1</span> Overview</a><ul>
<li><a href="#kaggle-data-description"><span class="toc-section-number">1.1</span> Kaggle Data Description</a></li>
</ul></li>
<li><a href="#environment-setup"><span class="toc-section-number">2</span> Environment Setup</a><ul>
<li><a href="#libraries"><span class="toc-section-number">2.1</span> Libraries</a></li>
<li><a href="#session-info"><span class="toc-section-number">2.2</span> Session Info</a></li>
</ul></li>
<li><a href="#a-note-on-libraries"><span class="toc-section-number">3</span> A note on libraries</a><ul>
<li><a href="#library-versions"><span class="toc-section-number">3.1</span> Library versions</a></li>
</ul></li>
<li><a href="#data-import"><span class="toc-section-number">4</span> Data Import</a></li>
<li><a href="#meta-data"><span class="toc-section-number">5</span> Meta Data</a><ul>
<li><a href="#data-transformation"><span class="toc-section-number">5.1</span> Data Transformation</a></li>
<li><a href="#additional-meta-data-file"><span class="toc-section-number">5.2</span> Additional Meta Data File</a></li>
</ul></li>
<li><a href="#defining-default"><span class="toc-section-number">6</span> Defining default</a></li>
<li><a href="#removing-variables-deemed-unfit-for-most-modeling"><span class="toc-section-number">7</span> Removing variables deemed unfit for most modeling</a></li>
<li><a href="#a-note-on-hypothesis-generation-vs.hypothesis-confirmation"><span class="toc-section-number">8</span> A note on hypothesis generation vs.Â hypothesis confirmation</a></li>
<li><a href="#exploratory-data-analysis"><span class="toc-section-number">9</span> Exploratory Data Analysis</a><ul>
<li><a href="#grade"><span class="toc-section-number">9.1</span> Grade</a></li>
<li><a href="#home-ownership"><span class="toc-section-number">9.2</span> Home Ownership</a></li>
<li><a href="#loan-amount-and-income"><span class="toc-section-number">9.3</span> Loan Amount and Income</a></li>
<li><a href="#time-series"><span class="toc-section-number">9.4</span> Time Series</a></li>
<li><a href="#geolocation-plots"><span class="toc-section-number">9.5</span> Geolocation Plots</a></li>
</ul></li>
<li><a href="#correlation"><span class="toc-section-number">10</span> Correlation</a></li>
<li><a href="#summary-plots"><span class="toc-section-number">11</span> Summary plots</a></li>
<li><a href="#modeling"><span class="toc-section-number">12</span> Modeling</a><ul>
<li><a href="#model-options"><span class="toc-section-number">12.1</span> Model options</a></li>
<li><a href="#imbalanced-data"><span class="toc-section-number">12.2</span> Imbalanced data</a></li>
<li><a href="#a-note-on-modeling-libraries"><span class="toc-section-number">12.3</span> A note on modeling libraries</a><ul>
<li><a href="#a-note-on-caret-library-in-particular"><span class="toc-section-number">12.3.1</span> A note on caret library in particular</a></li>
</ul></li>
<li><a href="#data-preparation-for-modeling"><span class="toc-section-number">12.4</span> Data preparation for modeling</a><ul>
<li><a href="#proper-names-for-character-variables"><span class="toc-section-number">12.4.1</span> Proper names for character variables</a></li>
<li><a href="#dummy-variables"><span class="toc-section-number">12.4.2</span> Dummy variables</a></li>
</ul></li>
<li><a href="#logistic-regression"><span class="toc-section-number">12.5</span> Logistic regression</a></li>
<li><a href="#model-evaluation-illustration"><span class="toc-section-number">12.6</span> Model evaluation illustration</a><ul>
<li><a href="#roc-curve"><span class="toc-section-number">12.6.1</span> ROC curve</a></li>
</ul></li>
<li><a href="#variable-selection"><span class="toc-section-number">12.7</span> Variable selection</a></li>
<li><a href="#a-more-complex-logistic-model-with-caret"><span class="toc-section-number">12.8</span> A more complex logistic model with caret</a><ul>
<li><a href="#traincontrol"><span class="toc-section-number">12.8.1</span> trainControl</a></li>
<li><a href="#train"><span class="toc-section-number">12.8.2</span> train</a></li>
</ul></li>
<li><a href="#a-note-on-computational-efficiency-parallelization-via-cluster"><span class="toc-section-number">12.9</span> A note on computational efficiency (parallelization via cluster)</a></li>
<li><a href="#a-note-on-tuning-parameters-in-caret"><span class="toc-section-number">12.10</span> A note on tuning parameters in caret</a></li>
<li><a href="#trees"><span class="toc-section-number">12.11</span> Trees</a><ul>
<li><a href="#simple-tree-using-cart-via-rpart-in-caret"><span class="toc-section-number">12.11.1</span> Simple tree using CART via rpart in caret</a></li>
<li><a href="#random-forest-via-randomforest-in-caret"><span class="toc-section-number">12.11.2</span> Random forest via randomForest in caret</a></li>
<li><a href="#stochastic-gradient-boosting-via-gbm-in-caret"><span class="toc-section-number">12.11.3</span> Stochastic Gradient Boosting via gbm in caret</a></li>
</ul></li>
</ul></li>
<li><a href="#further-resources"><span class="toc-section-number">13</span> Further Resources</a></li>
</ul>
</div>

<p></p>
<div id="overview" class="section level1">
<h1><span class="header-section-number">1</span> Overview</h1>
<p>A case study of machine learning / modeling in R with credit default data. Data is taken from <a href="https://www.kaggle.com/wendykan/lending-club-loan-data">Kaggle Lending Club Loan Data</a> but is also available publicly at <a href="https://www.lendingclub.com/info/download-data.action">Lending Club Statistics Page</a>. We illustrate the complete workflow from data ingestion, over data wrangling/transformation to exploratory data analysis and finally modeling approaches. Along the way will be helpful discussions on side topics such as training strategy, computational efficiency, R intricacies, and more. Note that the focus is more on methods in R rather than statistical rigour. This is meant to be a reference for an end-to-end data science workflow rather than a serious attempt to achieve best model performance.</p>
<div id="kaggle-data-description" class="section level2">
<h2><span class="header-section-number">1.1</span> Kaggle Data Description</h2>
<p>The files contain complete loan data for all loans issued through 2007-2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file containing loan data through the âpresentâ contains complete loan data for all loans issued through the previous completed calendar quarter. Additional features include credit scores, number of finance inquiries, address including zip codes, and state, and collections among others. The file is a matrix of about 890 thousand observations and 75 variables. A data dictionary is provided in a separate file.</p>
<p></p>
</div>
</div>
<div id="environment-setup" class="section level1">
<h1><span class="header-section-number">2</span> Environment Setup</h1>
<div id="libraries" class="section level2">
<h2><span class="header-section-number">2.1</span> Libraries</h2>
<p>This will automatically install any missing libraries. Make sure to have proper proxy settings. You do not necessarily need to have all of them but only those needed for respective sections (which will be indicated).</p>
<pre class="r"><code># define used libraries
libraries_used &lt;- 
  c(&quot;lazyeval&quot;, &quot;readr&quot;,&quot;plyr&quot; ,&quot;dplyr&quot;, &quot;readxl&quot;, &quot;ggplot2&quot;, 
    &quot;funModeling&quot;, &quot;scales&quot;, &quot;tidyverse&quot;, &quot;corrplot&quot;, &quot;GGally&quot;, &quot;caret&quot;,
    &quot;rpart&quot;, &quot;randomForest&quot;, &quot;pROC&quot;, &quot;gbm&quot;, &quot;choroplethr&quot;, &quot;choroplethrMaps&quot;,
    &quot;microbenchmark&quot;, &quot;doParallel&quot;, &quot;e1071&quot;)

# check missing libraries
libraries_missing &lt;- 
  libraries_used[!(libraries_used %in% installed.packages()[,&quot;Package&quot;])]
# install missing libraries
if(length(libraries_missing)) install.packages(libraries_missing)</code></pre>
</div>
<div id="session-info" class="section level2">
<h2><span class="header-section-number">2.2</span> Session Info</h2>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.4.1 (2017-06-30)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 15063)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.1252 
## [2] LC_CTYPE=English_United States.1252   
## [3] LC_MONETARY=English_United States.1252
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.1252    
## 
## attached base packages:
## [1] methods   stats     graphics  grDevices utils     datasets  base     
## 
## loaded via a namespace (and not attached):
##  [1] compiler_3.4.1  backports_1.1.0 bookdown_0.5    magrittr_1.5   
##  [5] rprojroot_1.2   tools_3.4.1     htmltools_0.3.6 yaml_2.1.14    
##  [9] Rcpp_0.12.12    stringi_1.1.5   rmarkdown_1.6   blogdown_0.1   
## [13] knitr_1.17      stringr_1.2.0   digest_0.6.12   evaluate_0.10.1</code></pre>
<p></p>
</div>
</div>
<div id="a-note-on-libraries" class="section level1">
<h1><span class="header-section-number">3</span> A note on libraries</h1>
<p>There are generally two ways to access functions from R packages (we use package and library interchangeably). Either via a direct access without loading the library, i.e. <code>package::function()</code> or by loading (attaching) the library into workspace (environment) and thus making all its functions available at once. The problem with the first option is that direct access sometimes can lead to issues (unexpected behavior/errors) and makes coding cumbersome. The problem with the second approach is a conflict of function (and other variables) names between two libraries (called masking). A third complication is that some functions require libraries to be loaded (e.g.Â some instances of <code>caret::train()</code>) and thus attach the library without being explicitly told. Again this may lead to masking of functions and if user is unaware this can lead to nasty problems (conceived bugs) down the road (sometimes libraries will issue a warning upon load, e.g. <code>plyr</code> when <code>dplyr</code> is already loaded). There is no golden way (at least we are not aware of it) and so we tend to do the following</p>
<ul>
<li>load major libraries that are used frequently into workspace but pay attention to load succession to avoid unwanted masking</li>
<li>access rarely used functions directly (being aware that they work and donât attach anything themselves)</li>
<li>sometimes use direct access to a fucntion although its library is loaded (either to make clear which library is currently used or because we explicitly need a function that has been masked due to another loaded library)</li>
</ul>
<p>We will load a few essential libraries here as they are used heavily. Other librraies may only be loaded in respective section. You can also try to follow the analysis without loading many librraies at the beginning but only when needed. Just be aware of the succession to avoid issues.</p>
<div id="library-versions" class="section level2">
<h2><span class="header-section-number">3.1</span> Library versions</h2>
<p>Also note that often the version of the library used matters (it always matters but what we mean is, it makes a difference) and some libraries are developed more actively than others which may lead to issues. For example, note that library <code>caret</code> is using <code>plyr</code> while the <code>tidyverse</code> of which <code>dplyr</code> (successor of <code>plyr</code>) is part has some new concepts, e.g.Â the default data frame is a <code>tibble::tibble()</code>. It seems that <code>caret</code> has issues with <code>tibbles</code>, see e.g. <a href="https://stackoverflow.com/questions/43018879/wrong-model-type-for-classification-in-regression-problems-in-r-caret">âWrong model type for classificationâ in regression problems in R-Caret</a>.</p>
<pre class="r"><code>library(plyr)
library(tidyverse)</code></pre>
<pre><code>## Loading tidyverse: ggplot2
## Loading tidyverse: tibble
## Loading tidyverse: tidyr
## Loading tidyverse: readr
## Loading tidyverse: purrr
## Loading tidyverse: dplyr</code></pre>
<pre><code>## Conflicts with tidy packages ----------------------------------------------</code></pre>
<pre><code>## arrange():   dplyr, plyr
## compact():   purrr, plyr
## count():     dplyr, plyr
## failwith():  dplyr, plyr
## filter():    dplyr, stats
## id():        dplyr, plyr
## lag():       dplyr, stats
## mutate():    dplyr, plyr
## rename():    dplyr, plyr
## summarise(): dplyr, plyr
## summarize(): dplyr, plyr</code></pre>
<pre class="r"><code>library(caret)</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## 
## Attaching package: &#39;caret&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     lift</code></pre>
<p></p>
</div>
</div>
<div id="data-import" class="section level1">
<h1><span class="header-section-number">4</span> Data Import</h1>
<p>Refer to section <a href="#overview">Overview</a> to see how the data may be acquired.</p>
<p>The loans data can be conveniently read via <code>readr::read_csv()</code>. Note that the function tries to determine the variable type by reading the first 1,000 rows which does not always guarantee correct import especially in cases of missing values at the begining of the file. Other packages treat imports differently, for example <code>data.table::fread()</code> takes a sample of 1,000 rows to determine column types (100 rows from 10 different points) which seems to get more robust results than <code>readr::read_csv()</code>. Both package functions allow the explicit definition of column classes, for <code>readr::read_csv()</code> it is parameter <code>col_types</code> and for <code>data.table::fread()</code> it is parameter <code>colClasses</code>. Alternatively, <code>readr::read_csv()</code> offers the parameter <code>guess_max</code> that allows increasing the number of rows being guessed similar to SAS import procedure parameter <code>guessingrows</code>. Naturally, import time increases if more rows are guessed. For details on <code>readr</code> including comparisons against <code>Base R</code> and <code>data.table::fread()</code> see <a href="https://cran.r-project.org/web/packages/readr/README.html">readr</a>.</p>
<pre class="r"><code>path &lt;- &quot;D:/data/lending_club&quot;
loans &lt;- readr::read_csv(paste0(path, &quot;./loan.csv&quot;))</code></pre>
<p>The meta data comes in an Excel file and needs to be parsed via a special library, in this case we use <code>readxl</code>.</p>
<pre class="r"><code>library(readxl)
# Load the Excel workbook
excel_file = paste0(&quot;./LendingClubDataDictionary.xlsx&quot;)
# see available tabs
excel_sheets(paste0(path, excel_file))</code></pre>
<pre><code>## [1] &quot;LoanStats&quot;   &quot;browseNotes&quot; &quot;RejectStats&quot;</code></pre>
<pre class="r"><code># Read in the first worksheet
meta_loan_stats = read_excel(paste0(path, excel_file), sheet = &quot;LoanStats&quot;)
meta_browse_notes = read_excel(paste0(path, excel_file), sheet = &quot;browseNotes&quot;)
meta_reject_stats = read_excel(paste0(path, excel_file), sheet = &quot;RejectStats&quot;)</code></pre>
<p></p>
</div>
<div id="meta-data" class="section level1">
<h1><span class="header-section-number">5</span> Meta Data</h1>
<p>Letâs have a look at the meta data in more detail. First, which variables are present in loan data set and what is their type. Then check meta data information and finally compare the two to see if anything is missing.</p>
<p>The usual approach may be to use <code>base::str()</code> function to get a summary of the data structure. However, it may be useful to quantify the âinformation powerâ of different metrics and dimensions by looking at the ratio of zeros and missing values to overall observations. This will not always reveal the truth (as there may be variables that are only populated if certain conditions apply) but it still gives some indication. The <a href="https://cran.r-project.org/web/packages/funModeling/index.html">funModeling</a> package offers the function <code>funModeling::df_status()</code> for that. It does not scale very well and has quite a few dependencies (so a direct call is preferred over a full library load) but it suits the purpose for this data. Unfortunately, it does not return the number of rows and columns. The data has 887379 observations (rows) and 74 variables (columns).</p>
<p>We will require the meta data at a later stage so we assign it to a variable. The function <code>funModeling::df_status()</code> has parameter <code>print_results = TRUE</code> set by default which means the data will be assigned and printed at the same time.</p>
<pre class="r"><code>meta_loans &lt;- funModeling::df_status(loans, print_results = FALSE)
knitr::kable(meta_loans)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">variable</th>
<th align="right">q_zeros</th>
<th align="right">p_zeros</th>
<th align="right">q_na</th>
<th align="right">p_na</th>
<th align="right">q_inf</th>
<th align="right">p_inf</th>
<th align="left">type</th>
<th align="right">unique</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">id</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">integer</td>
<td align="right">887379</td>
</tr>
<tr class="even">
<td align="left">member_id</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">integer</td>
<td align="right">887379</td>
</tr>
<tr class="odd">
<td align="left">loan_amnt</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">1372</td>
</tr>
<tr class="even">
<td align="left">funded_amnt</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">1372</td>
</tr>
<tr class="odd">
<td align="left">funded_amnt_inv</td>
<td align="right">233</td>
<td align="right">0.03</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">9856</td>
</tr>
<tr class="even">
<td align="left">term</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="left">int_rate</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">542</td>
</tr>
<tr class="even">
<td align="left">installment</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">68711</td>
</tr>
<tr class="odd">
<td align="left">grade</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">7</td>
</tr>
<tr class="even">
<td align="left">sub_grade</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">35</td>
</tr>
<tr class="odd">
<td align="left">emp_title</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">51457</td>
<td align="right">5.80</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">289148</td>
</tr>
<tr class="even">
<td align="left">emp_length</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">12</td>
</tr>
<tr class="odd">
<td align="left">home_ownership</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">6</td>
</tr>
<tr class="even">
<td align="left">annual_inc</td>
<td align="right">2</td>
<td align="right">0.00</td>
<td align="right">4</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">49384</td>
</tr>
<tr class="odd">
<td align="left">verification_status</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">3</td>
</tr>
<tr class="even">
<td align="left">issue_d</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">103</td>
</tr>
<tr class="odd">
<td align="left">loan_status</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">10</td>
</tr>
<tr class="even">
<td align="left">pymnt_plan</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="left">url</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">887379</td>
</tr>
<tr class="even">
<td align="left">desc</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">761597</td>
<td align="right">85.83</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">124453</td>
</tr>
<tr class="odd">
<td align="left">purpose</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">14</td>
</tr>
<tr class="even">
<td align="left">title</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">151</td>
<td align="right">0.02</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">61452</td>
</tr>
<tr class="odd">
<td align="left">zip_code</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">935</td>
</tr>
<tr class="even">
<td align="left">addr_state</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">51</td>
</tr>
<tr class="odd">
<td align="left">dti</td>
<td align="right">451</td>
<td align="right">0.05</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">4086</td>
</tr>
<tr class="even">
<td align="left">delinq_2yrs</td>
<td align="right">716961</td>
<td align="right">80.80</td>
<td align="right">29</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">29</td>
</tr>
<tr class="odd">
<td align="left">earliest_cr_line</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">29</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">697</td>
</tr>
<tr class="even">
<td align="left">inq_last_6mths</td>
<td align="right">497905</td>
<td align="right">56.11</td>
<td align="right">29</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">28</td>
</tr>
<tr class="odd">
<td align="left">mths_since_last_delinq</td>
<td align="right">1723</td>
<td align="right">0.19</td>
<td align="right">454312</td>
<td align="right">51.20</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">155</td>
</tr>
<tr class="even">
<td align="left">mths_since_last_record</td>
<td align="right">1283</td>
<td align="right">0.14</td>
<td align="right">750326</td>
<td align="right">84.56</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">123</td>
</tr>
<tr class="odd">
<td align="left">open_acc</td>
<td align="right">7</td>
<td align="right">0.00</td>
<td align="right">29</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">77</td>
</tr>
<tr class="even">
<td align="left">pub_rec</td>
<td align="right">751572</td>
<td align="right">84.70</td>
<td align="right">29</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">32</td>
</tr>
<tr class="odd">
<td align="left">revol_bal</td>
<td align="right">3402</td>
<td align="right">0.38</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">73740</td>
</tr>
<tr class="even">
<td align="left">revol_util</td>
<td align="right">3540</td>
<td align="right">0.40</td>
<td align="right">502</td>
<td align="right">0.06</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">1356</td>
</tr>
<tr class="odd">
<td align="left">total_acc</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">29</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">135</td>
</tr>
<tr class="even">
<td align="left">initial_list_status</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="left">out_prncp</td>
<td align="right">255798</td>
<td align="right">28.83</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">248332</td>
</tr>
<tr class="even">
<td align="left">out_prncp_inv</td>
<td align="right">255798</td>
<td align="right">28.83</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">266244</td>
</tr>
<tr class="odd">
<td align="left">total_pymnt</td>
<td align="right">17759</td>
<td align="right">2.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">506726</td>
</tr>
<tr class="even">
<td align="left">total_pymnt_inv</td>
<td align="right">18037</td>
<td align="right">2.03</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">506616</td>
</tr>
<tr class="odd">
<td align="left">total_rec_prncp</td>
<td align="right">18145</td>
<td align="right">2.04</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">260227</td>
</tr>
<tr class="even">
<td align="left">total_rec_int</td>
<td align="right">18214</td>
<td align="right">2.05</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">324635</td>
</tr>
<tr class="odd">
<td align="left">total_rec_late_fee</td>
<td align="right">874862</td>
<td align="right">98.59</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">6181</td>
</tr>
<tr class="even">
<td align="left">recoveries</td>
<td align="right">862702</td>
<td align="right">97.22</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">23055</td>
</tr>
<tr class="odd">
<td align="left">collection_recovery_fee</td>
<td align="right">863872</td>
<td align="right">97.35</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">20708</td>
</tr>
<tr class="even">
<td align="left">last_pymnt_d</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">17659</td>
<td align="right">1.99</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">98</td>
</tr>
<tr class="odd">
<td align="left">last_pymnt_amnt</td>
<td align="right">17673</td>
<td align="right">1.99</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">232451</td>
</tr>
<tr class="even">
<td align="left">next_pymnt_d</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">252971</td>
<td align="right">28.51</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">100</td>
</tr>
<tr class="odd">
<td align="left">last_credit_pull_d</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">53</td>
<td align="right">0.01</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">103</td>
</tr>
<tr class="even">
<td align="left">collections_12_mths_ex_med</td>
<td align="right">875553</td>
<td align="right">98.67</td>
<td align="right">145</td>
<td align="right">0.02</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">12</td>
</tr>
<tr class="odd">
<td align="left">mths_since_last_major_derog</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">665676</td>
<td align="right">75.02</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">168</td>
</tr>
<tr class="even">
<td align="left">policy_code</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left">application_type</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="left">annual_inc_joint</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">886868</td>
<td align="right">99.94</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">308</td>
</tr>
<tr class="odd">
<td align="left">dti_joint</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">886870</td>
<td align="right">99.94</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">449</td>
</tr>
<tr class="even">
<td align="left">verification_status_joint</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">886868</td>
<td align="right">99.94</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="left">acc_now_delinq</td>
<td align="right">883236</td>
<td align="right">99.53</td>
<td align="right">29</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">numeric</td>
<td align="right">8</td>
</tr>
<tr class="even">
<td align="left">tot_coll_amt</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">70276</td>
<td align="right">7.92</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">10325</td>
</tr>
<tr class="odd">
<td align="left">tot_cur_bal</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">70276</td>
<td align="right">7.92</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">327342</td>
</tr>
<tr class="even">
<td align="left">open_acc_6m</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">866007</td>
<td align="right">97.59</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">13</td>
</tr>
<tr class="odd">
<td align="left">open_il_6m</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">866007</td>
<td align="right">97.59</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">35</td>
</tr>
<tr class="even">
<td align="left">open_il_12m</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">866007</td>
<td align="right">97.59</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">12</td>
</tr>
<tr class="odd">
<td align="left">open_il_24m</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">866007</td>
<td align="right">97.59</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">17</td>
</tr>
<tr class="even">
<td align="left">mths_since_rcnt_il</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">866569</td>
<td align="right">97.65</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">201</td>
</tr>
<tr class="odd">
<td align="left">total_bal_il</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">866007</td>
<td align="right">97.59</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">17030</td>
</tr>
<tr class="even">
<td align="left">il_util</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">868762</td>
<td align="right">97.90</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">1272</td>
</tr>
<tr class="odd">
<td align="left">open_rv_12m</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">866007</td>
<td align="right">97.59</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">18</td>
</tr>
<tr class="even">
<td align="left">open_rv_24m</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">866007</td>
<td align="right">97.59</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">28</td>
</tr>
<tr class="odd">
<td align="left">max_bal_bc</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">866007</td>
<td align="right">97.59</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">10707</td>
</tr>
<tr class="even">
<td align="left">all_util</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">866007</td>
<td align="right">97.59</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">1128</td>
</tr>
<tr class="odd">
<td align="left">total_rev_hi_lim</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">70276</td>
<td align="right">7.92</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">21251</td>
</tr>
<tr class="even">
<td align="left">inq_fi</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">866007</td>
<td align="right">97.59</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">18</td>
</tr>
<tr class="odd">
<td align="left">total_cu_tl</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">866007</td>
<td align="right">97.59</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">33</td>
</tr>
<tr class="even">
<td align="left">inq_last_12m</td>
<td align="right">0</td>
<td align="right">0.00</td>
<td align="right">866007</td>
<td align="right">97.59</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">29</td>
</tr>
</tbody>
</table>
<p>It seems the data has two unique identifiers <code>id</code> and <code>member_id</code>. There is no variable with 100% missing or zero values i.e.Â information power of zero. There are a few which have a high ratio of NAs so the meta description needs to be checked whether this is expected. There are also variables which have only one unique value, e.g. <code>policy_code</code>. Again, the meta description should be checked to see the rationale but such dimensions are not useful for any analysis or model buidling.</p>
<p>Our meta table also contains the absolute number of unique values which is helpful for plotting (for attributes that is). Another interesting ratio to look at is that of unique values over all values for any attribute. A high ratio would indicate that this is probably a âfreeâ field, i.e.Â no particular constraints are put on its values (except key variables of course but in case of uniqueness their ratio should be one as is the case with <code>id</code> and <code>member_id</code>). When looking into correlations, these high ratio fields will have low correlation with other fields but they may still be useful e.g.Â because they have âdirectionâ information (e.g.Â the direction of an effect) or, in the case of strings, may be useful for text analytics. We thus add a respective variable to the meta data. For improved readability, we use the <code>scales::percent()</code> function to convert output to percent.</p>
<pre class="r"><code>meta_loans &lt;-
  meta_loans %&gt;%
  mutate(uniq_rat = unique / nrow(loans))

meta_loans %&gt;%
  select(variable, unique, uniq_rat) %&gt;%
  mutate(unique = unique, uniq_rat = scales::percent(uniq_rat)) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">variable</th>
<th align="right">unique</th>
<th align="left">uniq_rat</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">id</td>
<td align="right">887379</td>
<td align="left">100.0%</td>
</tr>
<tr class="even">
<td align="left">member_id</td>
<td align="right">887379</td>
<td align="left">100.0%</td>
</tr>
<tr class="odd">
<td align="left">loan_amnt</td>
<td align="right">1372</td>
<td align="left">0.2%</td>
</tr>
<tr class="even">
<td align="left">funded_amnt</td>
<td align="right">1372</td>
<td align="left">0.2%</td>
</tr>
<tr class="odd">
<td align="left">funded_amnt_inv</td>
<td align="right">9856</td>
<td align="left">1.1%</td>
</tr>
<tr class="even">
<td align="left">term</td>
<td align="right">2</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">int_rate</td>
<td align="right">542</td>
<td align="left">0.1%</td>
</tr>
<tr class="even">
<td align="left">installment</td>
<td align="right">68711</td>
<td align="left">7.7%</td>
</tr>
<tr class="odd">
<td align="left">grade</td>
<td align="right">7</td>
<td align="left">0.0%</td>
</tr>
<tr class="even">
<td align="left">sub_grade</td>
<td align="right">35</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">emp_title</td>
<td align="right">289148</td>
<td align="left">32.6%</td>
</tr>
<tr class="even">
<td align="left">emp_length</td>
<td align="right">12</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">home_ownership</td>
<td align="right">6</td>
<td align="left">0.0%</td>
</tr>
<tr class="even">
<td align="left">annual_inc</td>
<td align="right">49384</td>
<td align="left">5.6%</td>
</tr>
<tr class="odd">
<td align="left">verification_status</td>
<td align="right">3</td>
<td align="left">0.0%</td>
</tr>
<tr class="even">
<td align="left">issue_d</td>
<td align="right">103</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">loan_status</td>
<td align="right">10</td>
<td align="left">0.0%</td>
</tr>
<tr class="even">
<td align="left">pymnt_plan</td>
<td align="right">2</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">url</td>
<td align="right">887379</td>
<td align="left">100.0%</td>
</tr>
<tr class="even">
<td align="left">desc</td>
<td align="right">124453</td>
<td align="left">14.0%</td>
</tr>
<tr class="odd">
<td align="left">purpose</td>
<td align="right">14</td>
<td align="left">0.0%</td>
</tr>
<tr class="even">
<td align="left">title</td>
<td align="right">61452</td>
<td align="left">6.9%</td>
</tr>
<tr class="odd">
<td align="left">zip_code</td>
<td align="right">935</td>
<td align="left">0.1%</td>
</tr>
<tr class="even">
<td align="left">addr_state</td>
<td align="right">51</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">dti</td>
<td align="right">4086</td>
<td align="left">0.5%</td>
</tr>
<tr class="even">
<td align="left">delinq_2yrs</td>
<td align="right">29</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">earliest_cr_line</td>
<td align="right">697</td>
<td align="left">0.1%</td>
</tr>
<tr class="even">
<td align="left">inq_last_6mths</td>
<td align="right">28</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">mths_since_last_delinq</td>
<td align="right">155</td>
<td align="left">0.0%</td>
</tr>
<tr class="even">
<td align="left">mths_since_last_record</td>
<td align="right">123</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">open_acc</td>
<td align="right">77</td>
<td align="left">0.0%</td>
</tr>
<tr class="even">
<td align="left">pub_rec</td>
<td align="right">32</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">revol_bal</td>
<td align="right">73740</td>
<td align="left">8.3%</td>
</tr>
<tr class="even">
<td align="left">revol_util</td>
<td align="right">1356</td>
<td align="left">0.2%</td>
</tr>
<tr class="odd">
<td align="left">total_acc</td>
<td align="right">135</td>
<td align="left">0.0%</td>
</tr>
<tr class="even">
<td align="left">initial_list_status</td>
<td align="right">2</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">out_prncp</td>
<td align="right">248332</td>
<td align="left">28.0%</td>
</tr>
<tr class="even">
<td align="left">out_prncp_inv</td>
<td align="right">266244</td>
<td align="left">30.0%</td>
</tr>
<tr class="odd">
<td align="left">total_pymnt</td>
<td align="right">506726</td>
<td align="left">57.1%</td>
</tr>
<tr class="even">
<td align="left">total_pymnt_inv</td>
<td align="right">506616</td>
<td align="left">57.1%</td>
</tr>
<tr class="odd">
<td align="left">total_rec_prncp</td>
<td align="right">260227</td>
<td align="left">29.3%</td>
</tr>
<tr class="even">
<td align="left">total_rec_int</td>
<td align="right">324635</td>
<td align="left">36.6%</td>
</tr>
<tr class="odd">
<td align="left">total_rec_late_fee</td>
<td align="right">6181</td>
<td align="left">0.7%</td>
</tr>
<tr class="even">
<td align="left">recoveries</td>
<td align="right">23055</td>
<td align="left">2.6%</td>
</tr>
<tr class="odd">
<td align="left">collection_recovery_fee</td>
<td align="right">20708</td>
<td align="left">2.3%</td>
</tr>
<tr class="even">
<td align="left">last_pymnt_d</td>
<td align="right">98</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">last_pymnt_amnt</td>
<td align="right">232451</td>
<td align="left">26.2%</td>
</tr>
<tr class="even">
<td align="left">next_pymnt_d</td>
<td align="right">100</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">last_credit_pull_d</td>
<td align="right">103</td>
<td align="left">0.0%</td>
</tr>
<tr class="even">
<td align="left">collections_12_mths_ex_med</td>
<td align="right">12</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">mths_since_last_major_derog</td>
<td align="right">168</td>
<td align="left">0.0%</td>
</tr>
<tr class="even">
<td align="left">policy_code</td>
<td align="right">1</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">application_type</td>
<td align="right">2</td>
<td align="left">0.0%</td>
</tr>
<tr class="even">
<td align="left">annual_inc_joint</td>
<td align="right">308</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">dti_joint</td>
<td align="right">449</td>
<td align="left">0.1%</td>
</tr>
<tr class="even">
<td align="left">verification_status_joint</td>
<td align="right">3</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">acc_now_delinq</td>
<td align="right">8</td>
<td align="left">0.0%</td>
</tr>
<tr class="even">
<td align="left">tot_coll_amt</td>
<td align="right">10325</td>
<td align="left">1.2%</td>
</tr>
<tr class="odd">
<td align="left">tot_cur_bal</td>
<td align="right">327342</td>
<td align="left">36.9%</td>
</tr>
<tr class="even">
<td align="left">open_acc_6m</td>
<td align="right">13</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">open_il_6m</td>
<td align="right">35</td>
<td align="left">0.0%</td>
</tr>
<tr class="even">
<td align="left">open_il_12m</td>
<td align="right">12</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">open_il_24m</td>
<td align="right">17</td>
<td align="left">0.0%</td>
</tr>
<tr class="even">
<td align="left">mths_since_rcnt_il</td>
<td align="right">201</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">total_bal_il</td>
<td align="right">17030</td>
<td align="left">1.9%</td>
</tr>
<tr class="even">
<td align="left">il_util</td>
<td align="right">1272</td>
<td align="left">0.1%</td>
</tr>
<tr class="odd">
<td align="left">open_rv_12m</td>
<td align="right">18</td>
<td align="left">0.0%</td>
</tr>
<tr class="even">
<td align="left">open_rv_24m</td>
<td align="right">28</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">max_bal_bc</td>
<td align="right">10707</td>
<td align="left">1.2%</td>
</tr>
<tr class="even">
<td align="left">all_util</td>
<td align="right">1128</td>
<td align="left">0.1%</td>
</tr>
<tr class="odd">
<td align="left">total_rev_hi_lim</td>
<td align="right">21251</td>
<td align="left">2.4%</td>
</tr>
<tr class="even">
<td align="left">inq_fi</td>
<td align="right">18</td>
<td align="left">0.0%</td>
</tr>
<tr class="odd">
<td align="left">total_cu_tl</td>
<td align="right">33</td>
<td align="left">0.0%</td>
</tr>
<tr class="even">
<td align="left">inq_last_12m</td>
<td align="right">29</td>
<td align="left">0.0%</td>
</tr>
</tbody>
</table>
<p>An attribute like <code>emp_title</code> has over 30% unique values which makes it a poor candidate for modeling as it seems borrowers are free to describe their notion of employment title. A sophisticated model, however, may even take advantage of the data by checking the strings for âindicator wordsâ that may be associated with an honest or a dishonest (credible / non-credible) candidate.</p>
<div id="data-transformation" class="section level2">
<h2><span class="header-section-number">5.1</span> Data Transformation</h2>
<p>We should also check the variable types. As mentioned in section <a href="#data-import">Data Import</a> the import function uses heuristics to guess the data types and these may not always work.</p>
<ul>
<li>annual_inc_joint = character but should be numeric</li>
<li>dates are read as character and need to be transformed</li>
</ul>
<p>First letâs transform character to numeric. We make use of <code>dplyr::mutate_at()</code> function and provide a vector of columns to be mutated (transformed). In general, when using libraries from the <code>tidyverse</code> (these libraries are mainly authored by Hadley Wickham and other RStudio people), most functions offer a standard version as opposed to an NSE (non-standard evaluation) version which can take character values as variable names. These functions usually have a trailing underscore, e.g. <code>dplyr::select_()</code> as compared to the non-standard evaluation function <code>dplyr::select()</code>. For details, see the dplyr vignette on <a href="https://cran.r-project.org/web/packages/dplyr/vignettes/nse.html">Non-standard evaluation</a>. However, note that the <code>tidyverse</code> libraries are still changing a fair amount so best to cross-check whether used function is the most efficient one for given task or if there is a new one introduced in the mean time.</p>
<p>As opposed to packages like <code>data.table</code> tidyverse packages do not alter their input in place so we have to re-assign the result to the original object. We could also create a new object but this is memory-inefficient (in fact even the re-assignment to the original object creates a temporary copy). For more details on memory management, see chapter <a href="http://adv-r.had.co.nz/memory.html">Memory</a> in Hadley Wickhamâs book âAdvanced Râ.</p>
<pre class="r"><code>chr_to_num_vars &lt;- 
  c(&quot;annual_inc_joint&quot;, &quot;mths_since_last_major_derog&quot;, &quot;open_acc_6m&quot;,
    &quot;open_il_6m&quot;, &quot;open_il_12m&quot;, &quot;open_il_24m&quot;, &quot;mths_since_rcnt_il&quot;,
    &quot;total_bal_il&quot;, &quot;il_util&quot;, &quot;open_rv_12m&quot;, &quot;open_rv_24m&quot;,
    &quot;max_bal_bc&quot;, &quot;all_util&quot;, &quot;total_rev_hi_lim&quot;, &quot;total_cu_tl&quot;,
    &quot;inq_last_12m&quot;, &quot;dti_joint&quot;, &quot;inq_fi&quot;, &quot;tot_cur_bal&quot;, &quot;tot_coll_amt&quot;)

loans &lt;-
  loans %&gt;%
  mutate_at(.funs = funs(as.numeric), .vars = chr_to_num_vars)</code></pre>
<p>Letâs have a look at the date variables to see how they need to be transformed.</p>
<pre class="r"><code>chr_to_date_vars &lt;- 
  c(&quot;issue_d&quot;, &quot;last_pymnt_d&quot;, &quot;last_credit_pull_d&quot;,
    &quot;next_pymnt_d&quot;, &quot;earliest_cr_line&quot;, &quot;next_pymnt_d&quot;)

loans %&gt;%
  select_(.dots = chr_to_date_vars) %&gt;%
  str()</code></pre>
<pre><code>## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:    887379 obs. of  5 variables:
##  $ issue_d           : chr  &quot;Dec-2011&quot; &quot;Dec-2011&quot; &quot;Dec-2011&quot; &quot;Dec-2011&quot; ...
##  $ last_pymnt_d      : chr  &quot;Jan-2015&quot; &quot;Apr-2013&quot; &quot;Jun-2014&quot; &quot;Jan-2015&quot; ...
##  $ last_credit_pull_d: chr  &quot;Jan-2016&quot; &quot;Sep-2013&quot; &quot;Jan-2016&quot; &quot;Jan-2015&quot; ...
##  $ next_pymnt_d      : chr  NA NA NA NA ...
##  $ earliest_cr_line  : chr  &quot;Jan-1985&quot; &quot;Apr-1999&quot; &quot;Nov-2001&quot; &quot;Feb-1996&quot; ...</code></pre>
<pre class="r"><code>head(unique(loans$next_pymnt_d))</code></pre>
<pre><code>## [1] NA         &quot;Feb-2016&quot; &quot;Jan-2016&quot; &quot;Sep-2013&quot; &quot;Feb-2014&quot; &quot;May-2014&quot;</code></pre>
<pre class="r"><code>for (i in chr_to_date_vars){
  print(head(unique(loans[, i])))
  }</code></pre>
<pre><code>## # A tibble: 6 x 1
##    issue_d
##      &lt;chr&gt;
## 1 Dec-2011
## 2 Nov-2011
## 3 Oct-2011
## 4 Sep-2011
## 5 Aug-2011
## 6 Jul-2011
## # A tibble: 6 x 1
##   last_pymnt_d
##          &lt;chr&gt;
## 1     Jan-2015
## 2     Apr-2013
## 3     Jun-2014
## 4     Jan-2016
## 5     Apr-2012
## 6     Nov-2012
## # A tibble: 6 x 1
##   last_credit_pull_d
##                &lt;chr&gt;
## 1           Jan-2016
## 2           Sep-2013
## 3           Jan-2015
## 4           Sep-2015
## 5           Dec-2014
## 6           Aug-2012
## # A tibble: 6 x 1
##   next_pymnt_d
##          &lt;chr&gt;
## 1         &lt;NA&gt;
## 2     Feb-2016
## 3     Jan-2016
## 4     Sep-2013
## 5     Feb-2014
## 6     May-2014
## # A tibble: 6 x 1
##   earliest_cr_line
##              &lt;chr&gt;
## 1         Jan-1985
## 2         Apr-1999
## 3         Nov-2001
## 4         Feb-1996
## 5         Jan-1996
## 6         Nov-2004
## # A tibble: 6 x 1
##   next_pymnt_d
##          &lt;chr&gt;
## 1         &lt;NA&gt;
## 2     Feb-2016
## 3     Jan-2016
## 4     Sep-2013
## 5     Feb-2014
## 6     May-2014</code></pre>
<p>It seems the date format is consistent and follows month-year convention. We can use the <code>base::as.Date()</code> function to convert this to a date format. The function requires a day as well so we simply add the 1st of each month to the existing character string via pasting together strings using <code>base::paste0()</code>. Note that <code>base::paste0(&quot;bla&quot;, NA)</code> will not return <code>NA</code> but the concatenated string (here: <code>blaNA</code>). Conveniently, <code>base::as.Date()</code> will return <code>NA</code> so we can leave it at that. Alternatively, we could include an exception handler for NA values to explicitly handle those because we saw previously that some date variables include NA values. One way to approach that would be to wrap the date conversion function into a <code>base::ifelse()</code> call as so <code>ifelse(is.na(x), NA, some_date_function()</code>. However, it seems that <code>base::ifelse()</code> is dropping the date class that we just created with our date function, for details, see e.g. <a href="http://stackoverflow.com/questions/6668963/how-to-prevent-ifelse-from-turning-date-objects-into-numeric-objects">How to prevent ifelse from turning Date objects into numeric objects</a>. As we do not want to deal with these issues at this moment, we simply go with the default behavior of <code>base::as.Date()</code> as it returns <code>NA</code> in cases of bad input anyway.</p>
<p>Letâs also return the NA values of the input to remind ourselves of the number. It should coincide with the number of NA after we have converted / transformed the date variables.</p>
<pre class="r"><code>meta_loans %&gt;% 
  select(variable, q_na) %&gt;% 
  filter(variable %in% chr_to_date_vars)</code></pre>
<pre><code>##             variable   q_na
## 1            issue_d      0
## 2   earliest_cr_line     29
## 3       last_pymnt_d  17659
## 4       next_pymnt_d 252971
## 5 last_credit_pull_d     53</code></pre>
<p>Finally, this is how our custom date conversion function will look like, we call it <code>convert_date()</code> and it will take the string value to be converted as input. We define the date format by following the function conventions of <code>base::as.Date()</code>, for details see <code>?base::as.Date</code>. We could have also used an <a href="http://adv-r.had.co.nz/Functional-programming.html#anonymous-functions">anonymous function</a> directly in the <code>dplyr::mutate_at()</code> call but the creation of a specific function seemed appropriate as we may use it several times.</p>
<pre class="r"><code>convert_date &lt;- function(x){
  as.Date(paste0(&quot;01-&quot;, x), format = &quot;%d-%b-%Y&quot;)
  } 

loans &lt;-
  loans %&gt;%
  mutate_at(.funs = funs(convert_date), .vars = chr_to_date_vars)</code></pre>
<pre class="r"><code>num_vars &lt;- 
  loans %&gt;% 
  sapply(is.numeric) %&gt;% 
  which() %&gt;% 
  names()

meta_loans %&gt;%
  select(variable, p_zeros, p_na, unique) %&gt;%
  filter_(~ variable %in% num_vars) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">variable</th>
<th align="right">p_zeros</th>
<th align="right">p_na</th>
<th align="right">unique</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">id</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">887379</td>
</tr>
<tr class="even">
<td align="left">member_id</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">887379</td>
</tr>
<tr class="odd">
<td align="left">loan_amnt</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">1372</td>
</tr>
<tr class="even">
<td align="left">funded_amnt</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">1372</td>
</tr>
<tr class="odd">
<td align="left">funded_amnt_inv</td>
<td align="right">0.03</td>
<td align="right">0.00</td>
<td align="right">9856</td>
</tr>
<tr class="even">
<td align="left">int_rate</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">542</td>
</tr>
<tr class="odd">
<td align="left">installment</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">68711</td>
</tr>
<tr class="even">
<td align="left">annual_inc</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">49384</td>
</tr>
<tr class="odd">
<td align="left">dti</td>
<td align="right">0.05</td>
<td align="right">0.00</td>
<td align="right">4086</td>
</tr>
<tr class="even">
<td align="left">delinq_2yrs</td>
<td align="right">80.80</td>
<td align="right">0.00</td>
<td align="right">29</td>
</tr>
<tr class="odd">
<td align="left">inq_last_6mths</td>
<td align="right">56.11</td>
<td align="right">0.00</td>
<td align="right">28</td>
</tr>
<tr class="even">
<td align="left">mths_since_last_delinq</td>
<td align="right">0.19</td>
<td align="right">51.20</td>
<td align="right">155</td>
</tr>
<tr class="odd">
<td align="left">mths_since_last_record</td>
<td align="right">0.14</td>
<td align="right">84.56</td>
<td align="right">123</td>
</tr>
<tr class="even">
<td align="left">open_acc</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">77</td>
</tr>
<tr class="odd">
<td align="left">pub_rec</td>
<td align="right">84.70</td>
<td align="right">0.00</td>
<td align="right">32</td>
</tr>
<tr class="even">
<td align="left">revol_bal</td>
<td align="right">0.38</td>
<td align="right">0.00</td>
<td align="right">73740</td>
</tr>
<tr class="odd">
<td align="left">revol_util</td>
<td align="right">0.40</td>
<td align="right">0.06</td>
<td align="right">1356</td>
</tr>
<tr class="even">
<td align="left">total_acc</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">135</td>
</tr>
<tr class="odd">
<td align="left">out_prncp</td>
<td align="right">28.83</td>
<td align="right">0.00</td>
<td align="right">248332</td>
</tr>
<tr class="even">
<td align="left">out_prncp_inv</td>
<td align="right">28.83</td>
<td align="right">0.00</td>
<td align="right">266244</td>
</tr>
<tr class="odd">
<td align="left">total_pymnt</td>
<td align="right">2.00</td>
<td align="right">0.00</td>
<td align="right">506726</td>
</tr>
<tr class="even">
<td align="left">total_pymnt_inv</td>
<td align="right">2.03</td>
<td align="right">0.00</td>
<td align="right">506616</td>
</tr>
<tr class="odd">
<td align="left">total_rec_prncp</td>
<td align="right">2.04</td>
<td align="right">0.00</td>
<td align="right">260227</td>
</tr>
<tr class="even">
<td align="left">total_rec_int</td>
<td align="right">2.05</td>
<td align="right">0.00</td>
<td align="right">324635</td>
</tr>
<tr class="odd">
<td align="left">total_rec_late_fee</td>
<td align="right">98.59</td>
<td align="right">0.00</td>
<td align="right">6181</td>
</tr>
<tr class="even">
<td align="left">recoveries</td>
<td align="right">97.22</td>
<td align="right">0.00</td>
<td align="right">23055</td>
</tr>
<tr class="odd">
<td align="left">collection_recovery_fee</td>
<td align="right">97.35</td>
<td align="right">0.00</td>
<td align="right">20708</td>
</tr>
<tr class="even">
<td align="left">last_pymnt_amnt</td>
<td align="right">1.99</td>
<td align="right">0.00</td>
<td align="right">232451</td>
</tr>
<tr class="odd">
<td align="left">collections_12_mths_ex_med</td>
<td align="right">98.67</td>
<td align="right">0.02</td>
<td align="right">12</td>
</tr>
<tr class="even">
<td align="left">mths_since_last_major_derog</td>
<td align="right">0.00</td>
<td align="right">75.02</td>
<td align="right">168</td>
</tr>
<tr class="odd">
<td align="left">policy_code</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">annual_inc_joint</td>
<td align="right">0.00</td>
<td align="right">99.94</td>
<td align="right">308</td>
</tr>
<tr class="odd">
<td align="left">dti_joint</td>
<td align="right">0.00</td>
<td align="right">99.94</td>
<td align="right">449</td>
</tr>
<tr class="even">
<td align="left">acc_now_delinq</td>
<td align="right">99.53</td>
<td align="right">0.00</td>
<td align="right">8</td>
</tr>
<tr class="odd">
<td align="left">tot_coll_amt</td>
<td align="right">0.00</td>
<td align="right">7.92</td>
<td align="right">10325</td>
</tr>
<tr class="even">
<td align="left">tot_cur_bal</td>
<td align="right">0.00</td>
<td align="right">7.92</td>
<td align="right">327342</td>
</tr>
<tr class="odd">
<td align="left">open_acc_6m</td>
<td align="right">0.00</td>
<td align="right">97.59</td>
<td align="right">13</td>
</tr>
<tr class="even">
<td align="left">open_il_6m</td>
<td align="right">0.00</td>
<td align="right">97.59</td>
<td align="right">35</td>
</tr>
<tr class="odd">
<td align="left">open_il_12m</td>
<td align="right">0.00</td>
<td align="right">97.59</td>
<td align="right">12</td>
</tr>
<tr class="even">
<td align="left">open_il_24m</td>
<td align="right">0.00</td>
<td align="right">97.59</td>
<td align="right">17</td>
</tr>
<tr class="odd">
<td align="left">mths_since_rcnt_il</td>
<td align="right">0.00</td>
<td align="right">97.65</td>
<td align="right">201</td>
</tr>
<tr class="even">
<td align="left">total_bal_il</td>
<td align="right">0.00</td>
<td align="right">97.59</td>
<td align="right">17030</td>
</tr>
<tr class="odd">
<td align="left">il_util</td>
<td align="right">0.00</td>
<td align="right">97.90</td>
<td align="right">1272</td>
</tr>
<tr class="even">
<td align="left">open_rv_12m</td>
<td align="right">0.00</td>
<td align="right">97.59</td>
<td align="right">18</td>
</tr>
<tr class="odd">
<td align="left">open_rv_24m</td>
<td align="right">0.00</td>
<td align="right">97.59</td>
<td align="right">28</td>
</tr>
<tr class="even">
<td align="left">max_bal_bc</td>
<td align="right">0.00</td>
<td align="right">97.59</td>
<td align="right">10707</td>
</tr>
<tr class="odd">
<td align="left">all_util</td>
<td align="right">0.00</td>
<td align="right">97.59</td>
<td align="right">1128</td>
</tr>
<tr class="even">
<td align="left">total_rev_hi_lim</td>
<td align="right">0.00</td>
<td align="right">7.92</td>
<td align="right">21251</td>
</tr>
<tr class="odd">
<td align="left">inq_fi</td>
<td align="right">0.00</td>
<td align="right">97.59</td>
<td align="right">18</td>
</tr>
<tr class="even">
<td align="left">total_cu_tl</td>
<td align="right">0.00</td>
<td align="right">97.59</td>
<td align="right">33</td>
</tr>
<tr class="odd">
<td align="left">inq_last_12m</td>
<td align="right">0.00</td>
<td align="right">97.59</td>
<td align="right">29</td>
</tr>
</tbody>
</table>
<p>We also see that variables <code>mths_since_last_delinq</code>, <code>mths_since_last_record</code>, <code>mths_since_last_major_derog</code>, <code>dti_joint</code> and <code>annual_inc_joint</code> have a large share of NA values. If we think about this in more detail, it may be reasonable to assume that NA values for the variables <code>mths_since_last_delinq</code>, <code>mths_since_last_record</code> and <code>mths_since_last_major_derog</code> actually indicate that there was no event/record of any missed payment so there cannot be any time value. Analogously, a missing value for <code>annual_inc_joint</code> and <code>dti_joint</code> may simply indicate that it is a single borrower or the partner has no income. Thus, the first three variables actually carry valuable information that may be lost if we ignored it. We will thus replace the missing values with zeros to make them available for modeling. It should be noted though that a zero time could indicate an event that is just happening so we have to document our assumptions carefully.</p>
<pre class="r"><code>na_to_zero_vars &lt;-
  c(&quot;mths_since_last_delinq&quot;, &quot;mths_since_last_record&quot;,
    &quot;mths_since_last_major_derog&quot;)

loans &lt;- 
  loans %&gt;%
  mutate_at(.vars = na_to_zero_vars, .funs = funs(replace(., is.na(.), 0)))</code></pre>
<p>These transformations should have us covered for now. We recreate the meta table after all these changes.</p>
<pre class="r"><code>meta_loans &lt;- funModeling::df_status(loans, print_results = FALSE)
meta_loans &lt;-
  meta_loans %&gt;%
  mutate(uniq_rat = unique / nrow(loans))</code></pre>
</div>
<div id="additional-meta-data-file" class="section level2">
<h2><span class="header-section-number">5.2</span> Additional Meta Data File</h2>
<p>Next we look at the meta descriptions provided in the additional Excel file.</p>
<pre class="r"><code>knitr::kable(meta_loan_stats[,1:2])</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">LoanStatNew</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">acc_now_delinq</td>
<td align="left">The number of accounts on which the borrower is now delinquent.</td>
</tr>
<tr class="even">
<td align="left">acc_open_past_24mths</td>
<td align="left">Number of trades opened in past 24 months.</td>
</tr>
<tr class="odd">
<td align="left">addr_state</td>
<td align="left">The state provided by the borrower in the loan application</td>
</tr>
<tr class="even">
<td align="left">all_util</td>
<td align="left">Balance to credit limit on all trades</td>
</tr>
<tr class="odd">
<td align="left">annual_inc</td>
<td align="left">The self-reported annual income provided by the borrower during registration.</td>
</tr>
<tr class="even">
<td align="left">annual_inc_joint</td>
<td align="left">The combined self-reported annual income provided by the co-borrowers during registration</td>
</tr>
<tr class="odd">
<td align="left">application_type</td>
<td align="left">Indicates whether the loan is an individual application or a joint application with two co-borrowers</td>
</tr>
<tr class="even">
<td align="left">avg_cur_bal</td>
<td align="left">Average current balance of all accounts</td>
</tr>
<tr class="odd">
<td align="left">bc_open_to_buy</td>
<td align="left">Total open to buy on revolving bankcards.</td>
</tr>
<tr class="even">
<td align="left">bc_util</td>
<td align="left">Ratio of total current balance to high credit/credit limit for all bankcard accounts.</td>
</tr>
<tr class="odd">
<td align="left">chargeoff_within_12_mths</td>
<td align="left">Number of charge-offs within 12 months</td>
</tr>
<tr class="even">
<td align="left">collection_recovery_fee</td>
<td align="left">post charge off collection fee</td>
</tr>
<tr class="odd">
<td align="left">collections_12_mths_ex_med</td>
<td align="left">Number of collections in 12 months excluding medical collections</td>
</tr>
<tr class="even">
<td align="left">delinq_2yrs</td>
<td align="left">The number of 30+ days past-due incidences of delinquency in the borrowerâs credit file for the past 2 years</td>
</tr>
<tr class="odd">
<td align="left">delinq_amnt</td>
<td align="left">The past-due amount owed for the accounts on which the borrower is now delinquent.</td>
</tr>
<tr class="even">
<td align="left">desc</td>
<td align="left">Loan description provided by the borrower</td>
</tr>
<tr class="odd">
<td align="left">dti</td>
<td align="left">A ratio calculated using the borrowerâs total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrowerâs self-reported monthly income.</td>
</tr>
<tr class="even">
<td align="left">dti_joint</td>
<td align="left">A ratio calculated using the co-borrowersâ total monthly payments on the total debt obligations, excluding mortgages and the requested LC loan, divided by the co-borrowersâ combined self-reported monthly income</td>
</tr>
<tr class="odd">
<td align="left">earliest_cr_line</td>
<td align="left">The month the borrowerâs earliest reported credit line was opened</td>
</tr>
<tr class="even">
<td align="left">emp_length</td>
<td align="left">Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.</td>
</tr>
<tr class="odd">
<td align="left">emp_title</td>
<td align="left">The job title supplied by the Borrower when applying for the loan.*</td>
</tr>
<tr class="even">
<td align="left">fico_range_high</td>
<td align="left">The upper boundary range the borrowerâs FICO at loan origination belongs to.</td>
</tr>
<tr class="odd">
<td align="left">fico_range_low</td>
<td align="left">The lower boundary range the borrowerâs FICO at loan origination belongs to.</td>
</tr>
<tr class="even">
<td align="left">funded_amnt</td>
<td align="left">The total amount committed to that loan at that point in time.</td>
</tr>
<tr class="odd">
<td align="left">funded_amnt_inv</td>
<td align="left">The total amount committed by investors for that loan at that point in time.</td>
</tr>
<tr class="even">
<td align="left">grade</td>
<td align="left">LC assigned loan grade</td>
</tr>
<tr class="odd">
<td align="left">home_ownership</td>
<td align="left">The home ownership status provided by the borrower during registrationÂ or obtained from the credit report.Â Our values are: RENT, OWN, MORTGAGE, OTHER</td>
</tr>
<tr class="even">
<td align="left">id</td>
<td align="left">A unique LC assigned ID for the loan listing.</td>
</tr>
<tr class="odd">
<td align="left">il_util</td>
<td align="left">Ratio of total current balance to high credit/credit limit on all install acct</td>
</tr>
<tr class="even">
<td align="left">initial_list_status</td>
<td align="left">The initial listing status of the loan. Possible values are â W, F</td>
</tr>
<tr class="odd">
<td align="left">inq_fi</td>
<td align="left">Number of personal finance inquiries</td>
</tr>
<tr class="even">
<td align="left">inq_last_12m</td>
<td align="left">Number of credit inquiries in past 12 months</td>
</tr>
<tr class="odd">
<td align="left">inq_last_6mths</td>
<td align="left">The number of inquiries in past 6 months (excluding auto and mortgage inquiries)</td>
</tr>
<tr class="even">
<td align="left">installment</td>
<td align="left">The monthly payment owed by the borrower if the loan originates.</td>
</tr>
<tr class="odd">
<td align="left">int_rate</td>
<td align="left">Interest Rate on the loan</td>
</tr>
<tr class="even">
<td align="left">issue_d</td>
<td align="left">The month which the loan was funded</td>
</tr>
<tr class="odd">
<td align="left">last_credit_pull_d</td>
<td align="left">The most recent month LC pulled credit for this loan</td>
</tr>
<tr class="even">
<td align="left">last_fico_range_high</td>
<td align="left">The upper boundary range the borrowerâs last FICO pulled belongs to.</td>
</tr>
<tr class="odd">
<td align="left">last_fico_range_low</td>
<td align="left">The lower boundary range the borrowerâs last FICO pulled belongs to.</td>
</tr>
<tr class="even">
<td align="left">last_pymnt_amnt</td>
<td align="left">Last total payment amount received</td>
</tr>
<tr class="odd">
<td align="left">last_pymnt_d</td>
<td align="left">Last month payment was received</td>
</tr>
<tr class="even">
<td align="left">loan_amnt</td>
<td align="left">The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.</td>
</tr>
<tr class="odd">
<td align="left">loan_status</td>
<td align="left">Current status of the loan</td>
</tr>
<tr class="even">
<td align="left">max_bal_bc</td>
<td align="left">Maximum current balance owed on all revolving accounts</td>
</tr>
<tr class="odd">
<td align="left">member_id</td>
<td align="left">A unique LC assigned Id for the borrower member.</td>
</tr>
<tr class="even">
<td align="left">mo_sin_old_il_acct</td>
<td align="left">Months since oldest bank installment account opened</td>
</tr>
<tr class="odd">
<td align="left">mo_sin_old_rev_tl_op</td>
<td align="left">Months since oldest revolving account opened</td>
</tr>
<tr class="even">
<td align="left">mo_sin_rcnt_rev_tl_op</td>
<td align="left">Months since most recent revolving account opened</td>
</tr>
<tr class="odd">
<td align="left">mo_sin_rcnt_tl</td>
<td align="left">Months since most recent account opened</td>
</tr>
<tr class="even">
<td align="left">mort_acc</td>
<td align="left">Number of mortgage accounts.</td>
</tr>
<tr class="odd">
<td align="left">mths_since_last_delinq</td>
<td align="left">The number of months since the borrowerâs last delinquency.</td>
</tr>
<tr class="even">
<td align="left">mths_since_last_major_derog</td>
<td align="left">Months since most recent 90-day or worse rating</td>
</tr>
<tr class="odd">
<td align="left">mths_since_last_record</td>
<td align="left">The number of months since the last public record.</td>
</tr>
<tr class="even">
<td align="left">mths_since_rcnt_il</td>
<td align="left">Months since most recent installment accounts opened</td>
</tr>
<tr class="odd">
<td align="left">mths_since_recent_bc</td>
<td align="left">Months since most recent bankcard account opened.</td>
</tr>
<tr class="even">
<td align="left">mths_since_recent_bc_dlq</td>
<td align="left">Months since most recent bankcard delinquency</td>
</tr>
<tr class="odd">
<td align="left">mths_since_recent_inq</td>
<td align="left">Months since most recent inquiry.</td>
</tr>
<tr class="even">
<td align="left">mths_since_recent_revol_delinq</td>
<td align="left">Months since most recent revolving delinquency.</td>
</tr>
<tr class="odd">
<td align="left">next_pymnt_d</td>
<td align="left">Next scheduled payment date</td>
</tr>
<tr class="even">
<td align="left">num_accts_ever_120_pd</td>
<td align="left">Number of accounts ever 120 or more days past due</td>
</tr>
<tr class="odd">
<td align="left">num_actv_bc_tl</td>
<td align="left">Number of currently active bankcard accounts</td>
</tr>
<tr class="even">
<td align="left">num_actv_rev_tl</td>
<td align="left">Number of currently active revolving trades</td>
</tr>
<tr class="odd">
<td align="left">num_bc_sats</td>
<td align="left">Number of satisfactory bankcard accounts</td>
</tr>
<tr class="even">
<td align="left">num_bc_tl</td>
<td align="left">Number of bankcard accounts</td>
</tr>
<tr class="odd">
<td align="left">num_il_tl</td>
<td align="left">Number of installment accounts</td>
</tr>
<tr class="even">
<td align="left">num_op_rev_tl</td>
<td align="left">Number of open revolving accounts</td>
</tr>
<tr class="odd">
<td align="left">num_rev_accts</td>
<td align="left">Number of revolving accounts</td>
</tr>
<tr class="even">
<td align="left">num_rev_tl_bal_gt_0</td>
<td align="left">Number of revolving trades with balance &gt;0</td>
</tr>
<tr class="odd">
<td align="left">num_sats</td>
<td align="left">Number of satisfactory accounts</td>
</tr>
<tr class="even">
<td align="left">num_tl_120dpd_2m</td>
<td align="left">Number of accounts currently 120 days past due (updated in past 2 months)</td>
</tr>
<tr class="odd">
<td align="left">num_tl_30dpd</td>
<td align="left">Number of accounts currently 30 days past due (updated in past 2 months)</td>
</tr>
<tr class="even">
<td align="left">num_tl_90g_dpd_24m</td>
<td align="left">Number of accounts 90 or more days past due in last 24 months</td>
</tr>
<tr class="odd">
<td align="left">num_tl_op_past_12m</td>
<td align="left">Number of accounts opened in past 12 months</td>
</tr>
<tr class="even">
<td align="left">open_acc</td>
<td align="left">The number of open credit lines in the borrowerâs credit file.</td>
</tr>
<tr class="odd">
<td align="left">open_acc_6m</td>
<td align="left">Number of open trades in last 6 months</td>
</tr>
<tr class="even">
<td align="left">open_il_12m</td>
<td align="left">Number of installment accounts opened in past 12 months</td>
</tr>
<tr class="odd">
<td align="left">open_il_24m</td>
<td align="left">Number of installment accounts opened in past 24 months</td>
</tr>
<tr class="even">
<td align="left">open_il_6m</td>
<td align="left">Number of currently active installment trades</td>
</tr>
<tr class="odd">
<td align="left">open_rv_12m</td>
<td align="left">Number of revolving trades opened in past 12 months</td>
</tr>
<tr class="even">
<td align="left">open_rv_24m</td>
<td align="left">Number of revolving trades opened in past 24 months</td>
</tr>
<tr class="odd">
<td align="left">out_prncp</td>
<td align="left">Remaining outstanding principal for total amount funded</td>
</tr>
<tr class="even">
<td align="left">out_prncp_inv</td>
<td align="left">Remaining outstanding principal for portion of total amount funded by investors</td>
</tr>
<tr class="odd">
<td align="left">pct_tl_nvr_dlq</td>
<td align="left">Percent of trades never delinquent</td>
</tr>
<tr class="even">
<td align="left">percent_bc_gt_75</td>
<td align="left">Percentage of all bankcard accounts &gt; 75% of limit.</td>
</tr>
<tr class="odd">
<td align="left">policy_code</td>
<td align="left">publicly available policy_code=1</td>
</tr>
<tr class="even">
<td align="left">new products not publicly availab</td>
<td align="left">le policy_code=2</td>
</tr>
<tr class="odd">
<td align="left">pub_rec</td>
<td align="left">Number of derogatory public records</td>
</tr>
<tr class="even">
<td align="left">pub_rec_bankruptcies</td>
<td align="left">Number of public record bankruptcies</td>
</tr>
<tr class="odd">
<td align="left">purpose</td>
<td align="left">A category provided by the borrower for the loan request.</td>
</tr>
<tr class="even">
<td align="left">pymnt_plan</td>
<td align="left">Indicates if a payment plan has been put in place for the loan</td>
</tr>
<tr class="odd">
<td align="left">recoveries</td>
<td align="left">post charge off gross recovery</td>
</tr>
<tr class="even">
<td align="left">revol_bal</td>
<td align="left">Total credit revolving balance</td>
</tr>
<tr class="odd">
<td align="left">revol_util</td>
<td align="left">Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.</td>
</tr>
<tr class="even">
<td align="left">sub_grade</td>
<td align="left">LC assigned loan subgrade</td>
</tr>
<tr class="odd">
<td align="left">tax_liens</td>
<td align="left">Number of tax liens</td>
</tr>
<tr class="even">
<td align="left">term</td>
<td align="left">The number of payments on the loan. Values are in months and can be either 36 or 60.</td>
</tr>
<tr class="odd">
<td align="left">title</td>
<td align="left">The loan title provided by the borrower</td>
</tr>
<tr class="even">
<td align="left">tot_coll_amt</td>
<td align="left">Total collection amounts ever owed</td>
</tr>
<tr class="odd">
<td align="left">tot_cur_bal</td>
<td align="left">Total current balance of all accounts</td>
</tr>
<tr class="even">
<td align="left">tot_hi_cred_lim</td>
<td align="left">Total high credit/credit limit</td>
</tr>
<tr class="odd">
<td align="left">total_acc</td>
<td align="left">The total number of credit lines currently in the borrowerâs credit file</td>
</tr>
<tr class="even">
<td align="left">total_bal_ex_mort</td>
<td align="left">Total credit balance excluding mortgage</td>
</tr>
<tr class="odd">
<td align="left">total_bal_il</td>
<td align="left">Total current balance of all installment accounts</td>
</tr>
<tr class="even">
<td align="left">total_bc_limit</td>
<td align="left">Total bankcard high credit/credit limit</td>
</tr>
<tr class="odd">
<td align="left">total_cu_tl</td>
<td align="left">Number of finance trades</td>
</tr>
<tr class="even">
<td align="left">total_il_high_credit_limit</td>
<td align="left">Total installment high credit/credit limit</td>
</tr>
<tr class="odd">
<td align="left">total_pymnt</td>
<td align="left">Payments received to date for total amount funded</td>
</tr>
<tr class="even">
<td align="left">total_pymnt_inv</td>
<td align="left">Payments received to date for portion of total amount funded by investors</td>
</tr>
<tr class="odd">
<td align="left">total_rec_int</td>
<td align="left">Interest received to date</td>
</tr>
<tr class="even">
<td align="left">total_rec_late_fee</td>
<td align="left">Late fees received to date</td>
</tr>
<tr class="odd">
<td align="left">total_rec_prncp</td>
<td align="left">Principal received to date</td>
</tr>
<tr class="even">
<td align="left">total_rev_hi_lim</td>
<td align="left">Total revolving high credit/credit limit</td>
</tr>
<tr class="odd">
<td align="left">url</td>
<td align="left">URL for the LC page with listing data.</td>
</tr>
<tr class="even">
<td align="left">verification_status</td>
<td align="left">Indicates if income was verified by LC, not verified, or if the income source was verified</td>
</tr>
<tr class="odd">
<td align="left">verified_status_joint</td>
<td align="left">Indicates if the co-borrowersâ joint income was verified by LC, not verified, or if the income source was verified</td>
</tr>
<tr class="even">
<td align="left">zip_code</td>
<td align="left">The first 3 numbers of the zip code provided by the borrower in the loan application.</td>
</tr>
<tr class="odd">
<td align="left">NA</td>
<td align="left">NA</td>
</tr>
<tr class="even">
<td align="left">NA</td>
<td align="left">* Employer Title replaces Employer Name for all loans listed after 9/23/2013</td>
</tr>
</tbody>
</table>
<p>As expected, <code>id</code> is âA unique LC assigned ID for the loan listing.â and <code>member_id</code> is âA unique LC assigned Id for the borrower member.â. The <code>policy_code</code> is âpublicly available policy_code=1 new products not publicly available policy_code=2â. That means there could be different values but as we have seen before in this data it only takes on one value.</p>
<p>Finally, letâs look at variables that are either in the data and not in the meta description or vice versa. The <code>dplyr::setdiff()</code> function does what its name suggests, just pay attention to the order of arguments to understand which difference you actually get.</p>
<p>Variables in loans data but not in meta description.</p>
<pre class="r"><code>dplyr::setdiff(colnames(loans), meta_loan_stats$LoanStatNew)</code></pre>
<pre><code>## [1] &quot;verification_status_joint&quot; &quot;total_rev_hi_lim&quot;</code></pre>
<p>Variables in meta description but not in loans data.</p>
<pre class="r"><code>dplyr::setdiff(meta_loan_stats$LoanStatNew, colnames(loans))</code></pre>
<pre><code>##  [1] &quot;acc_open_past_24mths&quot;           &quot;avg_cur_bal&quot;                   
##  [3] &quot;bc_open_to_buy&quot;                 &quot;bc_util&quot;                       
##  [5] &quot;chargeoff_within_12_mths&quot;       &quot;delinq_amnt&quot;                   
##  [7] &quot;fico_range_high&quot;                &quot;fico_range_low&quot;                
##  [9] &quot;last_fico_range_high&quot;           &quot;last_fico_range_low&quot;           
## [11] &quot;mo_sin_old_il_acct&quot;             &quot;mo_sin_old_rev_tl_op&quot;          
## [13] &quot;mo_sin_rcnt_rev_tl_op&quot;          &quot;mo_sin_rcnt_tl&quot;                
## [15] &quot;mort_acc&quot;                       &quot;mths_since_recent_bc&quot;          
## [17] &quot;mths_since_recent_bc_dlq&quot;       &quot;mths_since_recent_inq&quot;         
## [19] &quot;mths_since_recent_revol_delinq&quot; &quot;num_accts_ever_120_pd&quot;         
## [21] &quot;num_actv_bc_tl&quot;                 &quot;num_actv_rev_tl&quot;               
## [23] &quot;num_bc_sats&quot;                    &quot;num_bc_tl&quot;                     
## [25] &quot;num_il_tl&quot;                      &quot;num_op_rev_tl&quot;                 
## [27] &quot;num_rev_accts&quot;                  &quot;num_rev_tl_bal_gt_0&quot;           
## [29] &quot;num_sats&quot;                       &quot;num_tl_120dpd_2m&quot;              
## [31] &quot;num_tl_30dpd&quot;                   &quot;num_tl_90g_dpd_24m&quot;            
## [33] &quot;num_tl_op_past_12m&quot;             &quot;pct_tl_nvr_dlq&quot;                
## [35] &quot;percent_bc_gt_75&quot;               &quot;pub_rec_bankruptcies&quot;          
## [37] &quot;tax_liens&quot;                      &quot;tot_hi_cred_lim&quot;               
## [39] &quot;total_bal_ex_mort&quot;              &quot;total_bc_limit&quot;                
## [41] &quot;total_il_high_credit_limit&quot;     &quot;total_rev_hi_lim Â &quot;            
## [43] &quot;verified_status_joint&quot;          NA</code></pre>
<p>It seems for the loans variables <code>verification_status_joint</code> and <code>total_rev_hi_lim</code> there are actually equivalents in the meta data but they carry slightly different names or have leading / trailing blanks. There are quite a few variables in the meta description that are not in the data. But that should be less of a concern as we donât need those anyway.</p>
<p></p>
</div>
</div>
<div id="defining-default" class="section level1">
<h1><span class="header-section-number">6</span> Defining default</h1>
<p>Our ultimate goal is the prediction of loan defaults from a given set of observations by selecting explanatory (independent) variables (also called feature in machine learning parlance) that result in an acceptable model performance as quantified by a pre-defined measure. This goal will also impact our exploratory data analysis. We will try to build some intuition about the data given our knowledge of the final goal. If we were given a data discovery task such as detecting interesting patterns via unsupervised learning methods we would probably perform a very different analysis.</p>
<p>For loans data the usual variable of interest is a delay or a default on required payments. So far we havenât looked at any default variable because, in fact, there is no one variable indicating it. One may speculate about the reasons but a plausible explanation may be that the definiton of default depends on the perspective. A risk-averse person may classify any delay in scheduled payments immediately as default from day one while others may apply a step-wise approach considering that borrowers may pay at a later stage. Yet another classification may look at any rating deterioration and include a default as last grade in a rating scale.</p>
<p>Letâs look at potential variables that may indicate a default / delay in payments:</p>
<ul>
<li>loan_status: Current status of the loan</li>
<li>delinq_2yrs: The number of 30+ days past-due incidences of delinquency in the borrowerâs credit file for the past 2 years</li>
<li>mths_since_last_delinq: The number of months since the borrowerâs last delinquency.</li>
</ul>
<p>We can look at the unique values of above variables by applying <code>base::unique()</code> via the <code>purrr::map()</code> function to each column of interest. The <code>purrr</code> library is part of the <code>tidyverse</code> set of libraries and applies functional paradigms via a level of abstraction.</p>
<pre class="r"><code>default_vars &lt;- c(&quot;loan_status&quot;, &quot;delinq_2yrs&quot;, &quot;mths_since_last_delinq&quot;)
purrr::map(.x = loans[, default_vars], .f = base::unique)</code></pre>
<pre><code>## $loan_status
##  [1] &quot;Fully Paid&quot;                                         
##  [2] &quot;Charged Off&quot;                                        
##  [3] &quot;Current&quot;                                            
##  [4] &quot;Default&quot;                                            
##  [5] &quot;Late (31-120 days)&quot;                                 
##  [6] &quot;In Grace Period&quot;                                    
##  [7] &quot;Late (16-30 days)&quot;                                  
##  [8] &quot;Does not meet the credit policy. Status:Fully Paid&quot; 
##  [9] &quot;Does not meet the credit policy. Status:Charged Off&quot;
## [10] &quot;Issued&quot;                                             
## 
## $delinq_2yrs
##  [1]  0  2  3  1  4  6  5  8  7  9 11 NA 13 15 10 12 17 18 29 24 14 21 22
## [24] 19 16 30 26 20 27 39
## 
## $mths_since_last_delinq
##   [1]   0  35  38  61   8  20  18  68  45  48  41  40  74  25  53  39  10
##  [18]  26  56  77  28  52  24  16  60  54  23   9  11  13  65  19  80  22
##  [35]  59  79  44  64  57  14  63  49  15  73  70  29  51   5  75  55   2
##  [52]  30  47  33  69   4  43  21  27  46  81  78  82  31  76  62  72  42
##  [69]  50   3  12  67  36  34  58  17  71  66  32   6  37   7   1  83  86
##  [86] 115  96 103 120 106  89 107  85  97  95 110  84 135  88  87 122  91
## [103] 146 134 114  99  93 127 101  94 102 129 113 139 131 156 143 109 119
## [120] 149 118 130 148 126  90 141 116 100 152  98  92 108 133 104 111 105
## [137] 170 124 136 180 188 140 151 159 121 123 157 112 154 171 142 125 117
## [154] 176 137</code></pre>
<p>We can see that <code>delinq_2yrs</code> shows only a few unique values which is a bit surprising as it could take many more values given its definition. The variable <code>mths_since_last_delinq</code> has some surprisingly large values. Both variables only indicate a delinquency in the past so they cannot help with the default definition.</p>
<p>The variable loan status seems to be an indicator of the current state a particular loan is in. We should also perform a count of the different values.</p>
<pre class="r"><code>loans %&gt;%
  group_by(loan_status) %&gt;%
  summarize(count = n(), rel_count = count/nrow(loans)) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">loan_status</th>
<th align="right">count</th>
<th align="right">rel_count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Charged Off</td>
<td align="right">45248</td>
<td align="right">0.0509906</td>
</tr>
<tr class="even">
<td align="left">Current</td>
<td align="right">601779</td>
<td align="right">0.6781533</td>
</tr>
<tr class="odd">
<td align="left">Default</td>
<td align="right">1219</td>
<td align="right">0.0013737</td>
</tr>
<tr class="even">
<td align="left">Does not meet the credit policy. Status:Charged Off</td>
<td align="right">761</td>
<td align="right">0.0008576</td>
</tr>
<tr class="odd">
<td align="left">Does not meet the credit policy. Status:Fully Paid</td>
<td align="right">1988</td>
<td align="right">0.0022403</td>
</tr>
<tr class="even">
<td align="left">Fully Paid</td>
<td align="right">207723</td>
<td align="right">0.2340860</td>
</tr>
<tr class="odd">
<td align="left">In Grace Period</td>
<td align="right">6253</td>
<td align="right">0.0070466</td>
</tr>
<tr class="even">
<td align="left">Issued</td>
<td align="right">8460</td>
<td align="right">0.0095337</td>
</tr>
<tr class="odd">
<td align="left">Late (16-30 days)</td>
<td align="right">2357</td>
<td align="right">0.0026561</td>
</tr>
<tr class="even">
<td align="left">Late (31-120 days)</td>
<td align="right">11591</td>
<td align="right">0.0130621</td>
</tr>
</tbody>
</table>
<p>It is not immediately obvious what the different values stand for, so we refer to Lending Clubâs documentation about â<a href="https://help.lendingclub.com/hc/en-us/articles/215488038-What-do-the-different-Note-statuses-mean-">What do the different Note statuses mean?</a>â</p>
<ul>
<li>Fully Paid: Loan has been fully repaid, either at the expiration of the 3- or 5-year year term or as a result of a prepayment.</li>
<li>Current: Loan is up to date on all outstanding payments.</li>
<li>Does not meet the credit policy. Status:Fully Paid: No explanation but see âfully paidâ.</li>
<li>Issued: New loan that has passed all Lending Club reviews, received full funding, and has been issued.</li>
<li>Charged Off: Loan for which there is no longer a reasonable expectation of further payments. Generally, Charge Off occurs no later than 30 days after the Default status is reached. Upon Charge Off, the remaining principal balance of the Note is deducted from the account balance. Learn more about the <a href="https://help.lendingclub.com/hc/en-us/articles/216127747">difference between âdefaultâ and âcharge offâ</a>.</li>
<li>Does not meet the credit policy. Status:Charged Off: No explanation but see âCharged Offâ</li>
<li>Late (31-120 days): Loan has not been current for 31 to 120 days.</li>
<li>In Grace Period: Loan is past due but within the 15-day grace period.</li>
<li>Late (16-30 days): Loan has not been current for 16 to 30 days.</li>
<li>Default: Loan has not been current for 121 days or more.</li>
</ul>
<p>Given above information, we will define a default as follows.</p>
<pre class="r"><code>defaulted &lt;- 
  c(&quot;Default&quot;, 
    &quot;Does not meet the credit policy. Status:Charged Off&quot;, 
    &quot;In Grace Period&quot;, 
    &quot;Late (16-30 days)&quot;, 
    &quot;Late (31-120 days)&quot;)</code></pre>
<p>We now have to add an indicator variable to the data. We do this by reassigning the mutated data to the original object. An alternative would be to update the object via a compound assignment pipe-operator from the magrittr package <code>magrittr::%&lt;&gt;%</code> or an assignment in place <code>:=</code> from the <code>data.table</code> package. We use a Boolean (True/False) indicator variable which will have nicer plotting properties (as it is treated like a character variable by the plotting library <code>ggplot2</code>) rather than a numerical value such as 1/0. R is usually clever enough to still allow calculations on Boolean values.</p>
<pre class="r"><code>loans &lt;-
  loans %&gt;%
  mutate(default = ifelse(!(loan_status %in% defaulted), FALSE, TRUE))</code></pre>
<p>Given our new indicator variable, we can now compute the frequency of actual defaults in the training set. It is around 0.0249961.</p>
<pre class="r"><code>loans %&gt;%
  summarise(default_freq = sum(default / n()))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   default_freq
##          &lt;dbl&gt;
## 1   0.02499608</code></pre>
<pre class="r"><code># alternatively in a table
table(loans$default) / nrow(loans)</code></pre>
<pre><code>## 
##      FALSE       TRUE 
## 0.97500392 0.02499608</code></pre>
<p></p>
</div>
<div id="removing-variables-deemed-unfit-for-most-modeling" class="section level1">
<h1><span class="header-section-number">7</span> Removing variables deemed unfit for most modeling</h1>
<p>As stated before some variables may actually have information value but are kicked out as we deem them unfit for most practical purposes. Arguably one would have to look at the actual value distribution as e.g.Â a high number of unique values may be non-sense values for only a few loans but we donât dig deeper here.</p>
<p>We can get rid of following variables with given reason</p>
<ul>
<li>annual_inc_joint: high NA ratio</li>
<li>dti_joint: high NA ratio</li>
<li>verification_status_joint: high NA ratio</li>
<li>policy_code: only one unique value -&gt; standard deviation = 0</li>
<li>id: key variable</li>
<li>member_id: key variable</li>
<li>emp_title: high amount of unique values</li>
<li>url: high amount of unique values</li>
<li>desc: high NA ratio</li>
<li>title: high amount of unique values</li>
<li>next_pymnt_d: high NA ratio</li>
<li>open_acc_6m: high NA ratio</li>
<li>open_il_6m: high NA ratio</li>
<li>open_il_12m: high NA ratio</li>
<li>open_il_24m: high NA ratio</li>
<li>mths_since_rcnt_il: high NA ratio</li>
<li>total_bal_il: high NA ratio</li>
<li>il_util: high NA ratio</li>
<li>open_rv_12m: high NA ratio</li>
<li>open_rv_24m: high NA ratio</li>
<li>max_bal_bc: high NA ratio</li>
<li>all_util: high NA ratio</li>
<li>total_rev_hi_lim: high NA ratio</li>
<li>inq_fi: high NA ratio</li>
<li>total_cu_tl: high NA ratio</li>
<li>inq_last_12m: high NA ratio</li>
</ul>
<pre class="r"><code>vars_to_remove &lt;- 
  c(&quot;annual_inc_joint&quot;, &quot;dti_joint&quot;, &quot;policy_code&quot;, &quot;id&quot;, &quot;member_id&quot;,
    &quot;emp_title&quot;, &quot;url&quot;, &quot;desc&quot;, &quot;title&quot;, &quot;open_acc_6m&quot;, &quot;open_il_6m&quot;, 
    &quot;open_il_12m&quot;, &quot;open_il_24m&quot;, &quot;mths_since_rcnt_il&quot;, &quot;total_bal_il&quot;, 
    &quot;il_util&quot;, &quot;open_rv_12m&quot;, &quot;open_rv_24m&quot;, &quot;max_bal_bc&quot;, &quot;all_util&quot;,
    &quot;total_rev_hi_lim&quot;, &quot;inq_fi&quot;, &quot;total_cu_tl&quot;, &quot;inq_last_12m&quot;,
    &quot;verification_status_joint&quot;, &quot;next_pymnt_d&quot;)

loans &lt;- loans %&gt;% select(-one_of(vars_to_remove))</code></pre>
<p>We further remove variables for different (stated) reasons</p>
<ul>
<li>sub_grade: contains same (but more granular) information as grade</li>
<li>loan_status: has been used to define target variable</li>
</ul>
<pre class="r"><code>vars_to_remove &lt;- 
  c(&quot;sub_grade&quot;, &quot;loan_status&quot;)

loans &lt;- loans %&gt;% select(-one_of(vars_to_remove))</code></pre>
<p></p>
</div>
<div id="a-note-on-hypothesis-generation-vs.hypothesis-confirmation" class="section level1">
<h1><span class="header-section-number">8</span> A note on hypothesis generation vs.Â hypothesis confirmation</h1>
<p>Before we start our analysis, we should be clear about its aim and what the data is used for. In the book âR for Data Scienceâ the authors put it quite nicely under the name <a href="http://r4ds.had.co.nz/model-intro.html">Hypothesis generation vs.Â hypothesis confirmation</a>:</p>
<blockquote>
<ol style="list-style-type: decimal">
<li>Each observation can either be used for exploration or confirmation, not both.</li>
<li>You can use an observation as many times as you like for exploration, but you can only use it once for confirmation. As soon as you use an observation twice, youâve switched from confirmation to exploration. This is necessary because to confirm a hypothesis you must use data independent of the data that you used to generate the hypothesis. Otherwise you will be over optimistic. There is absolutely nothing wrong with exploration, but you should never sell an exploratory analysis as a confirmatory analysis because it is fundamentally misleading.</li>
</ol>
</blockquote>
<p>In a strict sense, this requires us to split the data into different sets. The authors go on suggesting a split:</p>
<blockquote>
<ol style="list-style-type: decimal">
<li>60% of your data goes into a training (or exploration) set. Youâre allowed to do anything you like with this data: visualise it and fit tons of models to it.</li>
<li>20% goes into a query set. You can use this data to compare models or visualisations by hand, but youâre not allowed to use it as part of an automated process.</li>
<li>20% is held back for a test set. You can only use this data ONCE, to test your final model.</li>
</ol>
</blockquote>
<p>This means that even for exploratory data analysis (EDA), we would only look at parts of the data. We will split the data into two sets with 80% train and 20% test ratio at random. As we are dealing with time-series data, we could also split the data by time. But time itself may be an explanatory variable which could be modeled. All exploratory analysis will be performed on the training data only. We will use <code>base::set.seed()</code> to make the random split reproducible. You can have an argument whether it is sensible to even use split data for EDA but EDA usually builds your intuition about the data and thus will shape data transformation and model decisions. The test data allows for testing all these assumptions in addition to the actual model performance. There are other methods such as cross validation which do not necessarily require a test data set but we have enough observations to afford one.</p>
<p>One note of caution is necessary here. Since not all data is used for model fitting, the test data may have labels that do not occur in the training set and with same rationale feautures may have unseen values. In addition, the data is imbalanced, i.e.Â only a few lenders default while many more do not. The last fact may actually require a non-random split considering the class label (default / non-default). The same may hold true for the features (independent variables). For more details on dealing with imbalanced data, see <a href="https://www.svds.com/learning-imbalanced-classes/">Learning from Imbalanced Classes</a> or <a href="http://www.ele.uri.edu/faculty/he/PDFfiles/ImbalancedLearning.pdf">Learning from Imbalanced Data</a>. Tom Fawcett puts it nicely in previous first mentioned blog post:</p>
<blockquote>
<p>Conventional algorithms are often biased towards the majority class because their loss functions attempt to optimize quantities such as error rate, not taking the data distribution into consideration. In the worst case, minority examples are treated as outliers of the majority class and ignored. The learning algorithm simply generates a trivial classifier that classifies every example as the majority class.</p>
</blockquote>
<p>We can do the split manually (see commented out code) but in order to ensure class distributions within the split data, we use function <code>createDataPartition()</code> from the <code>caret</code> package which performs stratified sampling. For details on the function, see <a href="https://topepo.github.io/caret/data-splitting.html">The caret package: 4.1 Simple Splitting Based on the Outcome</a>.</p>
<pre class="r"><code># ## manual approach
# # 80% of the sample size
# smp_size &lt;- floor(0.8 * nrow(loans))
# 
# # set the seed to make your partition reproductible
# set.seed(123)
# train_index &lt;- sample(seq_len(nrow(loans)), size = smp_size)
# 
# train &lt;- loans[train_index, ]
# test &lt;- loans[-train_index, ]

## with caret
set.seed(6438)

train_index &lt;- 
  caret::createDataPartition(y = loans$default, times = 1, 
                             p = .8, list = FALSE)

train &lt;- loans[train_index, ]
test &lt;- loans[-train_index, ]</code></pre>
<p></p>
</div>
<div id="exploratory-data-analysis" class="section level1">
<h1><span class="header-section-number">9</span> Exploratory Data Analysis</h1>
<p>There are many excellent resources on exploratory analysis, e.g.</p>
<ul>
<li><a href="http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2009.00001.x/full">A protocol for data exploration to avoid common statistical problems</a></li>
</ul>
<p>A visual look at the data should always precede any model considerations. A useful visualization library is <code>ggplot2</code> (which is part of the <code>tidyverse</code> and further on also referred to as ggplot) that requires a few other libraries on top for extensions such as <code>scales</code>. We are not after publication-ready visualizations yet as this phase is considered data exploration or data discovery rather than results reporting. Nontheless, used visualization libraries already produce visually appealing graphics as they use smart heuristics and default values to guess sensible parameter settings.</p>
<pre class="r"><code>library(scales)</code></pre>
<p>The most important questions around visualization are which variables are numeric and if so are they continous or discrete and which are strings. Furthermore, which variables are attributes (categorical) and which make up sensible metric-attribute pairs. An important information for efficient visualization with categorical variables is also the amount of unique values they can take and the ratio of zero or missing values, both which were already analyzed in section <a href="#meta-data">Meta Data</a>. In a first exploratory analysis we aim for categorical variables that have high information power and not too many unique values to keep the information density at a manageable level. Also consider group sizes and differences between median and mean driven by outliers. Especially when drawing conclusions from summarized / aggregated information, we should be aware of group size. We thus may add this info directly in the plot or look at it before plotting in a grouped table.</p>
<p>A generally good idea is to look at the distributions of relevant continous variables. These are probably</p>
<ul>
<li>loan_amnt</li>
<li>funded_amnt</li>
<li>funded_amnt_inv</li>
<li>annual_inc</li>
</ul>
<p>Among the target variable <code>default</code> some interesting categorical variables might be</p>
<ul>
<li>term</li>
<li>int_rate</li>
<li>grade</li>
<li>emp_title</li>
<li>home_ownership</li>
<li>verification_status</li>
<li>issue_d (timestamp)</li>
<li>loan_status</li>
<li>purpose</li>
<li>zip_code (geo-info)</li>
<li>addr_state (geo-info)</li>
<li>application_type</li>
</ul>
<p>Assign continous variables to a character vector and reshape data for plotting of distributions.</p>
<pre class="r"><code>income_vars &lt;- c(&quot;annual_inc&quot;)
loan_amount_vars &lt;- c(&quot;loan_amnt&quot;, &quot;funded_amnt&quot;, &quot;funded_amnt_inv&quot;)</code></pre>
<p>We can reshape and plot the original data for specified variables in a tidyr-dplyr-ggplot pipe. For details on tidying with tidyr, see <a href="http://garrettgman.github.io/tidying/">Data Science with R - Data Tidying</a> or <a href="https://rpubs.com/bradleyboehmke/data_wrangling">Data Processing with dplyr &amp; tidyr</a>.</p>
<pre class="r"><code>train %&gt;%
  select_(.dots = income_vars) %&gt;%
  gather_(&quot;variable&quot;, &quot;value&quot;, gather_cols = income_vars) %&gt;%
  ggplot(aes(x = value)) +
  facet_wrap(~ variable, scales = &quot;free_x&quot;, ncol = 3) +
  geom_histogram()</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<pre><code>## Warning: Removed 4 rows containing non-finite values (stat_bin).</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>We can see that a lot of loans have corresponding annual income of zero and in general income seems low. As already known, joint income has a large number of NA values (i.e.Â cannot be displayed) and those few values that are present do not seem to have significant exposure. Most loan applications must have been submitted by single income borrowers, i.e.Â either single persons or single-income households.</p>
<pre class="r"><code>train %&gt;%
  select_(.dots = loan_amount_vars) %&gt;%
  gather_(&quot;variable&quot;, &quot;value&quot;, gather_cols = loan_amount_vars) %&gt;%
  ggplot(aes(x = value)) +
  facet_wrap(~ variable, scales = &quot;free_x&quot;, ncol = 3) +
  geom_histogram()</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>The loan amount distributions seems similar in shape suggesting not too much divergence between the loan amount applied for, the amount committed and the amount invested.</p>
<p>We had already identified a number of interesting categorical variables. Letâs combine our selection with the meta information gathered in an earlier stage to see the information power and uniqueness.</p>
<pre class="r"><code>categorical_vars &lt;- 
  c(&quot;term&quot;, &quot;grade&quot;, &quot;sub_grade&quot;, &quot;emp_title&quot;, &quot;home_ownership&quot;,
    &quot;verification_status&quot;, &quot;loan_status&quot;, &quot;purpose&quot;, &quot;zip_code&quot;,
    &quot;addr_state&quot;, &quot;application_type&quot;, &quot;policy_code&quot;)

meta_loans %&gt;%
  select(variable, p_zeros, p_na, type, unique) %&gt;%
  filter_(~ variable %in% categorical_vars) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">variable</th>
<th align="right">p_zeros</th>
<th align="right">p_na</th>
<th align="left">type</th>
<th align="right">unique</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">term</td>
<td align="right">0</td>
<td align="right">0.0</td>
<td align="left">character</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="left">grade</td>
<td align="right">0</td>
<td align="right">0.0</td>
<td align="left">character</td>
<td align="right">7</td>
</tr>
<tr class="odd">
<td align="left">sub_grade</td>
<td align="right">0</td>
<td align="right">0.0</td>
<td align="left">character</td>
<td align="right">35</td>
</tr>
<tr class="even">
<td align="left">emp_title</td>
<td align="right">0</td>
<td align="right">5.8</td>
<td align="left">character</td>
<td align="right">289148</td>
</tr>
<tr class="odd">
<td align="left">home_ownership</td>
<td align="right">0</td>
<td align="right">0.0</td>
<td align="left">character</td>
<td align="right">6</td>
</tr>
<tr class="even">
<td align="left">verification_status</td>
<td align="right">0</td>
<td align="right">0.0</td>
<td align="left">character</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="left">loan_status</td>
<td align="right">0</td>
<td align="right">0.0</td>
<td align="left">character</td>
<td align="right">10</td>
</tr>
<tr class="even">
<td align="left">purpose</td>
<td align="right">0</td>
<td align="right">0.0</td>
<td align="left">character</td>
<td align="right">14</td>
</tr>
<tr class="odd">
<td align="left">zip_code</td>
<td align="right">0</td>
<td align="right">0.0</td>
<td align="left">character</td>
<td align="right">935</td>
</tr>
<tr class="even">
<td align="left">addr_state</td>
<td align="right">0</td>
<td align="right">0.0</td>
<td align="left">character</td>
<td align="right">51</td>
</tr>
<tr class="odd">
<td align="left">policy_code</td>
<td align="right">0</td>
<td align="right">0.0</td>
<td align="left">numeric</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">application_type</td>
<td align="right">0</td>
<td align="right">0.0</td>
<td align="left">character</td>
<td align="right">2</td>
</tr>
</tbody>
</table>
<p>This gives us some ideas on what may be useful for a broad data exploration. Variables such as <code>emp_title</code> have too many unique values to be suitable for a classical categorical graph. Other variables may lend themselves to pairwise or correlation graphs such as <code>int_rate</code> while again others may be used in time series plots such as <code>issue_d</code>. We may even be able to plot some appealing geographical plots with geolocation variables such as <code>zip_code</code> or <code>addr_state</code>.</p>
<div id="grade" class="section level2">
<h2><span class="header-section-number">9.1</span> Grade</h2>
<p>For a start, letâs look at grade which seems to be a rating classification scheme that Lending Club uses to assign loans into risk buckets similar to other popular rating schemes like S&amp;P or Moodys. For more details, see <a href="https://www.lendingclub.com/foliofn/rateDetail.action">Lending Club Rate Information</a>. For now, it suffices to know that grades take values A, B, C, D, E, F, G where A represents the highest quality loan and G the lowest.</p>
<p>As a first relation, we investigate the distribution of loan amount over the different grades with a standard <a href="https://en.wikipedia.org/wiki/Box_plot">boxplot</a> highlighting potential outliers in red. One may also select a pre-defined theme from the <code>ggthemes</code> package by adding a call to ggplot such as <code>theme_economist()</code> or even create a theme oneself. For details on themes, see <a href="http://docs.ggplot2.org/dev/vignettes/themes.html">ggplot2 themes</a> and <a href="https://cran.r-project.org/web/packages/ggthemes/vignettes/ggthemes.html">Introduction to ggthemes</a>.</p>
<p>As mentioned before we want to be group size aware so letâs write a short function to be used inside the <code>ggplot::geom_boxplot()</code> call in combination with <code>ggplot::stat_summary()</code> where we can call user-defined functions using parameter <code>fun.data</code>. As it is often the case with plots, we need to experiment with the position of additional text elements which we achieve by scaling the y-position with some constant multiplier around the median (as the boxplot will have the median as horizontal bar within the rectangles). The mean can be added in a similar fashion, however we donât need to specify the y-position explicitly. The count as text and the mean as point may overlap themselves and with the median horizontal bar. This is acceptable in an exploratory setting. For publication-ready plots one would have to perform some adjustments.</p>
<pre class="r"><code># see http://stackoverflow.com/questions/15660829/how-to-add-a-number-of-observations-per-group-and-use-group-mean-in-ggplot2-boxp
give_count &lt;- 
  stat_summary(fun.data = function(x) return(c(y = median(x)*1.06,
                                               label = length(x))),
               geom = &quot;text&quot;)

# see http://stackoverflow.com/questions/19876505/boxplot-show-the-value-of-mean
give_mean &lt;- 
  stat_summary(fun.y = mean, colour = &quot;darkgreen&quot;, geom = &quot;point&quot;, 
               shape = 18, size = 3, show.legend = FALSE)</code></pre>
<pre class="r"><code>train %&gt;%
  ggplot(aes(grade, loan_amnt)) +
  geom_boxplot(fill = &quot;white&quot;, colour = &quot;darkblue&quot;, 
               outlier.colour = &quot;red&quot;, outlier.shape = 1) +
  give_count +
  give_mean +
  scale_y_continuous(labels = comma) +
  facet_wrap(~ default) +
  labs(title=&quot;Loan Amount by Grade&quot;, x = &quot;Grade&quot;, y = &quot;Loan Amount \n&quot;)</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>We can derive a few points from the plot:</p>
<ul>
<li>there is not a lot of difference between default and non-default</li>
<li>lower quality loans tend to have a higher loan amount</li>
<li>there are virtually no outliers except for grade B</li>
<li>the loan amount spread (IQR) seems to be slightly higher for lower quality loans</li>
</ul>
<p>According to Lending Clubâs rate information, we would expect a strong increasing relationship between grade and interest rate so we can test this. We also consider the term of the loan as one might expect that longer term loans could have a higher interest rate.</p>
<pre class="r"><code>train %&gt;%
  ggplot(aes(grade, int_rate)) +
  geom_boxplot(fill = &quot;white&quot;, colour = &quot;darkblue&quot;, 
               outlier.colour = &quot;red&quot;, outlier.shape = 1) +
  give_count +
  give_mean +
  scale_y_continuous(labels = comma) +
  labs(title=&quot;Interest Rate by Grade&quot;, x = &quot;Grade&quot;, y = &quot;Interest Rate \n&quot;) +
  facet_wrap(~ term)</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>We can derive a few points from the plot:</p>
<ul>
<li>interest rate increases with grade worsening</li>
<li>a few loans seem to have an equally low interest rate independent of grade</li>
<li>the spread of rates seems to increase with grade worsening</li>
<li>there tend to be more outliers on the lower end of the rate</li>
<li>The 3-year term has a much higher number of high-rated borrowers while the 5-year term has a larger number in the low-rating grades</li>
</ul>
</div>
<div id="home-ownership" class="section level2">
<h2><span class="header-section-number">9.2</span> Home Ownership</h2>
<p>We can do the same for home ownership.</p>
<pre class="r"><code>train %&gt;%
  ggplot(aes(home_ownership, int_rate)) +
  geom_boxplot(fill = &quot;white&quot;, colour = &quot;darkblue&quot;, 
               outlier.colour = &quot;red&quot;, outlier.shape = 1) +
  give_count +
  give_mean +
  scale_y_continuous(labels = comma) +
  facet_wrap(~ default) +
  labs(title=&quot;Interest Rate by Home Ownership&quot;, x = &quot;Home Ownership&quot;, y = &quot;Interest Rate \n&quot;)</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>We can derive a few points from the plot:</p>
<ul>
<li>there seems no immediate conclusion with respect to the impact of home ownership, e.g.Â we can see that median/mean interest rate is higher for people who own a house than for those who still pay a mortgage.</li>
<li>interest rates are highest for values âanyâ and ânoneâ which could be loans of limited data quality but there are very few data points.</li>
<li>interest rate is higher for default loans, which is probably driven by other factors (e.g.Â grade)</li>
</ul>
</div>
<div id="loan-amount-and-income" class="section level2">
<h2><span class="header-section-number">9.3</span> Loan Amount and Income</h2>
<p>Another interesting plot may be the relationship between loan amount and funded / invested amount. As all variables are continous, we can do that with a simple <a href="https://en.wikipedia.org/wiki/Scatter_plot">scatterplot</a> but we will need to reshape the data to have both loan values plotted against loan amount.</p>
<p>In fact the reshaping here may be slightly odd as we like to display the same variable on the x-axis but different values on the y-axis facetted by their variable names. To achieve that, we gather the data three times with the same x variable but changing y variables.</p>
<pre class="r"><code>funded_amnt &lt;-
  train %&gt;%
  transmute(loan_amnt = loan_amnt, value = funded_amnt, 
              variable = &quot;funded_amnt&quot;)

funded_amnt_inv &lt;-
  train %&gt;%
  transmute(loan_amnt = loan_amnt, value = funded_amnt_inv, 
              variable = &quot;funded_amnt_inv&quot;)

plot_data &lt;- rbind(funded_amnt, funded_amnt_inv)
# remove unnecessary data using regex
ls()</code></pre>
<pre><code>##  [1] &quot;categorical_vars&quot;  &quot;chr_to_date_vars&quot;  &quot;chr_to_num_vars&quot;  
##  [4] &quot;convert_date&quot;      &quot;default_vars&quot;      &quot;defaulted&quot;        
##  [7] &quot;excel_file&quot;        &quot;funded_amnt&quot;       &quot;funded_amnt_inv&quot;  
## [10] &quot;give_count&quot;        &quot;give_mean&quot;         &quot;i&quot;                
## [13] &quot;income_vars&quot;       &quot;libraries_missing&quot; &quot;libraries_used&quot;   
## [16] &quot;loan_amount_vars&quot;  &quot;loans&quot;             &quot;meta_browse_notes&quot;
## [19] &quot;meta_loan_stats&quot;   &quot;meta_loans&quot;        &quot;meta_reject_stats&quot;
## [22] &quot;na_to_zero_vars&quot;   &quot;num_vars&quot;          &quot;path&quot;             
## [25] &quot;plot_data&quot;         &quot;test&quot;              &quot;train&quot;            
## [28] &quot;train_index&quot;       &quot;vars_to_remove&quot;</code></pre>
<pre class="r"><code>rm(list = ls()[grep(&quot;^funded&quot;, ls())])</code></pre>
<p>Now letâs plot the data.</p>
<pre class="r"><code>plot_data %&gt;%
  ggplot(aes(x = loan_amnt, y = value)) +
  facet_wrap(~ variable, scales = &quot;free_x&quot;, ncol = 3) +
  geom_point()</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<p>We can derive a few points from the plot:</p>
<ul>
<li>there are instances when funded amount is smaller loan amount</li>
<li>there seems to be a number of loans where investment is smaller funded amount i.e.Â not the full loan is invested in</li>
</ul>
<p>Letâs do the same but only for annual income versus loan amount.</p>
<pre class="r"><code>train %&gt;%
  ggplot(aes(x = annual_inc, y = loan_amnt)) +
  geom_point()</code></pre>
<pre><code>## Warning: Removed 4 rows containing missing values (geom_point).</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<p>We can derive a few points from the plot:</p>
<ul>
<li>there is no immediatly discernible relationship</li>
<li>there are quite a few income outliers with questionable values (e.g.Â why would a person with annual income 9500000 request a loan amount of 24000)</li>
</ul>
</div>
<div id="time-series" class="section level2">
<h2><span class="header-section-number">9.4</span> Time Series</h2>
<p>Now letâs take a look at interest rates over time but split the time series by grade to see if there are differences in interest rate development depending on the borrower grade. Again we make use of a dplyr pipe, first selecting the variables of interest, then grouping by attributes and finally summarising the metric interest rate by taking the mean for each goup. We use <code>facet_wrap</code> to split by attribute grade. As we are using the mean for building an aggregate representation, we should be weary of outliers and group size which we had already looked at earlier (ignoring time dimension).</p>
<pre class="r"><code>train %&gt;%
  select(int_rate, grade) %&gt;%
  group_by(grade) %&gt;%
  summarise(int_rate_mean = mean(int_rate, na.rm = TRUE),
            int_rate_median = median(int_rate, na.rm = TRUE),
            n = n()) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">grade</th>
<th align="right">int_rate_mean</th>
<th align="right">int_rate_median</th>
<th align="right">n</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">A</td>
<td align="right">7.244639</td>
<td align="right">7.26</td>
<td align="right">118281</td>
</tr>
<tr class="even">
<td align="left">B</td>
<td align="right">10.830884</td>
<td align="right">10.99</td>
<td align="right">203741</td>
</tr>
<tr class="odd">
<td align="left">C</td>
<td align="right">13.979427</td>
<td align="right">13.98</td>
<td align="right">196815</td>
</tr>
<tr class="even">
<td align="left">D</td>
<td align="right">17.178566</td>
<td align="right">16.99</td>
<td align="right">111490</td>
</tr>
<tr class="odd">
<td align="left">E</td>
<td align="right">19.899484</td>
<td align="right">19.99</td>
<td align="right">56765</td>
</tr>
<tr class="even">
<td align="left">F</td>
<td align="right">23.572306</td>
<td align="right">23.76</td>
<td align="right">18454</td>
</tr>
<tr class="odd">
<td align="left">G</td>
<td align="right">25.624502</td>
<td align="right">25.83</td>
<td align="right">4358</td>
</tr>
</tbody>
</table>
<p>Group size are relatively large except for grade G which may have only a few hundred loans per group when grouping by grade and date. Mean and median seem fairly close at this non-granular level. Letâs plot the data.</p>
<pre class="r"><code>train %&gt;%
  select(int_rate, grade, issue_d) %&gt;%
  group_by(grade, issue_d) %&gt;%
  summarise(int_rate_mean = mean(int_rate, na.rm = TRUE)) %&gt;%
  ggplot(aes(issue_d, int_rate_mean)) +
  geom_line(color= &quot;darkblue&quot;, size = 1) +
  facet_wrap(~ grade)</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<p>We can derive a few points from the plot:</p>
<ul>
<li>the mean interest rate is falling or relatively constant for high-rated clients</li>
<li>the mean interest rate is increasing significantly for low-rated clients</li>
</ul>
<p>Letâs also look at loan amounts over time in the same manner.</p>
<pre class="r"><code>train %&gt;%
  select(loan_amnt, grade, issue_d) %&gt;%
  group_by(grade, issue_d) %&gt;%
  summarise(loan_amnt_mean = mean(loan_amnt, na.rm = TRUE)) %&gt;%
  ggplot(aes(issue_d, loan_amnt_mean)) +
  geom_line(color= &quot;darkblue&quot;, size = 1) +
  facet_wrap(~ grade)</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<p>We can derive a few points from the plot:</p>
<ul>
<li>the mean loan amount is increasing for all grades</li>
<li>while high-rated clients have some mean loan amount volatility, it is much higher for low-rated clients</li>
</ul>
</div>
<div id="geolocation-plots" class="section level2">
<h2><span class="header-section-number">9.5</span> Geolocation Plots</h2>
<p>Letâs remind ourselves of the geolocation variables in the data and their information power.</p>
<ul>
<li>zip_code (geo-info)</li>
<li>addr_state (geo-info)</li>
</ul>
<pre class="r"><code>geo_vars &lt;- c(&quot;zip_code&quot;, &quot;addr_state&quot;)

meta_loans %&gt;%
  select(variable, p_zeros, p_na, type, unique) %&gt;%
  filter_(~ variable %in% geo_vars) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">variable</th>
<th align="right">p_zeros</th>
<th align="right">p_na</th>
<th align="left">type</th>
<th align="right">unique</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">zip_code</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">935</td>
</tr>
<tr class="even">
<td align="left">addr_state</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">character</td>
<td align="right">51</td>
</tr>
</tbody>
</table>
<pre class="r"><code>loans %&gt;%
  select_(.dots = geo_vars) %&gt;%
  str()</code></pre>
<pre><code>## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:    887379 obs. of  2 variables:
##  $ zip_code  : chr  &quot;860xx&quot; &quot;309xx&quot; &quot;606xx&quot; &quot;917xx&quot; ...
##  $ addr_state: chr  &quot;AZ&quot; &quot;GA&quot; &quot;IL&quot; &quot;CA&quot; ...</code></pre>
<p>We see that <code>zip_code</code> seems to be the truncated US postal code with only first three digits having a value. The <code>addr_state</code> seems to be the state names in a two-letter abbreviation.</p>
<p>We can use the <code>choroplethr</code> package to work with maps (alternatives may be <code>maps</code> among other packages). While <code>choroplethr</code> provides functions to work with the data, its sister package <code>choroplethrMaps</code> contains corresponding maps that can be used by <code>choroplethr</code>. One issue with <code>choroplethr</code> is that it attaches the package <code>plyr</code> which means many functions from <code>dplyr</code> are masked as it is the successor to <code>plyr</code>. Thus we do not load the package <code>choroplethr</code> despite using its functions frequently. However, we need to load <code>choroplethrMaps</code> as it has some data that is not exported from its namespace so syntax like <code>choroplethrMaps::state.map</code> where <code>state.map</code> is the rdata file does not work. An alternative might be to load the file directly by going to the underlying directory, e.g. <code>R-3.x.x\library\choroplethrMaps\data</code> but this is cumbersome. As discussed before, when attaching a library, we just need to make sure that important functions of other packages are not masked. In this case itâs fine.</p>
<p>For details on those libraries, see <a href="https://cran.r-project.org/web/packages/choroplethr">CRAN choroplethr</a> and <a href="https://cran.r-project.org/web/packages/choroplethrMaps">CRAN choroplethrMaps</a>.</p>
<p>First we aggregate the default rate by state.</p>
<pre class="r"><code>default_rate_state &lt;- 
  train %&gt;%
  select(default, addr_state) %&gt;%
  group_by(addr_state) %&gt;%
  summarise(default_rate = sum(default, na.rm = TRUE) / n())

knitr::kable(default_rate_state)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">addr_state</th>
<th align="right">default_rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">AK</td>
<td align="right">0.0289280</td>
</tr>
<tr class="even">
<td align="left">AL</td>
<td align="right">0.0300944</td>
</tr>
<tr class="odd">
<td align="left">AR</td>
<td align="right">0.0243994</td>
</tr>
<tr class="even">
<td align="left">AZ</td>
<td align="right">0.0260203</td>
</tr>
<tr class="odd">
<td align="left">CA</td>
<td align="right">0.0247008</td>
</tr>
<tr class="even">
<td align="left">CO</td>
<td align="right">0.0233654</td>
</tr>
<tr class="odd">
<td align="left">CT</td>
<td align="right">0.0222655</td>
</tr>
<tr class="even">
<td align="left">DC</td>
<td align="right">0.0131646</td>
</tr>
<tr class="odd">
<td align="left">DE</td>
<td align="right">0.0291604</td>
</tr>
<tr class="even">
<td align="left">FL</td>
<td align="right">0.0267678</td>
</tr>
<tr class="odd">
<td align="left">GA</td>
<td align="right">0.0238065</td>
</tr>
<tr class="even">
<td align="left">HI</td>
<td align="right">0.0303441</td>
</tr>
<tr class="odd">
<td align="left">IA</td>
<td align="right">0.2000000</td>
</tr>
<tr class="even">
<td align="left">ID</td>
<td align="right">0.0000000</td>
</tr>
<tr class="odd">
<td align="left">IL</td>
<td align="right">0.0201743</td>
</tr>
<tr class="even">
<td align="left">IN</td>
<td align="right">0.0242243</td>
</tr>
<tr class="odd">
<td align="left">KS</td>
<td align="right">0.0162975</td>
</tr>
<tr class="even">
<td align="left">KY</td>
<td align="right">0.0204172</td>
</tr>
<tr class="odd">
<td align="left">LA</td>
<td align="right">0.0268172</td>
</tr>
<tr class="even">
<td align="left">MA</td>
<td align="right">0.0246096</td>
</tr>
<tr class="odd">
<td align="left">MD</td>
<td align="right">0.0279492</td>
</tr>
<tr class="even">
<td align="left">ME</td>
<td align="right">0.0023474</td>
</tr>
<tr class="odd">
<td align="left">MI</td>
<td align="right">0.0249429</td>
</tr>
<tr class="even">
<td align="left">MN</td>
<td align="right">0.0239122</td>
</tr>
<tr class="odd">
<td align="left">MO</td>
<td align="right">0.0222711</td>
</tr>
<tr class="even">
<td align="left">MS</td>
<td align="right">0.0301540</td>
</tr>
<tr class="odd">
<td align="left">MT</td>
<td align="right">0.0289296</td>
</tr>
<tr class="even">
<td align="left">NC</td>
<td align="right">0.0263557</td>
</tr>
<tr class="odd">
<td align="left">ND</td>
<td align="right">0.0075377</td>
</tr>
<tr class="even">
<td align="left">NE</td>
<td align="right">0.0156087</td>
</tr>
<tr class="odd">
<td align="left">NH</td>
<td align="right">0.0206490</td>
</tr>
<tr class="even">
<td align="left">NJ</td>
<td align="right">0.0249566</td>
</tr>
<tr class="odd">
<td align="left">NM</td>
<td align="right">0.0264766</td>
</tr>
<tr class="even">
<td align="left">NV</td>
<td align="right">0.0303213</td>
</tr>
<tr class="odd">
<td align="left">NY</td>
<td align="right">0.0294889</td>
</tr>
<tr class="even">
<td align="left">OH</td>
<td align="right">0.0225960</td>
</tr>
<tr class="odd">
<td align="left">OK</td>
<td align="right">0.0283897</td>
</tr>
<tr class="even">
<td align="left">OR</td>
<td align="right">0.0209814</td>
</tr>
<tr class="odd">
<td align="left">PA</td>
<td align="right">0.0256838</td>
</tr>
<tr class="even">
<td align="left">RI</td>
<td align="right">0.0245849</td>
</tr>
<tr class="odd">
<td align="left">SC</td>
<td align="right">0.0218611</td>
</tr>
<tr class="even">
<td align="left">SD</td>
<td align="right">0.0301164</td>
</tr>
<tr class="odd">
<td align="left">TN</td>
<td align="right">0.0267720</td>
</tr>
<tr class="even">
<td align="left">TX</td>
<td align="right">0.0240810</td>
</tr>
<tr class="odd">
<td align="left">UT</td>
<td align="right">0.0250696</td>
</tr>
<tr class="even">
<td align="left">VA</td>
<td align="right">0.0279697</td>
</tr>
<tr class="odd">
<td align="left">VT</td>
<td align="right">0.0172771</td>
</tr>
<tr class="even">
<td align="left">WA</td>
<td align="right">0.0216223</td>
</tr>
<tr class="odd">
<td align="left">WI</td>
<td align="right">0.0213508</td>
</tr>
<tr class="even">
<td align="left">WV</td>
<td align="right">0.0225434</td>
</tr>
<tr class="odd">
<td align="left">WY</td>
<td align="right">0.0179455</td>
</tr>
</tbody>
</table>
<p>The data is already in a good format but when using <code>choroplethr</code> we need to adhere to some conventions to make it work out of the box. The first thing would be to bring the state names into a standard format. In fact, we can investigate the format by looking e.g.Â into the <code>choroplethrMaps::state.map</code> dataset which we later use to map our data. We first load the library and then the data via the function <code>utils::data()</code>. From the documentation, we also learn that state.map is a âdata.frame which contains a map of all 50 US States plus the District of Columbia.â It is based on a <a href="https://en.wikipedia.org/wiki/Shapefile">shapefile</a> which is often used in geospatial visualizations and âtaken from the US Census 2010 Cartographic Boundary shapefiles pageâ. We are interested in the <code>region</code> variable which seems to hold the state names.</p>
<pre class="r"><code>library(choroplethrMaps)
utils::data(state.map)
str(state.map)</code></pre>
<pre><code>## &#39;data.frame&#39;:    50763 obs. of  12 variables:
##  $ long      : num  -113 -113 -112 -112 -111 ...
##  $ lat       : num  37 37 37 37 37 ...
##  $ order     : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ hole      : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ piece     : Factor w/ 66 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ group     : Factor w/ 226 levels &quot;0.1&quot;,&quot;1.1&quot;,&quot;10.1&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ id        : chr  &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ...
##  $ GEO_ID    : Factor w/ 51 levels &quot;0400000US01&quot;,..: 3 3 3 3 3 3 3 3 3 3 ...
##  $ STATE     : Factor w/ 51 levels &quot;01&quot;,&quot;02&quot;,&quot;04&quot;,..: 3 3 3 3 3 3 3 3 3 3 ...
##  $ region    : chr  &quot;arizona&quot; &quot;arizona&quot; &quot;arizona&quot; &quot;arizona&quot; ...
##  $ LSAD      : Factor w/ 0 levels: NA NA NA NA NA NA NA NA NA NA ...
##  $ CENSUSAREA: num  113594 113594 113594 113594 113594 ...</code></pre>
<pre class="r"><code>unique(state.map$region)</code></pre>
<pre><code>##  [1] &quot;arizona&quot;              &quot;arkansas&quot;             &quot;louisiana&quot;           
##  [4] &quot;minnesota&quot;            &quot;mississippi&quot;          &quot;montana&quot;             
##  [7] &quot;new mexico&quot;           &quot;north dakota&quot;         &quot;oklahoma&quot;            
## [10] &quot;pennsylvania&quot;         &quot;tennessee&quot;            &quot;virginia&quot;            
## [13] &quot;california&quot;           &quot;delaware&quot;             &quot;west virginia&quot;       
## [16] &quot;wisconsin&quot;            &quot;wyoming&quot;              &quot;alabama&quot;             
## [19] &quot;alaska&quot;               &quot;florida&quot;              &quot;idaho&quot;               
## [22] &quot;kansas&quot;               &quot;maryland&quot;             &quot;colorado&quot;            
## [25] &quot;new jersey&quot;           &quot;north carolina&quot;       &quot;south carolina&quot;      
## [28] &quot;washington&quot;           &quot;vermont&quot;              &quot;utah&quot;                
## [31] &quot;iowa&quot;                 &quot;kentucky&quot;             &quot;maine&quot;               
## [34] &quot;massachusetts&quot;        &quot;connecticut&quot;          &quot;michigan&quot;            
## [37] &quot;missouri&quot;             &quot;nebraska&quot;             &quot;nevada&quot;              
## [40] &quot;new hampshire&quot;        &quot;new york&quot;             &quot;ohio&quot;                
## [43] &quot;oregon&quot;               &quot;rhode island&quot;         &quot;south dakota&quot;        
## [46] &quot;district of columbia&quot; &quot;texas&quot;                &quot;georgia&quot;             
## [49] &quot;hawaii&quot;               &quot;illinois&quot;             &quot;indiana&quot;</code></pre>
<p>So it seems we need small case long form state names which implies we need a mapping from the short names in our data. Some internet research may give us the site <a href="https://www.census.gov/geo/reference/ansi_statetables.html">American National Standards Institute (ANSI) Codes for States</a> where we find a mapping under section <code>FIPS Codes for the States and the District of Columbia</code>. We could try using some parsing tool to extract the table from the web page but for now we take a short cut and simply copy paste the data into a tab-delimited txt file and read the file with <code>readr::read_tsv()</code>. We then cross-check if all abbreviations used in our loans data are present in the newly created mapping table.</p>
<pre class="r"><code>states &lt;- readr::read_tsv(&quot;./data//us_states.txt&quot;)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   Name = col_character(),
##   `FIPS State Numeric Code` = col_integer(),
##   `Official USPS Code` = col_character()
## )</code></pre>
<pre class="r"><code>str(states)</code></pre>
<pre><code>## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:    51 obs. of  3 variables:
##  $ Name                   : chr  &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ...
##  $ FIPS State Numeric Code: int  1 2 4 5 6 8 9 10 11 12 ...
##  $ Official USPS Code     : chr  &quot;AL&quot; &quot;AK&quot; &quot;AZ&quot; &quot;AR&quot; ...
##  - attr(*, &quot;spec&quot;)=List of 2
##   ..$ cols   :List of 3
##   .. ..$ Name                   : list()
##   .. .. ..- attr(*, &quot;class&quot;)= chr  &quot;collector_character&quot; &quot;collector&quot;
##   .. ..$ FIPS State Numeric Code: list()
##   .. .. ..- attr(*, &quot;class&quot;)= chr  &quot;collector_integer&quot; &quot;collector&quot;
##   .. ..$ Official USPS Code     : list()
##   .. .. ..- attr(*, &quot;class&quot;)= chr  &quot;collector_character&quot; &quot;collector&quot;
##   ..$ default: list()
##   .. ..- attr(*, &quot;class&quot;)= chr  &quot;collector_guess&quot; &quot;collector&quot;
##   ..- attr(*, &quot;class&quot;)= chr &quot;col_spec&quot;</code></pre>
<pre class="r"><code># give some better variable names (spaces never helpful)
names(states) &lt;- c(&quot;name&quot;, &quot;fips_code&quot;, &quot;usps_code&quot;)
# see if all states are covered
dplyr::setdiff(default_rate_state$addr_state, states$usps_code)</code></pre>
<pre><code>## character(0)</code></pre>
<p>Comparing the unique abbreviations in the loans data with the usps codes reveals that we are all covered so letâs bring the data together using <code>dplyr::left_join()</code>. For details on dplyr join syntax, see <a href="https://cran.r-project.org/web/packages/dplyr/vignettes/two-table.html">Two-table verbs</a>. To go with the <code>choroplethr</code> conventions, we also need to</p>
<ul>
<li>have state names in lower case</li>
<li>rename the mapping variable to <code>region</code> and the numeric metric to <code>value</code>.</li>
<li>only select above two variables in the function call</li>
</ul>
<p>Note that we are re-assigning the data to itself to avoid creating a new table.</p>
<pre class="r"><code>default_rate_state &lt;-
  default_rate_state %&gt;%
  left_join(states[, c(&quot;usps_code&quot;, &quot;name&quot;)], 
            by = c(&quot;addr_state&quot; = &quot;usps_code&quot;)) %&gt;%
  rename(region = name, value = default_rate) %&gt;%
  mutate(region = tolower(region)) %&gt;%
  select(region, value)</code></pre>
<p>Finally, we can plot the data using the default colors from <code>choroplethr</code>. As we are having data on state level, we use function <code>choroplethr::state_choropleth()</code>.</p>
<pre class="r"><code>choroplethr::state_choropleth(df = default_rate_state, 
                              title = &quot;Default rate by State&quot;)</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>
<p>We can derive a few points from the plot:</p>
<ul>
<li>default rate varies across states</li>
<li>we may want to adjust the default binning</li>
<li>we are only looking at default rate but not defaulted exposure (there may be states with many defaults but the defaulted amount is small)</li>
</ul>
<p>We could do the same exercise but on a more granular level as we have zip codes available. They are truncated so we would have to find a way to work with them. They also have a high number of unique values so the map legend may be cramped if we were looking at all of US.</p>
<p></p>
</div>
</div>
<div id="correlation" class="section level1">
<h1><span class="header-section-number">10</span> Correlation</h1>
<p>Many models rely on the notion of correlation between independent and dependent variables so a natural exploratoy visualization would be a correlation plot or correlogram. One library offering this is <code>corrplot</code> with its main function <code>corrplot::corrplot()</code>. The function takes as input the correlation matrix that can be produced with <code>stats::cor()</code>. This of course is only defined for numeric, non-missing variables. In order to have a reasonable information density in the correlation matrix, we will kick out some variables with a missing value share of larger 50%.</p>
<p>For a discussion on missing value treatment, see e.g. <a href="http://www.stat.columbia.edu/~gelman/arm/missing.pdf">Data Analysis Using Regression and Multilevel/Hierarchical Models - Chapter 25: Missing-data imputation</a></p>
<p>Letâs again build a numeric variable vector after all previous operations and look at correlations.</p>
<pre class="r"><code>num_vars &lt;- 
  train %&gt;% 
  sapply(is.numeric) %&gt;% 
  which() %&gt;% 
  names()

meta_train &lt;- funModeling::df_status(train, print_results = FALSE)

meta_train %&gt;%
  select(variable, p_zeros, p_na, unique) %&gt;%
  filter_(~ variable %in% num_vars) %&gt;%
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">variable</th>
<th align="right">p_zeros</th>
<th align="right">p_na</th>
<th align="right">unique</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">loan_amnt</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">1369</td>
</tr>
<tr class="even">
<td align="left">funded_amnt</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">1369</td>
</tr>
<tr class="odd">
<td align="left">funded_amnt_inv</td>
<td align="right">0.03</td>
<td align="right">0.00</td>
<td align="right">8203</td>
</tr>
<tr class="even">
<td align="left">int_rate</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">540</td>
</tr>
<tr class="odd">
<td align="left">installment</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">64438</td>
</tr>
<tr class="even">
<td align="left">annual_inc</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">41978</td>
</tr>
<tr class="odd">
<td align="left">dti</td>
<td align="right">0.05</td>
<td align="right">0.00</td>
<td align="right">4072</td>
</tr>
<tr class="even">
<td align="left">delinq_2yrs</td>
<td align="right">80.80</td>
<td align="right">0.00</td>
<td align="right">26</td>
</tr>
<tr class="odd">
<td align="left">inq_last_6mths</td>
<td align="right">56.10</td>
<td align="right">0.00</td>
<td align="right">27</td>
</tr>
<tr class="even">
<td align="left">mths_since_last_delinq</td>
<td align="right">51.43</td>
<td align="right">0.00</td>
<td align="right">152</td>
</tr>
<tr class="odd">
<td align="left">mths_since_last_record</td>
<td align="right">84.70</td>
<td align="right">0.00</td>
<td align="right">123</td>
</tr>
<tr class="even">
<td align="left">open_acc</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">74</td>
</tr>
<tr class="odd">
<td align="left">pub_rec</td>
<td align="right">84.70</td>
<td align="right">0.00</td>
<td align="right">31</td>
</tr>
<tr class="even">
<td align="left">revol_bal</td>
<td align="right">0.38</td>
<td align="right">0.00</td>
<td align="right">68979</td>
</tr>
<tr class="odd">
<td align="left">revol_util</td>
<td align="right">0.40</td>
<td align="right">0.05</td>
<td align="right">1312</td>
</tr>
<tr class="even">
<td align="left">total_acc</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">131</td>
</tr>
<tr class="odd">
<td align="left">out_prncp</td>
<td align="right">28.85</td>
<td align="right">0.00</td>
<td align="right">209810</td>
</tr>
<tr class="even">
<td align="left">out_prncp_inv</td>
<td align="right">28.85</td>
<td align="right">0.00</td>
<td align="right">224620</td>
</tr>
<tr class="odd">
<td align="left">total_pymnt</td>
<td align="right">2.01</td>
<td align="right">0.00</td>
<td align="right">423038</td>
</tr>
<tr class="even">
<td align="left">total_pymnt_inv</td>
<td align="right">2.04</td>
<td align="right">0.00</td>
<td align="right">424921</td>
</tr>
<tr class="odd">
<td align="left">total_rec_prncp</td>
<td align="right">2.05</td>
<td align="right">0.00</td>
<td align="right">222003</td>
</tr>
<tr class="even">
<td align="left">total_rec_int</td>
<td align="right">2.06</td>
<td align="right">0.00</td>
<td align="right">287778</td>
</tr>
<tr class="odd">
<td align="left">total_rec_late_fee</td>
<td align="right">98.59</td>
<td align="right">0.00</td>
<td align="right">5169</td>
</tr>
<tr class="even">
<td align="left">recoveries</td>
<td align="right">97.22</td>
<td align="right">0.00</td>
<td align="right">18681</td>
</tr>
<tr class="odd">
<td align="left">collection_recovery_fee</td>
<td align="right">97.35</td>
<td align="right">0.00</td>
<td align="right">16796</td>
</tr>
<tr class="even">
<td align="left">last_pymnt_amnt</td>
<td align="right">2.00</td>
<td align="right">0.00</td>
<td align="right">197434</td>
</tr>
<tr class="odd">
<td align="left">collections_12_mths_ex_med</td>
<td align="right">98.65</td>
<td align="right">0.02</td>
<td align="right">12</td>
</tr>
<tr class="even">
<td align="left">mths_since_last_major_derog</td>
<td align="right">75.06</td>
<td align="right">0.00</td>
<td align="right">167</td>
</tr>
<tr class="odd">
<td align="left">acc_now_delinq</td>
<td align="right">99.53</td>
<td align="right">0.00</td>
<td align="right">8</td>
</tr>
<tr class="even">
<td align="left">tot_coll_amt</td>
<td align="right">78.94</td>
<td align="right">7.92</td>
<td align="right">9319</td>
</tr>
<tr class="odd">
<td align="left">tot_cur_bal</td>
<td align="right">0.01</td>
<td align="right">7.92</td>
<td align="right">293158</td>
</tr>
</tbody>
</table>
<p>Finally, we can produce a correlation plot. Dealing with missing values, using option <code>use = &quot;pairwise.complete.obs&quot;</code> in function <code>stats::cor()</code> is considered bad practice as it uses pair matching and correlations may not be comparable, see e.g. <a href="http://bwlewis.github.io/covar/missing.html">Pairwise-complete correlation considered dangerous</a>. Alternatively, we can use option <code>use = complete.obs</code> which only considers complete observations but may discard a lot of data. However, as we have looked into the meta information, after our wrangling the proportion of missing values for the numeric variables has dropped a lot so we should be fine.</p>
<p>Once we have the correlation matrix, we can use the function <code>corrplot::corrplot()</code> from the <code>corrplot</code> library. It offers a number of different visualization options. For details, see <a href="https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html">An Introduction to corrplot Package</a>.</p>
<pre class="r"><code>library(corrplot)
corrplot::corrplot(cor(train[, num_vars], use = &quot;complete.obs&quot;), 
                   method = &quot;pie&quot;, type = &quot;upper&quot;)</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-49-1.png" width="1344" /></p>
<p>We can derive a few things from the plot</p>
<ul>
<li>overall correlation between variables seems low but there are a few highly correlated ones</li>
<li>some highly correlated variables may indicate very similar information, e.g. <code>mths_since_last_delinq</code> and <code>mths_since_last_major_derog</code></li>
<li>some variables are perfectly correlated and thus should be removed</li>
</ul>
<p>Rather than a visual inspection, an (automatic) inspection of correlations and removal of highly correlated features can be done via function <code>caret::findCorrelation()</code> with a defined <code>cutoff</code> parameter. If two variables have a high correlation, the function looks at the mean absolute correlation of each variable and removes the variable with the largest mean absolute correlation. Using <code>exact = TRUE</code> will cause the function to re-evaluate the average correlations at each step while <code>exact = FALSE</code> uses all the correlations regardless of whether they have been eliminated or not. The exact calculations will remove a smaller number of predictors but can be much slower when the problem dimensions are âbigâ.</p>
<pre class="r"><code>caret::findCorrelation(cor(train[, num_vars], use = &quot;complete.obs&quot;), 
                       names = TRUE, cutoff = .5)</code></pre>
<pre><code>##  [1] &quot;loan_amnt&quot;                   &quot;funded_amnt&quot;                
##  [3] &quot;funded_amnt_inv&quot;             &quot;installment&quot;                
##  [5] &quot;total_pymnt_inv&quot;             &quot;total_pymnt&quot;                
##  [7] &quot;total_rec_prncp&quot;             &quot;out_prncp&quot;                  
##  [9] &quot;total_acc&quot;                   &quot;mths_since_last_record&quot;     
## [11] &quot;mths_since_last_major_derog&quot; &quot;recoveries&quot;</code></pre>
<p>Given above, we remove a few variables</p>
<pre class="r"><code>vars_to_remove &lt;- 
  c(&quot;loan_amnt&quot;, &quot;funded_amnt&quot;, &quot;funded_amnt_inv&quot;, &quot;installment&quot;,
    &quot;total_pymnt_inv&quot;, &quot;total_rec_prncp&quot;, &quot;mths_since_last_delinq&quot;, 
    &quot;out_prncp&quot;, &quot;total_pymnt&quot;, &quot;total_rec_int&quot;, &quot;total_acc&quot;,
    &quot;mths_since_last_record&quot;, &quot;recoveries&quot;)

train &lt;- train %&gt;% select(-one_of(vars_to_remove))</code></pre>
<p></p>
</div>
<div id="summary-plots" class="section level1">
<h1><span class="header-section-number">11</span> Summary plots</h1>
<p>An alternative to producing individual exploratory plots is using summary plots that come with sensible default settings and display the data according to its data type (numeric or categorical). The advantages are that a lot of information can be put into one graph and potential relationships may be spotted more easily. The disadvantages are potential performance issues when plotting many variables of a large dataset and readability issues in case of many dimensions for categorical variables. These disadvantages can be somewhat remediated by looking at summary plots after a lot of variables have already been discarded due to other findings (as we have done here). To show the general idea, we use the <code>GGally</code> library which builds on top of <code>ggplot2</code> and provides some nice visualization functions. We choose a subset of variables for illustration of the <code>GGally::ggpairs()</code> function. Even with this subset the production of the graph may take some time depending on your hardware. For a theoretical background, see <a href="http://www.bricol.net/downloads/pubs/Emerson_et_al.2013.pdf">The Generalized Pairs Plot</a>. For the many customization options, see <a href="http://ggobi.github.io/ggally">GGally</a>. As always when visualizing a lot of information, there is a trade-off between information density and visualization efficiency. The more variables are selected for the plot, the more difficult it becomes to efficiently visualize it. This needs some playing around or a really big screen.</p>
<p><em>Note</em></p>
<blockquote>
<p>Simply leaving the variable <code>default</code> as boolean would produce an error in <code>ggpairs</code> (or rather in <code>ggplot2</code> which it calls): <code>Error: StatBin requires a continuous x variable the x variable is discrete. Perhaps you want stat=&quot;count&quot;?</code>. Besides, we like it to be treated as categorical so we convert it to character for this function call.</p>
</blockquote>
<pre class="r"><code>library(GGally)
plot_ggpairs &lt;-
  train %&gt;%
  select(annual_inc, term, grade, default) %&gt;%
  mutate(default = as.character(default)) %&gt;%
  ggpairs()

plot_ggpairs</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-52-1.png" width="1344" /></p>
<p>The resulting graph has a lot of information in it and we wonât go into further detail here but the most important points to read the graph properly. In the columns:</p>
<ul>
<li>for single continuous numeric variable plots density estimate</li>
<li>for continuous numeric variable against another continuous numeric variable plots scatterplot</li>
<li>for continuous numeric variable against another categorical variable plots histogram</li>
<li>for single categorical variable plots histogram</li>
</ul>
<p>In the rows:</p>
<ul>
<li>for continuous numeric variable against another continuous numeric variable plots correlation</li>
<li>for continuous numeric variable against another categorical variable plots boxplot by dimension of categorical variable</li>
<li>for categorical variable against another categorical variable plots historgram by dimension of y-axis categorical variable</li>
<li>for categorical variable against another continuous variable plots scatterplot by dimension of categorical variable</li>
</ul>
<p>Another interesting plot would be to visualize the density (estimates) of the numeric variables color-coded by the classes of the target variable (here: default/non-default) to inspect whether there are visual clues about differences in the distributions. We can achieve this by using <code>ggplot</code> but need to bring the data into a long format rather than its current wide format (see <a href="http://garrettgman.github.io/tidying/">Data Science with R - Chapter Data Tidying</a> for details on reshaping). We use <code>tidyr::gather()</code> to bring the numeric variables into a long format and then add the target variable again. Note that there are different lengths of the reshaped data and the original target variable vector. As <code>dplyr::mutate()</code> will not properly take care of the recycling rules, we have to recycle (repeat) the variable <code>default</code> manually to have the same length as the reshaped train data. See <a href="https://cran.r-project.org/doc/manuals/R-intro.pdf">An Introduction to R - Chapter 2.2: Vector arithmetic</a> how <code>recycling</code> works in general.</p>
<blockquote>
<p>Vectors occurring in the same expression need not all be of the same length. If they are not, the value of the expression is a vector with the same length as the longest vector which occurs in the expression. Shorter vectors in the expression are recycled as often as need be (perhaps fractionally) until they match the length of the longest vector. In particular a constant is simply repeated.</p>
</blockquote>
<p>Note that we make use of a few plot parameters to make the visualization easier to follow. We like <code>default</code> to be a factor so the densities are independent of class distribution. First, we switch the levels of variable <code>default</code> to have <code>TRUE</code> on top (as the highest objective is to get the true positive right). We use the <code>fill</code> and <code>color</code> parameters to encode with the target variable. This allows a visual distinction of overlaying densities. In addition, we set the <code>alpha</code> value to have the area under the curve transparently filled. For color selection we make use of <code>brewer</code> scales which offer a well selected pallette of colors. Finally, we split the plots by variable with <code>facet_wrap</code> allowing the scales of each indifidual plot to be <code>free</code> and defining the number of columns via <code>ncol</code>.</p>
<pre class="r"><code>num_vars &lt;- train %&gt;% sapply(is.numeric) %&gt;% which() %&gt;% names()

train %&gt;%
  select_(.dots = num_vars) %&gt;%
  gather(measure, value) %&gt;%
  mutate(default = factor(rep(x = train$default, 
                              length.out = length(num_vars) * dim(train)[1]), 
                          levels = c(&quot;TRUE&quot;, &quot;FALSE&quot;))) %&gt;%
  ggplot(data = ., aes(x = value, fill = default, 
                       color = default, order = -default)) +
  geom_density(alpha = 0.3, size = 0.5) +
  scale_fill_brewer(palette = &quot;Set1&quot;) +
  scale_color_brewer(palette = &quot;Set1&quot;) +
  facet_wrap( ~ measure, scales = &quot;free&quot;, ncol = 3)</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-53-1.png" width="1344" /></p>
<p>We can derive a few things from the plot:</p>
<ul>
<li>most of the densities are right-skewed with probably a few outliers extending the x-axis which makes discerning patterns in the area with highest mass difficult</li>
<li>even with above mentioned visualization impediment it seems that there is not a lot of difference between default and non-default loans for most variables</li>
<li>some densities seem wobbly for the majority class (e.g. <code>open_acc</code>) which is caused by the fact that the smoothing algorithm has many more data points to smooth in between but the density follows the same general shape</li>
</ul>
<p>Given above observations, we can try to improve the plot by removing (adjusting) outliers. There are many different methods that can be used, see e.g.</p>
<ul>
<li><a href="https://pdfs.semanticscholar.org/be78/b80341d0bda8bfce9a9c3b4a04953fe3a11d.pdf">A Survey of Outlier Detection Methodologies</a></li>
<li><a href="http://www.dbs.ifi.lmu.de/~zimek/publications/SDM2012/SDM12-outlierevaluation.pdf">On Evaluation of Outlier Rankings and Outlier Scores</a></li>
<li><a href="https://ww2.amstat.org/sections/srms/Proceedings/y2012/files/304068_72402.pdf">Outliers: An Evaluation of Methodologies</a></li>
<li><a href="http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2009.00001.x/full">A protocol for data exploration to avoid common statistical problems</a></li>
<li><a href="https://datascienceplus.com/outlier-detection-and-treatment-with-r/">Outlier detection and treatment with R</a></li>
<li><a href="http://www.eng.tau.ac.il/~bengal/outlier.pdf">Data Mining and Knowledge Discovery Handbook - Chapter 1: Outlier Detection</a></li>
</ul>
<p>Most of these methods focus on removing outliers for modeling which requires a lot of care and understanding how model characteristics change when removing / replacing outliers. In our case, we can be more pragmatic as we are only interested in improving our understanding of the mass of the distribution in a graphical way to see if there are easily discernible pattern differences between the two outcomes of our target variable. For this, we like to replace outliers with some sensible value derived from the distribution of the particular variable. One could use the interquartile range as it is used in boxplots. We will use <a href="https://en.wikipedia.org/wiki/Winsorizing">winsorization</a>, i.e.Â replacing outliers with a specified percentile, e.g.Â 95%. As this is a fairly simple task, we write our own function to do that. Alternatively, one may use e.g.Â function <code>psych::winsor()</code> which trims outliers from both ends of the distribution. Since we have already seen, that there is no negative value present, we only cut from the right side of the distribution. We call the function <code>winsor_outlier</code> and allow settings for the percentile and removing NA values. The design is inspired by an SO question <a href="https://stackoverflow.com/questions/13339685/how-to-replace-outliers-with-the-5th-and-95th-percentile-values-in-r">How to replace outliers with the 5th and 95th percentile values in R</a>.</p>
<pre class="r"><code>winsor_outlier &lt;- function(x, cutoff = .95, na.rm = FALSE){
    quantiles &lt;- quantile(x, cutoff, na.rm = na.rm)
    x[x &gt; quantiles] &lt;- quantiles
    x
    }

train %&gt;%
  select_(.dots = num_vars) %&gt;%
  mutate_all(.funs = winsor_outlier, cutoff = .95, na.rm = TRUE) %&gt;%
  gather(measure, value) %&gt;%
  mutate(default = factor(rep(x = train$default, 
                              length.out = length(num_vars)*dim(train)[1]), 
                          levels = c(&quot;TRUE&quot;, &quot;FALSE&quot;))) %&gt;%
  ggplot(data = ., aes(x = value, fill = default, 
                       color = default, order = -default)) +
  geom_density(alpha = 0.3, size = 0.5) +
  scale_fill_brewer(palette = &quot;Set1&quot;) +
  scale_color_brewer(palette = &quot;Set1&quot;) +
  facet_wrap( ~ measure, scales = &quot;free&quot;, ncol = 3)</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-54-1.png" width="1344" /></p>
<p>We can derive a few things from the plot:</p>
<ul>
<li>most densities donât show a discernible difference between default and non-default</li>
<li>ignore the densities centered around zero as they stem from too few observations</li>
<li>some densities show a bump on the right stemming from the outlier replacement and thus indicating that there is actually data in the tail</li>
</ul>
<p>Interestingly, variable <code>out_prncp_inv</code> shows a significant bump around zero for <code>default == FALSE</code> which can be confirmed via a count. However, the description tells us that this variable represents <code>Remaining outstanding principal for portion of total amount funded by investors</code> so itâs unclear how useful it is (especially when knowing that this variable will only be available after a loan has been granted).</p>
<pre class="r"><code>train %&gt;% 
  select(default, out_prncp_inv) %&gt;% 
  filter(out_prncp_inv == 0) %&gt;%
  group_by(default) %&gt;% 
  summarize(n = n())</code></pre>
<pre><code>## # A tibble: 2 x 2
##   default      n
##     &lt;lgl&gt;  &lt;int&gt;
## 1   FALSE 204207
## 2    TRUE    613</code></pre>
<p>Overall, it seems from visual inspection that the numeric variables do not contain a lot of information gain regarding the classification of the target variable. One obvious exception is the interest rate which is a function of the expected risk of the borrower probably mostly driven by the rating. Here we see that defaulted clients tend to have higher interest rates and also seem to have more outliers in the tail.</p>
<p><strong>A side note on plotting</strong></p>
<p>If your screen is too small, you can export any plot to e.g.Â svg or pdf using graphics devices of the cairo API in package <code>grDevices</code> (usually part of base R distro) and scale the plot size to something bigger (especially helpful if itâs a long plot). Here we show an example for the previously created <code>ggpairs</code> plot stored as pdf.</p>
<pre class="r"><code>cairo_pdf(filename = paste0(path, &quot;/plot_ggpairs.pdf&quot;),
    width=15,
    height=20,
    pointsize=10)
plot_ggpairs
dev.off()</code></pre>
<p></p>
</div>
<div id="modeling" class="section level1">
<h1><span class="header-section-number">12</span> Modeling</h1>
<div id="model-options" class="section level2">
<h2><span class="header-section-number">12.1</span> Model options</h2>
<p>There are a couple of different model options available for a supervised binary classification problem such as determining whether a loan will default or not. In general, we may distinguish the following model approaches (among others):</p>
<ul>
<li>logistic regression</li>
<li>linear discriminant analysis</li>
<li>k-nearest neighbors (kNN)</li>
<li>trees</li>
<li>random forests</li>
<li>boosting</li>
<li>support vector machines (SVM)</li>
</ul>
<p>In addition, there are multiple ways to assess model performance such as model parameters, cross-validation, bootstrap and multiple feature selection methods such as ridge regression, the lasso, principal component anaylsis, etc.</p>
<p>We will look at some of those models and methods and see how they compare. For a more detailed treatment and further background, an accessible text is <a href="http://www-bcf.usc.edu/~gareth/ISL/">âAn Introduction to Statistical Learningâ</a> or for a more advanced treatment <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">âThe Elements of Statistical Learning: Data Mining, Inference, and Predictionâ</a>.</p>
</div>
<div id="imbalanced-data" class="section level2">
<h2><span class="header-section-number">12.2</span> Imbalanced data</h2>
<p>Before we can think about any modeling, we have to realize that the dataset is highly imbalanced, i.e.Â the target variable has a very low proportion of defaults (namely 0.0249963). This can lead to several issues in many algorithms, e.g.</p>
<ul>
<li>biased accuracy</li>
<li>loss functions attempt to optimize quantities such as error rate, not taking the data distribution into consideration</li>
<li>errors have the same cost (not pertaining to imbalanced data only)</li>
</ul>
<p>More details can be found in</p>
<ul>
<li><a href="https://svds.com/learning-imbalanced-classes/">Learning from Imbalanced Classes</a></li>
<li><a href="https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/">Practical Guide to deal with Imbalanced Classification Problems in R</a></li>
<li><a href="https://topepo.github.io/caret/subsampling-for-class-imbalances.html">The caret package: Subsampling For Class Imbalances</a></li>
</ul>
<p>There are a couple of approaches to deal with the problem which can be divided into (taken from <a href="https://svds.com/learning-imbalanced-classes/">Learning from Imbalanced Classes</a>)</p>
<ul>
<li>do nothing (not a good idea in general)</li>
<li>balance training set
<ul>
<li>oversample minority class</li>
<li>undersample majority class</li>
<li>synthesize new minority classes</li>
</ul></li>
<li>throw away minority examples and switch to an anomaly detection framework</li>
<li>at the algorithm level, or after it:
<ul>
<li>adjust the class weight (misclassification costs)</li>
<li>adjust the decision threshold</li>
<li>modify an existing algorithm to be more sensitive to rare classes</li>
</ul></li>
<li>construct an entirely new algorithm to perform well on imbalanced data</li>
</ul>
<p>We will focus on balancing the training set. There are basically three options available</p>
<ul>
<li>undersample majority class (randomly or informed)</li>
<li>oversample minority class (randomly or informed)</li>
<li>do a mixture of both (e.g.Â SMOTE or ROSE method)</li>
</ul>
<p>Undersampling the majority class may lose information but via decreasing dataset also lead to more computational efficiency. We also tried SMOTE and ROSE but functions <code>DMwR::SMOTE()</code> and <code>ROSE::ROSE()</code> seem to be very picky about their input and so far we failed to run them. For details on SMOTE, see <a href="https://www.jair.org/media/953/live-953-2037-jair.pdf">SMOTE: Synthetic Minority Over-sampling Technique</a> and for details on ROSE, see <a href="https://journal.r-project.org/archive/2014-1/menardi-lunardon-torelli.pdf">ROSE: A Package for Binary Imbalanced Learning</a>. As pointed out <a href="https://shiring.github.io/machine_learning/2017/04/02/unbalanced">here</a> a random sampling for either under- or oversampling is not the best idea. It is recommended to use cross-validation and perform over- or under-sampling on each fold independently to get an honest estimate of model performance. In fact, function <code>caret::train()</code> also allows the re-balancing of data during the training of a model via <code>caret::trainControl(sampling = &quot;...&quot;)</code>, see <a href="https://topepo.github.io/caret/subsampling-for-class-imbalances.html">The caret package 11.2: Subsampling During Resampling</a> for details. For now, we go with undersampling which still leaves a fair amount of training observations (at least for non-deep learning approaches).</p>
<pre class="r"><code>train_down &lt;- 
  caret::downSample(x = train[, !(names(train) %in% c(&quot;default&quot;))], 
                    y = as.factor(train$default), yname = &quot;default&quot;)

base::prop.table(table(train_down$default))</code></pre>
<pre><code>## 
## FALSE  TRUE 
##   0.5   0.5</code></pre>
<pre class="r"><code>base::table(train_down$default)</code></pre>
<pre><code>## 
## FALSE  TRUE 
## 17745 17745</code></pre>
</div>
<div id="a-note-on-modeling-libraries" class="section level2">
<h2><span class="header-section-number">12.3</span> A note on modeling libraries</h2>
<p>There are many modeling libraries in R (in fact it is one of the major reasons for its popularity) and they can be very roughly distinguished into</p>
<ul>
<li>general modeling libraries comprising many different model functions (sometimes wrappers around specialized model libraries)</li>
<li>specialized modeling libraries focusing on only one or a certain class of models</li>
</ul>
<p>We will be using general modeling libraries and in particular the (standard) library shipping with R <a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/00Index.html">stats</a> and a popular library <a href="http://topepo.github.io/caret/index.html">caret</a>. Another library often used is <a href="https://cran.r-project.org/web/packages/rms/index.html">rms</a>. The advantage of the general libraries is their easier access via a common API, usually a fairly comprehensive documentation and a relatively large user base which means high chance that someone already did what you like to do.</p>
<div id="a-note-on-caret-library-in-particular" class="section level3">
<h3><span class="header-section-number">12.3.1</span> A note on caret library in particular</h3>
<p>The caret package (short for _C_lassification _A_nd _RE_gression _T_raining) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for:</p>
<ul>
<li>data splitting</li>
<li>pre-processing</li>
<li>feature selection</li>
<li>model tuning using resampling</li>
<li>variable importance estimation</li>
</ul>
<p>as well as other functionality. It aims at providing a uniform interface to R modeling functions themselves, as well as a way to standardize common tasks (such as parameter tuning and variable importance). The <a href="https://topepo.github.io/caret/train-models-by-tag.html">documentation</a> gives an overview of the models that can be used via the <code>caret::train()</code> function which is the workhorse for building models. A quick look at available models reveals that there is a lot of choice.</p>
<pre class="r"><code>names(caret::getModelInfo())</code></pre>
<pre><code>##   [1] &quot;ada&quot;                 &quot;AdaBag&quot;              &quot;AdaBoost.M1&quot;        
##   [4] &quot;adaboost&quot;            &quot;amdai&quot;               &quot;ANFIS&quot;              
##   [7] &quot;avNNet&quot;              &quot;awnb&quot;                &quot;awtan&quot;              
##  [10] &quot;bag&quot;                 &quot;bagEarth&quot;            &quot;bagEarthGCV&quot;        
##  [13] &quot;bagFDA&quot;              &quot;bagFDAGCV&quot;           &quot;bam&quot;                
##  [16] &quot;bartMachine&quot;         &quot;bayesglm&quot;            &quot;binda&quot;              
##  [19] &quot;blackboost&quot;          &quot;blasso&quot;              &quot;blassoAveraged&quot;     
##  [22] &quot;bridge&quot;              &quot;brnn&quot;                &quot;BstLm&quot;              
##  [25] &quot;bstSm&quot;               &quot;bstTree&quot;             &quot;C5.0&quot;               
##  [28] &quot;C5.0Cost&quot;            &quot;C5.0Rules&quot;           &quot;C5.0Tree&quot;           
##  [31] &quot;cforest&quot;             &quot;chaid&quot;               &quot;CSimca&quot;             
##  [34] &quot;ctree&quot;               &quot;ctree2&quot;              &quot;cubist&quot;             
##  [37] &quot;dda&quot;                 &quot;deepboost&quot;           &quot;DENFIS&quot;             
##  [40] &quot;dnn&quot;                 &quot;dwdLinear&quot;           &quot;dwdPoly&quot;            
##  [43] &quot;dwdRadial&quot;           &quot;earth&quot;               &quot;elm&quot;                
##  [46] &quot;enet&quot;                &quot;evtree&quot;              &quot;extraTrees&quot;         
##  [49] &quot;fda&quot;                 &quot;FH.GBML&quot;             &quot;FIR.DM&quot;             
##  [52] &quot;foba&quot;                &quot;FRBCS.CHI&quot;           &quot;FRBCS.W&quot;            
##  [55] &quot;FS.HGD&quot;              &quot;gam&quot;                 &quot;gamboost&quot;           
##  [58] &quot;gamLoess&quot;            &quot;gamSpline&quot;           &quot;gaussprLinear&quot;      
##  [61] &quot;gaussprPoly&quot;         &quot;gaussprRadial&quot;       &quot;gbm_h2o&quot;            
##  [64] &quot;gbm&quot;                 &quot;gcvEarth&quot;            &quot;GFS.FR.MOGUL&quot;       
##  [67] &quot;GFS.GCCL&quot;            &quot;GFS.LT.RS&quot;           &quot;GFS.THRIFT&quot;         
##  [70] &quot;glm.nb&quot;              &quot;glm&quot;                 &quot;glmboost&quot;           
##  [73] &quot;glmnet_h2o&quot;          &quot;glmnet&quot;              &quot;glmStepAIC&quot;         
##  [76] &quot;gpls&quot;                &quot;hda&quot;                 &quot;hdda&quot;               
##  [79] &quot;hdrda&quot;               &quot;HYFIS&quot;               &quot;icr&quot;                
##  [82] &quot;J48&quot;                 &quot;JRip&quot;                &quot;kernelpls&quot;          
##  [85] &quot;kknn&quot;                &quot;knn&quot;                 &quot;krlsPoly&quot;           
##  [88] &quot;krlsRadial&quot;          &quot;lars&quot;                &quot;lars2&quot;              
##  [91] &quot;lasso&quot;               &quot;lda&quot;                 &quot;lda2&quot;               
##  [94] &quot;leapBackward&quot;        &quot;leapForward&quot;         &quot;leapSeq&quot;            
##  [97] &quot;Linda&quot;               &quot;lm&quot;                  &quot;lmStepAIC&quot;          
## [100] &quot;LMT&quot;                 &quot;loclda&quot;              &quot;logicBag&quot;           
## [103] &quot;LogitBoost&quot;          &quot;logreg&quot;              &quot;lssvmLinear&quot;        
## [106] &quot;lssvmPoly&quot;           &quot;lssvmRadial&quot;         &quot;lvq&quot;                
## [109] &quot;M5&quot;                  &quot;M5Rules&quot;             &quot;manb&quot;               
## [112] &quot;mda&quot;                 &quot;Mlda&quot;                &quot;mlp&quot;                
## [115] &quot;mlpKerasDecay&quot;       &quot;mlpKerasDecayCost&quot;   &quot;mlpKerasDropout&quot;    
## [118] &quot;mlpKerasDropoutCost&quot; &quot;mlpML&quot;               &quot;mlpSGD&quot;             
## [121] &quot;mlpWeightDecay&quot;      &quot;mlpWeightDecayML&quot;    &quot;monmlp&quot;             
## [124] &quot;msaenet&quot;             &quot;multinom&quot;            &quot;mxnet&quot;              
## [127] &quot;mxnetAdam&quot;           &quot;naive_bayes&quot;         &quot;nb&quot;                 
## [130] &quot;nbDiscrete&quot;          &quot;nbSearch&quot;            &quot;neuralnet&quot;          
## [133] &quot;nnet&quot;                &quot;nnls&quot;                &quot;nodeHarvest&quot;        
## [136] &quot;null&quot;                &quot;OneR&quot;                &quot;ordinalNet&quot;         
## [139] &quot;ORFlog&quot;              &quot;ORFpls&quot;              &quot;ORFridge&quot;           
## [142] &quot;ORFsvm&quot;              &quot;ownn&quot;                &quot;pam&quot;                
## [145] &quot;parRF&quot;               &quot;PART&quot;                &quot;partDSA&quot;            
## [148] &quot;pcaNNet&quot;             &quot;pcr&quot;                 &quot;pda&quot;                
## [151] &quot;pda2&quot;                &quot;penalized&quot;           &quot;PenalizedLDA&quot;       
## [154] &quot;plr&quot;                 &quot;pls&quot;                 &quot;plsRglm&quot;            
## [157] &quot;polr&quot;                &quot;ppr&quot;                 &quot;PRIM&quot;               
## [160] &quot;protoclass&quot;          &quot;pythonKnnReg&quot;        &quot;qda&quot;                
## [163] &quot;QdaCov&quot;              &quot;qrf&quot;                 &quot;qrnn&quot;               
## [166] &quot;randomGLM&quot;           &quot;ranger&quot;              &quot;rbf&quot;                
## [169] &quot;rbfDDA&quot;              &quot;Rborist&quot;             &quot;rda&quot;                
## [172] &quot;regLogistic&quot;         &quot;relaxo&quot;              &quot;rf&quot;                 
## [175] &quot;rFerns&quot;              &quot;RFlda&quot;               &quot;rfRules&quot;            
## [178] &quot;ridge&quot;               &quot;rlda&quot;                &quot;rlm&quot;                
## [181] &quot;rmda&quot;                &quot;rocc&quot;                &quot;rotationForest&quot;     
## [184] &quot;rotationForestCp&quot;    &quot;rpart&quot;               &quot;rpart1SE&quot;           
## [187] &quot;rpart2&quot;              &quot;rpartCost&quot;           &quot;rpartScore&quot;         
## [190] &quot;rqlasso&quot;             &quot;rqnc&quot;                &quot;RRF&quot;                
## [193] &quot;RRFglobal&quot;           &quot;rrlda&quot;               &quot;RSimca&quot;             
## [196] &quot;rvmLinear&quot;           &quot;rvmPoly&quot;             &quot;rvmRadial&quot;          
## [199] &quot;SBC&quot;                 &quot;sda&quot;                 &quot;sdwd&quot;               
## [202] &quot;simpls&quot;              &quot;SLAVE&quot;               &quot;slda&quot;               
## [205] &quot;smda&quot;                &quot;snn&quot;                 &quot;sparseLDA&quot;          
## [208] &quot;spikeslab&quot;           &quot;spls&quot;                &quot;stepLDA&quot;            
## [211] &quot;stepQDA&quot;             &quot;superpc&quot;             &quot;svmBoundrangeString&quot;
## [214] &quot;svmExpoString&quot;       &quot;svmLinear&quot;           &quot;svmLinear2&quot;         
## [217] &quot;svmLinear3&quot;          &quot;svmLinearWeights&quot;    &quot;svmLinearWeights2&quot;  
## [220] &quot;svmPoly&quot;             &quot;svmRadial&quot;           &quot;svmRadialCost&quot;      
## [223] &quot;svmRadialSigma&quot;      &quot;svmRadialWeights&quot;    &quot;svmSpectrumString&quot;  
## [226] &quot;tan&quot;                 &quot;tanSearch&quot;           &quot;treebag&quot;            
## [229] &quot;vbmpRadial&quot;          &quot;vglmAdjCat&quot;          &quot;vglmContRatio&quot;      
## [232] &quot;vglmCumulative&quot;      &quot;widekernelpls&quot;       &quot;WM&quot;                 
## [235] &quot;wsrf&quot;                &quot;xgbLinear&quot;           &quot;xgbTree&quot;            
## [238] &quot;xyf&quot;</code></pre>
</div>
</div>
<div id="data-preparation-for-modeling" class="section level2">
<h2><span class="header-section-number">12.4</span> Data preparation for modeling</h2>
<p>A number of models require data preparation in advance so here we are going to perform some pre-processing so that all models can work with the data. Note that a few model functions also allow the pre-processing during the training step (e.g.Â scale and center) but it is computationally more efficient to do at least some pre-processing before.</p>
<div id="proper-names-for-character-variables" class="section level3">
<h3><span class="header-section-number">12.4.1</span> Proper names for character variables</h3>
<p>Some models require a particular naming scheme for the values of character variables (e.g.Â no spcaes). The <code>base::make.names()</code> function does just that.</p>
<pre class="r"><code>vars_to_mutate &lt;-
  train_down %&gt;%
  select(which(sapply(.,is.character))) %&gt;%
  names()

vars_to_mutate</code></pre>
<pre><code>##  [1] &quot;term&quot;                &quot;grade&quot;               &quot;emp_length&quot;         
##  [4] &quot;home_ownership&quot;      &quot;verification_status&quot; &quot;pymnt_plan&quot;         
##  [7] &quot;purpose&quot;             &quot;zip_code&quot;            &quot;addr_state&quot;         
## [10] &quot;initial_list_status&quot; &quot;application_type&quot;</code></pre>
<pre class="r"><code>train_down &lt;-
  train_down %&gt;%
  mutate_at(.funs = make.names, .vars = vars_to_mutate)
  
test &lt;-
  test %&gt;%
  mutate_at(.funs = make.names, .vars = vars_to_mutate)</code></pre>
</div>
<div id="dummy-variables" class="section level3">
<h3><span class="header-section-number">12.4.2</span> Dummy variables</h3>
<p>A few models automatically convert character/factor variables into binary dummy variables (e.g. <code>stats::glm()</code>) while others donât. To build a common ground, we can create dummy variables ahead of modeling. The <code>caret::dummyVars()</code> function creates a full set of dummy variables (i.e.Â less than full rank parameterization). The function takes a formula and a data set and outputs an object that can be used to create the dummy variables using the predict method. Letâs also remove the zip code variable as it would create a huge binary dummy matrix and potentially lead to computational problems. In many cases it is anyway not allowed to use it for modeling as it may be discriminative against certain communities. We also adjust the test set as we will need it for evaluation. We throw away all variables that are not part of the training set as they are not used by the models and would only clutter memory. We keep the dummy data separate from the original training data as any model using explicit variable names (i.e.Â not the full set) would require us to list each dummy variable separately which is cumbersome. If the data was big, we may have chosen to keep it all in one data set. For more details, see <a href="https://topepo.github.io/caret/pre-processing.html#dummy">The caret package 3.1: Creating Dummy Variables</a>.</p>
<pre class="r"><code>vars_to_remove &lt;- c(&quot;zip_code&quot;)
train_down &lt;- train_down %&gt;% select(-one_of(vars_to_remove))

# train
dummies_train &lt;-
  dummyVars(&quot;~.&quot;, data = train_down[, !(names(train_down) %in% c(&quot;default&quot;))], 
            fullRank = FALSE)

train_down_dummy &lt;-
  train_down %&gt;%
  select(-which(sapply(.,is.character))) %&gt;%
  cbind(predict(dummies_train, newdata = train_down))

# test
dummies_test &lt;-
  dummyVars(&quot;~.&quot;, data = test[, dummies_train$vars], fullRank = FALSE)

test_dummy &lt;-
  test %&gt;%
  select(one_of(colnames(train_down))) %&gt;%
  select(-which(sapply(.,is.character))) %&gt;%
  cbind(predict(dummies_test, newdata = test))</code></pre>
<p></p>
</div>
</div>
<div id="logistic-regression" class="section level2">
<h2><span class="header-section-number">12.5</span> Logistic regression</h2>
<p>While linear regression is very often used for (continuous) quantitative variables, logistic regression is its counterpart for (discrete) qualitative responses also referred to as categorical. Rather than modeling the outcome directly as default or not, logistic regression models the probability that the response belongs to a particular category. The logistic function will ensure that probabilties are within the range [0, 1]. The model coefficients are estimated via the maximum likelihood method.</p>
<p>The logistic regression model is part of the larger family of generalized linear model function and implemented in many R packages. We will use the <code>stats::glm()</code> function that usually comes with the standard install. A strong contender for any regression modeling is the <code>caret</code> package that we have already used before and will use again.</p>
<p>We will start with a very simple model of one explanatory variable, namely <code>grade</code>, which intuitively should be a good predictor of default as it is a credit rating and therefore an evaluation of the borrower. Note that a call to <code>stats::glm()</code> will not return all model statistics by default but only some attributes of the created model object. We can inspect the objectâs attributes with the <code>base::attributes()</code> function. The function <code>base::summary()</code> is a generic function used to produce result summaries of the results of various model fitting functions. The function invokes particular methods which depend on the class of the first argument. In the case of a <code>glm</code> object, it will return model statistics such as</p>
<ul>
<li>deviance residuals</li>
<li>coefficients</li>
<li>standard error</li>
<li>z statistic</li>
<li>p-value</li>
<li>AIC</li>
</ul>
<p>and some more. We may also use the <code>summary</code> function on selected model objects, e.g.Â the coefficients only via <code>summary(model_object)$coef</code>.</p>
<p>Finally, before running the model, the documentation of <code>stats::glm()</code> tells us that if the response is of type <code>factor</code> the first level denotes failure and all others success. We can check and find that this is already the case here.</p>
<pre class="r"><code>levels(train_down$default)</code></pre>
<pre><code>## [1] &quot;FALSE&quot; &quot;TRUE&quot;</code></pre>
<pre class="r"><code>model_glm_1 &lt;- glm(formula = default ~ grade, 
                   family = binomial(link = &quot;logit&quot;), 
                   data = train_down, na.action = na.exclude)
class(model_glm_1)</code></pre>
<pre><code>## [1] &quot;glm&quot; &quot;lm&quot;</code></pre>
<pre class="r"><code>attributes(model_glm_1)</code></pre>
<pre><code>## $names
##  [1] &quot;coefficients&quot;      &quot;residuals&quot;         &quot;fitted.values&quot;    
##  [4] &quot;effects&quot;           &quot;R&quot;                 &quot;rank&quot;             
##  [7] &quot;qr&quot;                &quot;family&quot;            &quot;linear.predictors&quot;
## [10] &quot;deviance&quot;          &quot;aic&quot;               &quot;null.deviance&quot;    
## [13] &quot;iter&quot;              &quot;weights&quot;           &quot;prior.weights&quot;    
## [16] &quot;df.residual&quot;       &quot;df.null&quot;           &quot;y&quot;                
## [19] &quot;converged&quot;         &quot;boundary&quot;          &quot;model&quot;            
## [22] &quot;call&quot;              &quot;formula&quot;           &quot;terms&quot;            
## [25] &quot;data&quot;              &quot;offset&quot;            &quot;control&quot;          
## [28] &quot;method&quot;            &quot;contrasts&quot;         &quot;xlevels&quot;          
## 
## $class
## [1] &quot;glm&quot; &quot;lm&quot;</code></pre>
<pre class="r"><code>summary(model_glm_1)</code></pre>
<pre><code>## 
## Call:
## glm(formula = default ~ grade, family = binomial(link = &quot;logit&quot;), 
##     data = train_down, na.action = na.exclude)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.8377  -1.1985  -0.0327   1.1564   1.7406  
## 
## Coefficients:
##             Estimate Std. Error z value            Pr(&gt;|z|)    
## (Intercept) -1.26665    0.03911  -32.39 &lt;0.0000000000000002 ***
## gradeB       0.77173    0.04519   17.08 &lt;0.0000000000000002 ***
## gradeC       1.31620    0.04389   29.99 &lt;0.0000000000000002 ***
## gradeD       1.67669    0.04600   36.45 &lt;0.0000000000000002 ***
## gradeE       1.99273    0.05132   38.83 &lt;0.0000000000000002 ***
## gradeF       2.26212    0.06856   33.00 &lt;0.0000000000000002 ***
## gradeG       2.75092    0.12634   21.77 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 49200  on 35489  degrees of freedom
## Residual deviance: 46080  on 35483  degrees of freedom
## AIC: 46094
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code>summary(model_glm_1)$coef</code></pre>
<pre><code>##               Estimate Std. Error   z value      Pr(&gt;|z|)
## (Intercept) -1.2666450 0.03910963 -32.38704 4.178600e-230
## gradeB       0.7717315 0.04518690  17.07866  2.139941e-65
## gradeC       1.3161994 0.04388804  29.98993 1.327728e-197
## gradeD       1.6766852 0.04600412  36.44641 7.841098e-291
## gradeE       1.9927260 0.05131843  38.83061  0.000000e+00
## gradeF       2.2621234 0.06855512  32.99715 8.925306e-239
## gradeG       2.7509198 0.12633659  21.77453 4.046038e-105</code></pre>
<p>There are a couple of functions that work natively with lm / glm objects created via <code>stats::glm()</code> (taken from <a href="http://stephlocke.info/Rtraining/logisticregressions.html#">Steph Locke: Logistic Regressions</a>):</p>
<ul>
<li>coefficients: Extract coefficients</li>
<li>summary: Output a basic summary</li>
<li>fitted: Return the predicted values for the training data</li>
<li>predict: Predict some values for new data</li>
<li>plot: Produce some basic diagnostic plots</li>
<li>residuals: Return the errors on predicted values for the training data</li>
</ul>
<p>Also note that categorical variables will get transformed into dummy variables. A standardization is not necessarily needed, see e.g. <a href="https://stats.stackexchange.com/questions/48360/is-standardization-needed-before-fitting-logistic-regression">Is standardization needed before fitting logistic regression</a>. There are a number of model diagnostics one may perform, see e.g. <a href="https://stats.stackexchange.com/questions/45050/diagnostics-for-logistic-regression">Diagnostics for logistic regression</a> and pay attention to <code>Frank Harrell</code>âs answer.</p>
</div>
<div id="model-evaluation-illustration" class="section level2">
<h2><span class="header-section-number">12.6</span> Model evaluation illustration</h2>
<p>For the sake of illustration, letâs go once through the model evaluation steps for this very simple model. This workflow should be similar even when the model gets more complex. There are many perspectives one can take when evaluating a model and many packages that provide helper functions to do so. For this illustration, we focus on a fairly standard approach looking at some statistics and explaining them along the way. We will also be plotting an ROC curve to establish its basic functioning.</p>
<p>To evaluate the model performance, we have to use the fitted model to predict the target variable <code>default</code> on unseen (test) data. The function <code>stats::preditc.glm()</code> does exactly that. We set <code>type = &quot;response&quot;</code> which returns the predicted probabilities and thus allows us to evaluate against a chosen threshold. This can be useful in cases where the cost of missclassification is not equal and one outcome should be penalized heavier than another. Also note, that we choose the positive outcome to be <code>TRUE</code>, i.e.Â a true positive would be correctly identifiying a default. We use the function <code>caret::ConfusionMatrix()</code> to plot a confusion matrix (also called error matrix). The function also returns a number of model statistics (however note that the test data is highly imbalanced as we have not applied any resampling as in the training data - so a number of statistics are meaningless). Here is what the function reports:</p>
<ul>
<li>Accuracy: (TP + TN) / (TP + FP + TN + FN)</li>
<li>95% CI: 95% confidence interval for accuracy</li>
<li>No Information Rate: accuracy in case of random guessing (in case of binary outcome the proportion of the majority class)</li>
<li>P-Value [Acc &gt; NIR]: p-value that accuracy is larger No Information Rate</li>
<li>Kappa: (observed accuracy - expected accuracy) / (1 - expected accuracy), see <a href="https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english">Cohenâs kappa in plain English</a></li>
<li>Mcnemarâs Test P-Value</li>
<li>Sensitivity / recall: true positive rate = TP / (TP + FN)</li>
<li>Specificity: true negative rate = TN / (TN + FP)</li>
<li>Pos Pred Value: (sensitivity * prevalence) / ((sensitivity * prevalence) + ((1 - specificity) * (1 - prevalence)))</li>
<li>Neg Pred Value: (specificity * (1-prevalence)) / (((1-sensitivity) * prevalence) + ((specificity) * (1 - prevalence)))</li>
<li>Prevalence: (TP + FN) / (TP + FP + TN + FN)</li>
<li>Detection Rate: TP / (TP + FP + TN + FN)</li>
<li>Detection Prevalence: (TP + FP) / (TP + FP + TN + FN)</li>
<li>Balanced Accuracy: (sensitivity + specificity) / 2</li>
</ul>
<pre class="r"><code>model_glm_1_pred &lt;- 
  predict.glm(object = model_glm_1, newdata = test, type = &quot;response&quot;)
model_pred_t &lt;- function(pred, t) ifelse(pred &gt; t, TRUE, FALSE)
caret::confusionMatrix(data = model_pred_t(model_glm_1_pred, 0.5), 
                       reference = test$default,
                       positive = &quot;TRUE&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction FALSE  TRUE
##      FALSE 79712  1003
##      TRUE  93327  3433
##                                              
##                Accuracy : 0.4685             
##                  95% CI : (0.4662, 0.4708)   
##     No Information Rate : 0.975              
##     P-Value [Acc &gt; NIR] : 1                  
##                                              
##                   Kappa : 0.0211             
##  Mcnemar&#39;s Test P-Value : &lt;0.0000000000000002
##                                              
##             Sensitivity : 0.77390            
##             Specificity : 0.46066            
##          Pos Pred Value : 0.03548            
##          Neg Pred Value : 0.98757            
##              Prevalence : 0.02500            
##          Detection Rate : 0.01934            
##    Detection Prevalence : 0.54520            
##       Balanced Accuracy : 0.61728            
##                                              
##        &#39;Positive&#39; Class : TRUE               
## </code></pre>
<p>Looking at the model statistics, we find a mixed picture:</p>
<ul>
<li>we get a fair amount of true defaults right (sensitivity)</li>
<li>we get a large amount of non-defaults wrong (specificity)</li>
<li>the Kappa (which should consider class distributions) is very low</li>
</ul>
<p>We can visualize the classification (predicted probabilities) versus the actual class for the test set and the given threshold. We make use of a plotting function inspired by <a href="https://www.joyofdata.de/blog/illustrated-guide-to-roc-and-auc/">Illustrated Guide to ROC and AUC</a> with its source code residing on <a href="https://github.com/joyofdata/joyofdata-articles/blob/master/roc-auc/plot_pred_type_distribution.R">Github</a>. We have slightly adjusted the code to fit our needs.</p>
<pre class="r"><code>plot_pred_type_distribution &lt;- function(df, threshold) {
  v &lt;- rep(NA, nrow(df))
  v &lt;- ifelse(df$pred &gt;= threshold &amp; df$actual == 1, &quot;TP&quot;, v)
  v &lt;- ifelse(df$pred &gt;= threshold &amp; df$actual == 0, &quot;FP&quot;, v)
  v &lt;- ifelse(df$pred &lt; threshold &amp; df$actual == 1, &quot;FN&quot;, v)
  v &lt;- ifelse(df$pred &lt; threshold &amp; df$actual == 0, &quot;TN&quot;, v)
  
  df$pred_type &lt;- v
  
  ggplot(data = df, aes(x = actual, y = pred)) + 
    geom_violin(fill = rgb(1, 1 ,1, alpha = 0.6), color = NA) + 
    geom_jitter(aes(color = pred_type), alpha = 0.6) +
    geom_hline(yintercept = threshold, color = &quot;red&quot;, alpha = 0.6) +
    scale_color_discrete(name = &quot;type&quot;) +
    labs(title = sprintf(&quot;Threshold at %.2f&quot;, threshold))
  }

plot_pred_type_distribution(
  df = tibble::tibble(pred = model_glm_1_pred, actual = test$default), 
  threshold = 0.5)</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-63-1.png" width="672" /></p>
<p>We can derive a few things from the plot:</p>
<ul>
<li>We can clearly see the imbalance of the test dataset with many more observations being <code>FALSE</code>, i.e.Â non-defaulted</li>
<li>We can see that predictions tend to cluster more around the center (i.e.Â the threshold) rather than the top or bottom</li>
<li>We see that there are only seven distinct (unique) prediction probabilities, namely 0.512386, 0.2198321, 0.3787367, 0.6010975, 0.6739447, 0.8152174, 0.7301686 which corresponds to the number of unique levels in our predictor <code>grade</code>, namely B, C, A, E, F, D, G</li>
<li>We can see the trade-off of moving the threshold down or up leading to higher sensitivity (lower specificity) or higher specificity (lower sensitivity), respectively.</li>
</ul>
<div id="roc-curve" class="section level3">
<h3><span class="header-section-number">12.6.1</span> ROC curve</h3>
<p>Letâs have a look at the ROC curve which evaluates the performance over all possible thresholds rather than just one. Without going into too much detail, the ROC curve works as follows:</p>
<ul>
<li>draw sensitivity (true positive rate) on y-axis</li>
<li>draw specificivity (true negative rate) on x-axis (often also drawn as 1 - true negative rate)</li>
<li>go from threshold zero to threshold one in a number of pre-defined steps, e.g.Â with distance 0.01</li>
<li>for each threshold sensitivity is plotted against specificity</li>
<li>the result is usually a (smoothed) curve that lies above the diagonal splitting the area</li>
<li>the diagonal (also called base) represents the lower bound as it is the result of a random guess (assuming balanced class distribution) - so if the curve lies below it, the model is worse than random guessing and one could simply invert the prediction</li>
<li>in a perfect model the curve touches the upper left data point, i.e.Â having a sensitivity and specificity of one (perfect separation)</li>
<li>This setup nicely illustrates the trade-off between sensitivity and specificity as it often occurs in reality, i.e.Â increasing one, decreases the other</li>
<li>The curve also allows comparing different models against each other</li>
</ul>
<p>Although the curve gives a good indication of model performance, it is usually preferrable to have one model statistic to use as benchmark. This is the role of the AUC (AUROC) known as area under the curve (area under the ROC curve). It is simply calculated via integration. Naturally, the base is 0.5 (the diagonal) and thus the lower bound. The upper bound is therefore 1.</p>
<p>More details can be found e.g.Â in:</p>
<ul>
<li><a href="http://corysimon.github.io/articles/what-is-an-roc-curve/">ROC curves to evaluate binary classification algorithms</a></li>
<li><a href="http://www.joyofdata.de/blog/illustrated-guide-to-roc-and-auc/">Illustrated Guide to ROC and AUC</a></li>
<li>or for a more theoretical treatment <a href="https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf">An introduction to ROC analysis</a>.</li>
</ul>
<p>We will be using the <code>pROC</code> library and its main function <code>pROC::roc()</code> to compute ROC.</p>
<pre class="r"><code>roc_glm_1 &lt;- pROC::roc(response = test$default, predictor = model_glm_1_pred)
roc_glm_1</code></pre>
<pre><code>## 
## Call:
## roc.default(response = test$default, predictor = model_glm_1_pred)
## 
## Data: model_glm_1_pred in 173039 controls (test$default FALSE) &lt; 4436 cases (test$default TRUE).
## Area under the curve: 0.6641</code></pre>
<p>We see an area of 0.6641045 which is better than random guessing but not too good. Letâs plot it to see its shape. Note that by default <code>pROC::plot.roc()</code> has the x-axis as specificity rather than 1 - specificity as many authors and other packages have. To use 1 - specificity, one can call the function with parameter <code>legacy.axes = TRUE</code>. However, it seems that the actual trade-off between sensitivity and specificity is easier to see when plotting specificity as the mind does not easily interpret one minus some quantity. We also set <code>asp = NA</code> to create an x-axis range from zero to one which, depending on your graphics output settings, ensures the standard way of plotting an ROC curve accepting the risk of a misshape in case graphics output does not have (close to) quadratic shape.</p>
<pre class="r"><code>pROC::plot.roc(x = roc_glm_1, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
               col = &quot;green&quot;, print.auc = FALSE, print.auc.y = .4)

legend(x = &quot;bottomright&quot;, legend=c(&quot;glm_1 AUC = 0.664&quot;), 
       col = c(&quot;green&quot;), lty = 1, cex = 1.0)</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-65-1.png" width="672" /></p>
<p>We see that the curve lies over the diagonal but it does not have a strong tendency to touch the upper left corner. A more complex model may perform better but would involve a larger amount of predictors so naturally the question of variable selection will come up.</p>
</div>
</div>
<div id="variable-selection" class="section level2">
<h2><span class="header-section-number">12.7</span> Variable selection</h2>
<p>Many articles and book chapters have been written on the topic of variable selection (feature selection) and some authors believe it is one of, if not the most, important topic in model buidling. A few good resources giving an overview, can be found below:</p>
<ul>
<li><a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning - Chapter 6: Linear Model Selection and Regularization</a></li>
<li><a href="http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf">An Introduction to Variable and Feature Selection</a></li>
<li><a href="https://link.springer.com/article/10.1057%2Fjt.2009.26">Variable selection methods in regression: Ignorable problem, outing notable solution</a></li>
</ul>
<p>The different approaches can be categorized as follows:</p>
<ul>
<li>subset selection</li>
<li>best subset selection</li>
<li>stepwise selection (forward/backward/hybrid)</li>
<li>shrinkage</li>
<li>ridge regression (l<sub>2</sub>-norm)</li>
<li>lasso (l<sub>1</sub>-norm)</li>
<li>dimension reduction</li>
<li>principal component analysis (PCA)</li>
</ul>
<p>There are many trade-offs to be made when choosing one approach over the other but the two most important are statistical soundness and computational efficiency. We will not go into further details of variable selection but rather focus on the model building. We thus simply detrmine a predictor space that seems sensible given prior analysis.</p>
<p>Letâs remind ourselves of variables left for modeling.</p>
<pre class="r"><code>full_vars &lt;- colnames(train_down)
full_vars</code></pre>
<pre><code>##  [1] &quot;term&quot;                        &quot;int_rate&quot;                   
##  [3] &quot;grade&quot;                       &quot;emp_length&quot;                 
##  [5] &quot;home_ownership&quot;              &quot;annual_inc&quot;                 
##  [7] &quot;verification_status&quot;         &quot;issue_d&quot;                    
##  [9] &quot;pymnt_plan&quot;                  &quot;purpose&quot;                    
## [11] &quot;addr_state&quot;                  &quot;dti&quot;                        
## [13] &quot;delinq_2yrs&quot;                 &quot;earliest_cr_line&quot;           
## [15] &quot;inq_last_6mths&quot;              &quot;open_acc&quot;                   
## [17] &quot;pub_rec&quot;                     &quot;revol_bal&quot;                  
## [19] &quot;revol_util&quot;                  &quot;initial_list_status&quot;        
## [21] &quot;out_prncp_inv&quot;               &quot;total_rec_late_fee&quot;         
## [23] &quot;collection_recovery_fee&quot;     &quot;last_pymnt_d&quot;               
## [25] &quot;last_pymnt_amnt&quot;             &quot;last_credit_pull_d&quot;         
## [27] &quot;collections_12_mths_ex_med&quot;  &quot;mths_since_last_major_derog&quot;
## [29] &quot;application_type&quot;            &quot;acc_now_delinq&quot;             
## [31] &quot;tot_coll_amt&quot;                &quot;tot_cur_bal&quot;                
## [33] &quot;default&quot;</code></pre>
<p>We determine a subspace and report which variables we ignore.</p>
<pre class="r"><code>model_vars &lt;-
  c(&quot;term&quot;, &quot;grade&quot;, &quot;emp_length&quot;, &quot;home_ownership&quot;, &quot;annual_inc&quot;,
    &quot;purpose&quot;, &quot;pymnt_plan&quot;, &quot;out_prncp_inv&quot;, &quot;delinq_2yrs&quot;, &quot;default&quot;)

ignored_vars &lt;- dplyr::setdiff(full_vars, model_vars)
ignored_vars</code></pre>
<pre><code>##  [1] &quot;int_rate&quot;                    &quot;verification_status&quot;        
##  [3] &quot;issue_d&quot;                     &quot;addr_state&quot;                 
##  [5] &quot;dti&quot;                         &quot;earliest_cr_line&quot;           
##  [7] &quot;inq_last_6mths&quot;              &quot;open_acc&quot;                   
##  [9] &quot;pub_rec&quot;                     &quot;revol_bal&quot;                  
## [11] &quot;revol_util&quot;                  &quot;initial_list_status&quot;        
## [13] &quot;total_rec_late_fee&quot;          &quot;collection_recovery_fee&quot;    
## [15] &quot;last_pymnt_d&quot;                &quot;last_pymnt_amnt&quot;            
## [17] &quot;last_credit_pull_d&quot;          &quot;collections_12_mths_ex_med&quot; 
## [19] &quot;mths_since_last_major_derog&quot; &quot;application_type&quot;           
## [21] &quot;acc_now_delinq&quot;              &quot;tot_coll_amt&quot;               
## [23] &quot;tot_cur_bal&quot;</code></pre>
</div>
<div id="a-more-complex-logistic-model-with-caret" class="section level2">
<h2><span class="header-section-number">12.8</span> A more complex logistic model with caret</h2>
<p>The interested reader should check the comprehensive caret documentation on how the API works and for modeling in particular read <a href="https://topepo.github.io/caret/model-training-and-tuning.html">model training and tuning</a>. We only give a high-level overview here.</p>
<p>The <code>caret</code> package has several functions that attempt to streamline the model building and evaluation process. The train function can be used to</p>
<ul>
<li>evaluate, using resampling, the effect of model tuning parameters on performance</li>
<li>choose the âoptimalâ model across these parameters</li>
<li>estimate model performance from a training set</li>
</ul>
<p>First, a specific model must be chosen. User-defined models can also be created. Second, the corresponding model parameters need to be passed to the function call. The workhorse function is <code>caret::train()</code> but an important input is parameter <code>trControl</code> which also allows specifying a particular resampling approach, e.g.Â k-fold cross-validation (once or repeated), leave-one-out cross-validation and bootstrap (simple estimation or the 632 rule), etc.</p>
<div id="traincontrol" class="section level3">
<h3><span class="header-section-number">12.8.1</span> trainControl</h3>
<p>We will define the controlling parameters in a separate function call to <code>caret::trainControl()</code> and then pass this on to <code>caret::train(trControl = )</code>. This also allows using the control parameters for different models if applicable to those. The function <code>caret::trainControl()</code> generates parameters that further control how models are created. For more details, see <a href="https://topepo.github.io/caret/model-training-and-tuning.html">The caret package 5.5.4: The trainControl Function</a>.</p>
<p>We perform a 10-fold cross-validation repeated five times and return the class probabilities. Since we are interested in ROC as our performance measure, we need to set <code>summaryFunction = twoClassSummary</code>. If one likes to see a live log of the execution, <code>verboseIter = TRUE</code> provides that. Since we have no tuning parameters for logistic regression, we leave those out for now.</p>
<pre class="r"><code>ctrl &lt;- 
  trainControl(method = &quot;repeatedcv&quot;, 
               number = 10,
               repeats = 5,
               classProbs = TRUE,
               summaryFunction = twoClassSummary,
               savePredictions = TRUE,
               verboseIter = FALSE)</code></pre>
</div>
<div id="train" class="section level3">
<h3><span class="header-section-number">12.8.2</span> train</h3>
<p>Finally, we can call <code>caret::train()</code> with specified control parameters. Note that <code>caret</code> is picky about factor values which need to be valid R names. We thus convert the default variable to have values âyesâ and ânoâ rather than âTRUEâ and âFALSEâ (which when converted to variable names would not be valid as they are reserved keywords). Make sure that the test set is transformed likewise when using the model for prediction as otherwise your factor levels may be different to the training set and the class probabilities thus wrong.</p>
<p>We will skip the complex discussion around variable selection for now and simply determine the âbaseâ variable universe that seems plausible from intuition and prior exploratory analysis. We have removed categorical variables with many unique values (such as zip codes) as they would be mutated to large dummy matrices.</p>
<pre class="r"><code>model_glm_2 &lt;-
  train_down %&gt;%
  select(model_vars) %&gt;%
  mutate(default = as.factor(ifelse(default == TRUE, &quot;yes&quot;, &quot;no&quot;))) %&gt;%
  filter(complete.cases(.)) %&gt;%
  train(default ~ ., 
        data = ., 
        method = &quot;glm&quot;, 
        family = &quot;binomial&quot;,
        metric = &quot;ROC&quot;,
        trControl = ctrl)</code></pre>
<p>As with the previous model object created via <code>stats::glm()</code>, the final model object created has many methods and attributes for further processing. But we first look at the results/performance on the training set (which should give a fair estimate of the test set performance since we used cross-validation). Note that all reported metrics are interpretable since this is done on the balanced data.</p>
<pre class="r"><code>model_glm_2</code></pre>
<pre><code>## Generalized Linear Model 
## 
## 35488 samples
##     9 predictor
##     2 classes: &#39;no&#39;, &#39;yes&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 31938, 31938, 31940, 31939, 31940, 31940, ... 
## Resampling results:
## 
##   ROC        Sens       Spec    
##   0.6964721  0.6291356  0.662019</code></pre>
<p>We can see that we have improved on the ROCAUC (~ 70%) but got worse for sensitivity compared to the first, simpler model. Looking at the <code>predictors</code>, we also see how the model created dummies to encode categorical variables.</p>
<pre class="r"><code>attributes(model_glm_2)</code></pre>
<pre><code>## $names
##  [1] &quot;method&quot;       &quot;modelInfo&quot;    &quot;modelType&quot;    &quot;results&quot;     
##  [5] &quot;pred&quot;         &quot;bestTune&quot;     &quot;call&quot;         &quot;dots&quot;        
##  [9] &quot;metric&quot;       &quot;control&quot;      &quot;finalModel&quot;   &quot;preProcess&quot;  
## [13] &quot;trainingData&quot; &quot;resample&quot;     &quot;resampledCM&quot;  &quot;perfNames&quot;   
## [17] &quot;maximize&quot;     &quot;yLimits&quot;      &quot;times&quot;        &quot;levels&quot;      
## [21] &quot;terms&quot;        &quot;coefnames&quot;    &quot;contrasts&quot;    &quot;xlevels&quot;     
## 
## $class
## [1] &quot;train&quot;         &quot;train.formula&quot;</code></pre>
<pre class="r"><code>summary(model_glm_2)</code></pre>
<pre><code>## 
## Call:
## NULL
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.3163  -1.0774  -0.2782   1.0635   5.6347  
## 
## Coefficients:
##                                 Estimate     Std. Error z value
## (Intercept)                -1.7381657126   0.1420955332 -12.232
## termX60.months             -0.5453038253   0.0298754606 -18.253
## gradeB                      0.8159277950   0.0461745543  17.671
## gradeC                      1.3693629935   0.0459015145  29.833
## gradeD                      1.7227789216   0.0490499846  35.123
## gradeE                      2.0514999392   0.0562719050  36.457
## gradeF                      2.3642399559   0.0741417266  31.888
## gradeG                      2.8775473195   0.1315002573  21.882
## emp_lengthX..1.year         0.0719719375   0.0634309986   1.135
## emp_lengthX1.year          -0.0231801001   0.0663542839  -0.349
## emp_lengthX10..years       -0.1447186450   0.0541082748  -2.675
## emp_lengthX2.years         -0.0155518922   0.0628511985  -0.247
## emp_lengthX3.years          0.0019357572   0.0640494674   0.030
## emp_lengthX4.years         -0.0416200714   0.0681959839  -0.610
## emp_lengthX5.years         -0.1208526942   0.0671199278  -1.801
## emp_lengthX6.years          0.0685014427   0.0713774190   0.960
## emp_lengthX7.years          0.0169740882   0.0708191548   0.240
## emp_lengthX8.years         -0.0231357344   0.0711988793  -0.325
## emp_lengthX9.years         -0.0454158473   0.0751319778  -0.604
## home_ownershipNONE         -0.5795772695   1.3066383779  -0.444
## home_ownershipOTHER        12.3599134059  91.3562921237   0.135
## home_ownershipOWN           0.1059366445   0.0397823985   2.663
## home_ownershipRENT          0.1738380528   0.0253881630   6.847
## annual_inc                 -0.0000018620   0.0000002708  -6.875
## purposecredit_card          0.0940714394   0.1290711042   0.729
## purposedebt_consolidation   0.1957294997   0.1273405178   1.537
## purposeeducational          1.4894997798   0.4601526184   3.237
## purposehome_improvement     0.2605554222   0.1355011685   1.923
## purposehouse                0.0010870105   0.2087251602   0.005
## purposemajor_purchase       0.1111783795   0.1507309963   0.738
## purposemedical              0.1525955150   0.1664861916   0.917
## purposemoving               0.4431876570   0.1857656345   2.386
## purposeother                0.0979564862   0.1356860122   0.722
## purposerenewable_energy     0.0910160004   0.4132422786   0.220
## purposesmall_business       0.2405034075   0.1572708880   1.529
## purposevacation             0.1203054032   0.1926193435   0.625
## purposewedding             -0.4075063208   0.3202319627  -1.273
## pymnt_plany                11.9809020840 154.7613613910   0.077
## out_prncp_inv               0.0000520918   0.0000017015  30.614
## delinq_2yrs                 0.0865718494   0.0125320528   6.908
##                                       Pr(&gt;|z|)    
## (Intercept)               &lt; 0.0000000000000002 ***
## termX60.months            &lt; 0.0000000000000002 ***
## gradeB                    &lt; 0.0000000000000002 ***
## gradeC                    &lt; 0.0000000000000002 ***
## gradeD                    &lt; 0.0000000000000002 ***
## gradeE                    &lt; 0.0000000000000002 ***
## gradeF                    &lt; 0.0000000000000002 ***
## gradeG                    &lt; 0.0000000000000002 ***
## emp_lengthX..1.year                    0.25652    
## emp_lengthX1.year                      0.72684    
## emp_lengthX10..years                   0.00748 ** 
## emp_lengthX2.years                     0.80457    
## emp_lengthX3.years                     0.97589    
## emp_lengthX4.years                     0.54166    
## emp_lengthX5.years                     0.07177 .  
## emp_lengthX6.years                     0.33720    
## emp_lengthX7.years                     0.81058    
## emp_lengthX8.years                     0.74522    
## emp_lengthX9.years                     0.54552    
## home_ownershipNONE                     0.65736    
## home_ownershipOTHER                    0.89238    
## home_ownershipOWN                      0.00775 ** 
## home_ownershipRENT            0.00000000000753 ***
## annual_inc                    0.00000000000621 ***
## purposecredit_card                     0.46610    
## purposedebt_consolidation              0.12428    
## purposeeducational                     0.00121 ** 
## purposehome_improvement                0.05449 .  
## purposehouse                           0.99584    
## purposemajor_purchase                  0.46076    
## purposemedical                         0.35937    
## purposemoving                          0.01705 *  
## purposeother                           0.47033    
## purposerenewable_energy                0.82568    
## purposesmall_business                  0.12621    
## purposevacation                        0.53225    
## purposewedding                         0.20318    
## pymnt_plany                            0.93829    
## out_prncp_inv             &lt; 0.0000000000000002 ***
## delinq_2yrs                   0.00000000000491 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 49197  on 35487  degrees of freedom
## Residual deviance: 44806  on 35448  degrees of freedom
## AIC: 44886
## 
## Number of Fisher Scoring iterations: 11</code></pre>
<pre class="r"><code>predictors(model_glm_2)</code></pre>
<pre><code>##  [1] &quot;termX60.months&quot;            &quot;gradeB&quot;                   
##  [3] &quot;gradeC&quot;                    &quot;gradeD&quot;                   
##  [5] &quot;gradeE&quot;                    &quot;gradeF&quot;                   
##  [7] &quot;gradeG&quot;                    &quot;emp_lengthX..1.year&quot;      
##  [9] &quot;emp_lengthX1.year&quot;         &quot;emp_lengthX10..years&quot;     
## [11] &quot;emp_lengthX2.years&quot;        &quot;emp_lengthX3.years&quot;       
## [13] &quot;emp_lengthX4.years&quot;        &quot;emp_lengthX5.years&quot;       
## [15] &quot;emp_lengthX6.years&quot;        &quot;emp_lengthX7.years&quot;       
## [17] &quot;emp_lengthX8.years&quot;        &quot;emp_lengthX9.years&quot;       
## [19] &quot;home_ownershipNONE&quot;        &quot;home_ownershipOTHER&quot;      
## [21] &quot;home_ownershipOWN&quot;         &quot;home_ownershipRENT&quot;       
## [23] &quot;annual_inc&quot;                &quot;purposecredit_card&quot;       
## [25] &quot;purposedebt_consolidation&quot; &quot;purposeeducational&quot;       
## [27] &quot;purposehome_improvement&quot;   &quot;purposehouse&quot;             
## [29] &quot;purposemajor_purchase&quot;     &quot;purposemedical&quot;           
## [31] &quot;purposemoving&quot;             &quot;purposeother&quot;             
## [33] &quot;purposerenewable_energy&quot;   &quot;purposesmall_business&quot;    
## [35] &quot;purposevacation&quot;           &quot;purposewedding&quot;           
## [37] &quot;pymnt_plany&quot;               &quot;out_prncp_inv&quot;            
## [39] &quot;delinq_2yrs&quot;</code></pre>
<p>We can also look at the variable importance with <code>caret::varImp()</code>. For linear models, the absolute value of the t-statistic for each model parameter is used.</p>
<pre class="r"><code>varImp(model_glm_2)</code></pre>
<pre><code>## glm variable importance
## 
##   only 20 most important variables shown (out of 39)
## 
##                           Overall
## gradeE                    100.000
## gradeD                     96.340
## gradeF                     87.466
## out_prncp_inv              83.972
## gradeC                     81.827
## gradeG                     60.017
## termX60.months             50.059
## gradeB                     48.462
## delinq_2yrs                18.937
## annual_inc                 18.846
## home_ownershipRENT         18.770
## purposeeducational          8.866
## emp_lengthX10..years        7.323
## home_ownershipOWN           7.291
## purposemoving               6.531
## purposehome_improvement     5.261
## emp_lengthX5.years          4.925
## purposedebt_consolidation   4.202
## purposesmall_business       4.181
## purposewedding              3.477</code></pre>
<pre class="r"><code>plot(varImp(model_glm_2))</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-72-1.png" width="672" /></p>
<p>Finally, we use the model to predict the test set classes. Again, we need to be aware of any pre-processing required such as mutations or treatment of missing values.</p>
<pre class="r"><code>model_glm_2_pred &lt;- 
  predict(model_glm_2, 
          newdata = test %&gt;% 
            select(model_vars) %&gt;%
            mutate(default = as.factor(ifelse(default == TRUE, 
                                              &quot;yes&quot;, &quot;no&quot;))) %&gt;%
            filter(complete.cases(.)), 
          type = &quot;prob&quot;)

head(model_glm_2_pred, 3)</code></pre>
<pre><code>##          no       yes
## 1 0.7041954 0.2958046
## 2 0.8783103 0.1216897
## 3 0.6509819 0.3490181</code></pre>
<p>Note that <code>caret::predict.train</code> returns the probabilties for all outcomes i.e.Â in our binary case for yes and no. We repeat the analysis from earlier and see how the new model performs.</p>
<pre class="r"><code>caret::confusionMatrix(
  data = ifelse(model_glm_2_pred[, &quot;yes&quot;] &gt; 0.5, &quot;yes&quot;, &quot;no&quot;), 
  reference = as.factor(ifelse(test[complete.cases(test[, model_vars]), 
                                    &quot;default&quot;] == TRUE, &quot;yes&quot;, &quot;no&quot;)),
  positive = &quot;yes&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction     no    yes
##        no  109156   1482
##        yes  63876   2953
##                                              
##                Accuracy : 0.6317             
##                  95% CI : (0.6295, 0.634)    
##     No Information Rate : 0.975              
##     P-Value [Acc &gt; NIR] : 1                  
##                                              
##                   Kappa : 0.0378             
##  Mcnemar&#39;s Test P-Value : &lt;0.0000000000000002
##                                              
##             Sensitivity : 0.66584            
##             Specificity : 0.63084            
##          Pos Pred Value : 0.04419            
##          Neg Pred Value : 0.98660            
##              Prevalence : 0.02499            
##          Detection Rate : 0.01664            
##    Detection Prevalence : 0.37657            
##       Balanced Accuracy : 0.64834            
##                                              
##        &#39;Positive&#39; Class : yes                
## </code></pre>
<p>Again we build an ROC object.</p>
<pre class="r"><code>temp &lt;- as.factor(ifelse(test[complete.cases(test[, model_vars]),
                                             &quot;default&quot;] == TRUE, &quot;yes&quot;, &quot;no&quot;))

roc_glm_2 &lt;- 
  pROC::roc(response = temp, 
            predictor = model_glm_2_pred[, &quot;yes&quot;])

roc_glm_2</code></pre>
<pre><code>## 
## Call:
## roc.default(response = temp, predictor = model_glm_2_pred[, &quot;yes&quot;])
## 
## Data: model_glm_2_pred[, &quot;yes&quot;] in 173032 controls (temp no) &lt; 4435 cases (temp yes).
## Area under the curve: 0.6995</code></pre>
<pre class="r"><code>#data = as.data.frame(unclass(train[complete.cases(train), ]))</code></pre>
<p>The ROCAUC of ~ 70% is nearly the same as for the cross-validated training set. We plot the curve comparing it against the ROC from the simple model.</p>
<pre class="r"><code>pROC::plot.roc(x = roc_glm_1, legacy.axes = FALSE, xlim = c(1, 0), asp = NA, 
               col = &quot;green&quot;)

pROC::plot.roc(x = roc_glm_2, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
               add = TRUE, col = &quot;blue&quot;)

legend(x = &quot;bottomright&quot;, legend=c(&quot;glm_1 AUC = 0.664&quot;, &quot;glm_2 AUC = 0.7&quot;), 
       col = c(&quot;green&quot;, &quot;blue&quot;), lty = 1, cex = 1.0)</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-76-1.png" width="672" /></p>
<p>We can see that the more complex model has a better ROC above the simple model.</p>
</div>
</div>
<div id="a-note-on-computational-efficiency-parallelization-via-cluster" class="section level2">
<h2><span class="header-section-number">12.9</span> A note on computational efficiency (parallelization via cluster)</h2>
<p>Some models (and their tuning parameter calibration via resampling) will require significant computational resources both in terms of memory and CPUs. There are a few ways to increase computational efficiency which can be roughly divided into</p>
<ul>
<li>decrease training set (without loosing information)</li>
<li>compute on several cores (parallelization via cluster)</li>
<li>shift matrix computations to GPU (e.g.Â via CUDA)</li>
<li>use compute instances from cloud offerings such as Amazon Web Services, googleCloud or Microsoft Azure</li>
</ul>
<p>We will only look at parallelization here as it is supported by R and <code>caret</code> out of the box. For details, see <a href="https://topepo.github.io/caret/parallel-processing.html">The caret package 9: Parallel Processing</a>, <a href="https://github.com/tobigithub/caret-machine-learning/wiki/caret-ml-parallel">caret ml parallel</a> and <a href="https://github.com/tobigithub/R-parallel/wiki">HOME of the R parallel WIKI!</a> by Tobias Kind. There are a couple of libraries supporting parallel processing in R but one should be aware of the support for different operating systems. On Windows, one should use <a href="https://cran.r-project.org/web/packages/doParallel">doParallel</a> which is a parallel backend for the <code>foreach</code> package. Note that the multicore functionality only runs tasks on a single computer, not a cluster of computers. However, you can use the snow functionality to execute on a cluster, using Unix-like operating systems, Windows, or even a combination. It is pointless to use doParallel and parallel on a machine with only one processor with a single core. To get a speed improvement, it must run on a machine with multiple processors, multiple cores, or both. Remember: unless <code>doParallel::registerDoParallel()</code> is called, <code>foreach</code> will not run in parallel. Simply loading the <code>doParallel</code> package is not enough. Note that we can set the parameter <code>caret::trainControl(allowParallel = TRUE)</code> but it is actually the default setting, i.e. <code>caret::train()</code> will automatically run in parallel if it detects a registerd cluster, irrespective of library is used to build it.</p>
<p>To illustrate efficiency gains, we will benchmark an example model training using the previous <code>glm</code> model. We use library (and function with same name) <code>microbenchmark</code> and define to execute the operation several times (to get a stable time estimate) via parameter <code>times</code>. Note that parallizing usually means creating copies of the original data for each worker (core) which requires according memory. Since we only want to illustrate the logic here, we downsize the training data to avoid failures due to insufficient memory.</p>
<pre class="r"><code>benchmark_train_data &lt;- 
  train_down[seq(1, nrow(train_down), 2), model_vars] %&gt;%
  mutate(default = as.factor(ifelse(default == TRUE, &quot;yes&quot;, &quot;no&quot;))) %&gt;%
  filter(complete.cases(.))
format(utils::object.size(benchmark_train_data), units = &quot;Mb&quot;)</code></pre>
<pre><code>## [1] &quot;1.3 Mb&quot;</code></pre>
<p>We see the data has ~ 1 Mb in size.</p>
<pre class="r"><code>ctrl &lt;- 
  trainControl(method = &quot;repeatedcv&quot;, 
               number = 10,
               repeats = 5,
               classProbs = TRUE,
               summaryFunction = twoClassSummary,
               verboseIter = FALSE,
               allowParallel = TRUE) #default

library(microbenchmark)

glm_nopar &lt;-
  microbenchmark(glm_nopar =
      train(default ~ .,
            data = benchmark_train_data,
            method = &quot;glm&quot;,
            family = &quot;binomial&quot;,
            metric = &quot;ROC&quot;,
            trControl = ctrl),
    times = 5
    )</code></pre>
<pre><code>## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading</code></pre>
<p>We can look at the resulting run time.</p>
<pre class="r"><code>glm_nopar</code></pre>
<pre><code>## Unit: seconds
##       expr      min       lq     mean   median       uq     max neval
##  glm_nopar 27.51427 27.90744 28.27665 28.38591 28.71571 28.8599     5</code></pre>
<p>Now, we will setup a cluster of cores and repeat the benchmarking. We use <code>parallel::detectCores()</code> to establish how many cores are available and use some amount smaller 100% of cores to leave some resources for the system. Usually, we would write a log of the distributed calculation via <code>parallel::makeCluster(outfile = )</code>. It will be opened in append mode as all workers write to it. However, we found issues with running <code>parallel::makeCluster(outfile = )</code> so we simply run the cluster directly from <code>registerDoParallel()</code> which has no option to define a log file.</p>
<pre class="r"><code># doParallel would also load parallel
library(parallel)
library(doParallel)</code></pre>
<pre><code>## Warning: package &#39;doParallel&#39; was built under R version 3.4.2</code></pre>
<pre><code>## Loading required package: foreach</code></pre>
<pre><code>## 
## Attaching package: &#39;foreach&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:purrr&#39;:
## 
##     accumulate, when</code></pre>
<pre><code>## Loading required package: iterators</code></pre>
<pre class="r"><code>cores_2_use &lt;- floor(0.8 * detectCores())
# cl &lt;- makeCluster(cores_2_use, outfile = &quot;./logs/parallel_log.txt&quot;)
# registerDoParallel(cl, cores_2_use)
registerDoParallel(cores = cores_2_use)
getDoParWorkers()</code></pre>
<pre><code>## [1] 6</code></pre>
<p>We can inspect the open connections via <code>base::showConnections()</code>.</p>
<pre class="r"><code>knitr::kable(base::showConnections())</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left">description</th>
<th align="left">class</th>
<th align="left">mode</th>
<th align="left">text</th>
<th align="left">isopen</th>
<th align="left">can read</th>
<th align="left">can write</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>4</td>
<td align="left">output</td>
<td align="left">textConnection</td>
<td align="left">wr</td>
<td align="left">text</td>
<td align="left">opened</td>
<td align="left">no</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td>5</td>
<td align="left">&lt;-eclipse:11076</td>
<td align="left">sockconn</td>
<td align="left">a+b</td>
<td align="left">binary</td>
<td align="left">opened</td>
<td align="left">yes</td>
<td align="left">yes</td>
</tr>
<tr class="odd">
<td>6</td>
<td align="left">&lt;-eclipse:11076</td>
<td align="left">sockconn</td>
<td align="left">a+b</td>
<td align="left">binary</td>
<td align="left">opened</td>
<td align="left">yes</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td>7</td>
<td align="left">&lt;-eclipse:11076</td>
<td align="left">sockconn</td>
<td align="left">a+b</td>
<td align="left">binary</td>
<td align="left">opened</td>
<td align="left">yes</td>
<td align="left">yes</td>
</tr>
<tr class="odd">
<td>8</td>
<td align="left">&lt;-eclipse:11076</td>
<td align="left">sockconn</td>
<td align="left">a+b</td>
<td align="left">binary</td>
<td align="left">opened</td>
<td align="left">yes</td>
<td align="left">yes</td>
</tr>
<tr class="even">
<td>9</td>
<td align="left">&lt;-eclipse:11076</td>
<td align="left">sockconn</td>
<td align="left">a+b</td>
<td align="left">binary</td>
<td align="left">opened</td>
<td align="left">yes</td>
<td align="left">yes</td>
</tr>
<tr class="odd">
<td>10</td>
<td align="left">&lt;-eclipse:11076</td>
<td align="left">sockconn</td>
<td align="left">a+b</td>
<td align="left">binary</td>
<td align="left">opened</td>
<td align="left">yes</td>
<td align="left">yes</td>
</tr>
</tbody>
</table>
<p>Now we tried running in parallel but the setup may produce an error on Windows.</p>
<p><code>Error in e$fun(obj, substitute(ex), parent.frame(), e$data) : unable to find variable &quot;optimismBoot&quot;</code></p>
<p>This seems to be a bug as noted in SO question <a href="https://stackoverflow.com/questions/46244763/caret-train-function-unable-to-find-variable-optimismboot">Caret train function - unable to find variable âoptimismBootâ</a>. The bug seems to be solved in <code>caret</code> development version which can be installed from GitHub via <code>devtools::install_github('topepo/caret/pkg/caret')</code> (note that building <code>caret</code> from source requires <code>RBuildTools</code> which are part of <code>RTools</code> which need to be installed separately, e.g.Â from <a href="https://cran.rstudio.com/bin/windows/Rtools">Building R for Windows</a> - RStudio may manage the install for you). The issue is also tracked on GitHub under <a href="https://github.com/topepo/caret/issues/706">optimismBoot error #706</a> and may already be resolved in the <code>caret</code> version you are using.</p>
<p>We do not run the install of the development version here again but you should in case you do not have it and wish to test the parallel execution. Before you do, make sure to close the cluster with below code (not executed now as we continue).</p>
<pre class="r"><code>parallel::stopCluster(cl)
foreach::registerDoSEQ()</code></pre>
<pre class="r"><code>glm_par &lt;-
  microbenchmark(glm_par =
    train(default ~ .,
            data = benchmark_train_data,
            method = &quot;glm&quot;,
            family = &quot;binomial&quot;,
            metric = &quot;ROC&quot;,
            trControl = ctrl),
    times = 5
    )</code></pre>
<p>We make sure to stop the cluster and force R back to sequential execution by using <code>foreach::registerDoSEQ()</code>. In case you used the setup via <code>parallel::makeCluster()</code>, you need to call <code>parallel::stopCluster()</code> first. Otherwise, memory and CPU may be occupied with legacy clusters.</p>
<pre class="r"><code>#parallel::stopCluster(cl)
foreach::registerDoSEQ()</code></pre>
<p>Note that we ran into issues when using too much system resources on a Windows 7 workstation as documented in the SO question <a href="https://stackoverflow.com/questions/46119014/caret-train-binary-glm-fails-on-parallel-cluster-via-doparallel/46119519?noredirect=1#comment79203772_46119519">caret train binary glm fails on parallel cluster via doParallel</a>. Below error was produced.</p>
<p><code>Error in serialize(data, node$con) : error writing to connection</code></p>
<p>However, on a Windows 10 machine with fewer cores and development version of <code>caret</code> the issue could not be reproduced.</p>
<p>We have also tried running <code>caret::train()</code> parallel on Linux (Ubuntu) via the <code>doMC</code> library and found that below setting runs just fine and also faster than a sequential execution. We only include the code here for info without execution.</p>
<pre class="r"><code>library(doMC)

cores_2_use &lt;- floor(0.8 * parallel::detectCores())
registerDoMC(cores_2_use)

microbenchmark(
  glm_par =
    train(y ~ .,
          data = df,
          method = &quot;glm&quot;,
          family = &quot;binomial&quot;,
          metric = &quot;ROC&quot;,
          trControl = ctrl),
  times = 5)</code></pre>
<p>Finally, letâs compare parallel versus non-parallel execution time. We can see that the parallel setup performed significantly better but we would have expected even more as we are using a multiple of the one-core setup.</p>
<pre class="r"><code>glm_nopar</code></pre>
<pre><code>## Unit: seconds
##       expr      min       lq     mean   median       uq     max neval
##  glm_nopar 27.51427 27.90744 28.27665 28.38591 28.71571 28.8599     5</code></pre>
<pre class="r"><code>glm_par</code></pre>
<pre><code>## Unit: seconds
##     expr      min       lq     mean  median       uq      max neval
##  glm_par 13.65229 13.79392 17.34424 13.8397 13.89238 31.54293     5</code></pre>
</div>
<div id="a-note-on-tuning-parameters-in-caret" class="section level2">
<h2><span class="header-section-number">12.10</span> A note on tuning parameters in caret</h2>
<p>When it comes to more complex models in general, often tuning parameters can be used to improve the model. There are different ways to provide the range of parameters to take. In particular, one may use</p>
<ul>
<li>pre-defined number (e.g.Â if domain knowledge already provides and idea)</li>
<li>use <a href="https://topepo.github.io/caret/model-training-and-tuning.html#custom">grid search</a> (default in <code>caret::train()</code>)</li>
<li>use <a href="https://topepo.github.io/caret/random-hyperparameter-search.html">random search</a></li>
</ul>
<p>In general, the more tuning parameters are available, the higher the computational cost of selecting them. We will show some applications of options below. For further information, see <a href="https://topepo.github.io/caret/model-training-and-tuning.html">The caret package 5: Model Training and Tuning</a></p>
</div>
<div id="trees" class="section level2">
<h2><span class="header-section-number">12.11</span> Trees</h2>
<p>Trees stratify or segment the predictor space into a number of simple regions. In order to make a prediction for a given observation, we typically use the mean or the mode of the training observations in the region to which it belongs. Trees are easy to interpret and built but as pointed out in <a href="http://www-bcf.usc.edu/~gareth/ISL/">âAn Introduction to Statistical Learningâ</a> they may not compete with the best supervised learning approaches:</p>
<blockquote>
<p>Tree-based methods are simple and useful for interpretation. However, they typically are not competitive with the best supervised learning approaches, such as those seen in Chapters 6 and 7, in terms of prediction accuracy. Hence in this chapter we also introduce bagging, random forests, and boosting. Each of these approaches involves producing multiple trees which are then combined to yield a single consensus prediction. We will see that combining a large number of trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss in interpretation.</p>
</blockquote>
<p>Following this guidance, we will also look into some alternatives / enhancements of the basic tree model used in <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">CART</a>. Some further resources on trees are found in</p>
<ul>
<li><a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning</a></li>
<li><a href="https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/">A Complete Tutorial on Tree Based Modeling from Scratch (in R &amp; Python)</a></li>
</ul>
<p>As usual, there are many ways to implement tree models in R but we will stay within the <code>caret</code> package and use the API that was already explained earlier.</p>
<div id="simple-tree-using-cart-via-rpart-in-caret" class="section level3">
<h3><span class="header-section-number">12.11.1</span> Simple tree using CART via rpart in caret</h3>
<p>We start by building a simple tree using the CART method via rpart in caret. Note that <code>caret</code> may not offer the full set of features that are available when using the <code>rpart</code> library directly but it should suffice for our illustration. For more details, see <a href="https://cran.r-project.org/web/packages/rpart/index.html">rpart on CRAN</a> and in particular <a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf">An Introduction to Recursive Partitioning Using the RPART Routines</a> for some background.</p>
<p>We define our control and train functions as before and look at the results.</p>
<pre class="r"><code>ctrl &lt;- 
  trainControl(method = &quot;repeatedcv&quot;, 
               number = 10,
               repeats = 5,
               classProbs = TRUE,
               summaryFunction = twoClassSummary,
               verboseIter = FALSE,
               allowParallel = TRUE)

# library(rpart)

model_rpart &lt;-
  train_down %&gt;%
  select(model_vars) %&gt;%
  mutate(default = as.factor(ifelse(default == TRUE, &quot;yes&quot;, &quot;no&quot;))) %&gt;%
  filter(complete.cases(.)) %&gt;%
  train(default ~ .,
        data = .,
        method = &#39;rpart&#39;,
        metric = &quot;ROC&quot;,
        preProc = c(&quot;center&quot;, &quot;scale&quot;),
        trControl = ctrl)</code></pre>
<pre><code>## Warning in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut
## = 10, : These variables have zero variances: home_ownershipNONE</code></pre>
<pre class="r"><code>model_rpart</code></pre>
<pre><code>## CART 
## 
## 35488 samples
##     9 predictor
##     2 classes: &#39;no&#39;, &#39;yes&#39; 
## 
## Pre-processing: centered (39), scaled (39) 
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 31939, 31938, 31940, 31939, 31940, 31939, ... 
## Resampling results across tuning parameters:
## 
##   cp          ROC        Sens       Spec     
##   0.01445641  0.7018554  0.5572830  0.7674466
##   0.03111086  0.6461547  0.3900115  0.8799842
##   0.26027166  0.5707699  0.6019078  0.5396320
## 
## ROC was used to select the optimal model using  the largest value.
## The final value used for the model was cp = 0.01445641.</code></pre>
<p>We see that ROCAUC is ~ 70% for the best model iteration but sensitivity is worse than in some previous models. <code>caret</code> stepped through some complexity parameters (cp) and printed results for them. We can look at <code>cp</code> visually for the different values. In fact, under <code>caret</code> this is the only tuning parameter available for <code>method = rpart</code>.</p>
<pre class="r"><code>ggplot(model_rpart)</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-88-1.png" width="672" /></p>
<p>We can also visualize the final tree by accessing the <code>finalModel</code> element from the <code>caret::train()</code> model object. We have to add the text manually to the plot via <code>graphics::text()</code>. We use a larger margin to have enough space for the text.</p>
<pre class="r"><code>plot(model_rpart$finalModel, uniform = TRUE, margin = 0.2)
graphics::text(model_rpart$finalModel)</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-89-1.png" width="672" /></p>
<p>Apparently, there are only two splits, for out_prncp_inv and gradeB.</p>
<p>Rather than using the default grid search for choosing the tuning parameter <code>cp</code>, we could use random search (as mentioned earlier). For a theoretical background on why this might be useful, see e.g. <a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">Random Search for Hyper-Parameter Optimization</a>. We implement random search via the parameter <code>caret::trainControl(search = &quot;random&quot;)</code>.</p>
<pre class="r"><code>ctrl &lt;- 
  trainControl(method = &quot;repeatedcv&quot;, 
               number = 10,
               repeats = 5,
               classProbs = TRUE,
               summaryFunction = twoClassSummary,
               verboseIter = FALSE,
               allowParallel = TRUE,
               search = &quot;random&quot;)

model_rpart &lt;-
  train_down %&gt;%
  select(model_vars) %&gt;%
  mutate(default = as.factor(ifelse(default == TRUE, &quot;yes&quot;, &quot;no&quot;))) %&gt;%
  filter(complete.cases(.)) %&gt;%
  train(default ~ .,
        data = .,
        method = &#39;rpart&#39;,
        metric = &quot;ROC&quot;,
        preProc = c(&quot;center&quot;, &quot;scale&quot;),
        trControl = ctrl)

model_rpart</code></pre>
<pre><code>## CART 
## 
## 35488 samples
##     9 predictor
##     2 classes: &#39;no&#39;, &#39;yes&#39; 
## 
## Pre-processing: centered (39), scaled (39) 
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 31939, 31940, 31940, 31938, 31940, 31940, ... 
## Resampling results across tuning parameters:
## 
##   cp            ROC        Sens       Spec     
##   0.0002254410  0.7418953  0.5806143  0.7904981
##   0.0002536211  0.7433551  0.5725220  0.8003501
##   0.0005072423  0.7462419  0.5532501  0.8225663
## 
## ROC was used to select the optimal model using  the largest value.
## The final value used for the model was cp = 0.0005072423.</code></pre>
<p>We can see that ROCAUC went up to ~ 74% and sensitivity also increased somewhat with the random search cp parameterization which ended up using a value much smaller than grid search. We again visualize the tree to see how it changed.</p>
<pre class="r"><code>plot(model_rpart$finalModel, uniform = TRUE, margin = 0.1)
graphics::text(model_rpart$finalModel, cex = 0.5)</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-91-1.png" width="1344" /> Clearly the model has become more complex which was expected given that its ROCAUC went up. As before, we will predict test outcome with the final model.</p>
<pre class="r"><code>model_rpart_pred &lt;- 
  predict(model_rpart, 
          newdata = test %&gt;% 
            select(model_vars) %&gt;%
            mutate(default = as.factor(ifelse(default == TRUE, 
                                              &quot;yes&quot;, &quot;no&quot;))) %&gt;%
            filter(complete.cases(.)), 
          type = &quot;prob&quot;)

caret::confusionMatrix(
  data = ifelse(model_rpart_pred[, &quot;yes&quot;] &gt; 0.5, &quot;yes&quot;, &quot;no&quot;), 
  reference = as.factor(ifelse(test[complete.cases(test[, model_vars]), 
                                    &quot;default&quot;] == TRUE, &quot;yes&quot;, &quot;no&quot;)),
  positive = &quot;yes&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    no   yes
##        no  91226   682
##        yes 81806  3753
##                                              
##                Accuracy : 0.5352             
##                  95% CI : (0.5329, 0.5375)   
##     No Information Rate : 0.975              
##     P-Value [Acc &gt; NIR] : 1                  
##                                              
##                   Kappa : 0.0377             
##  Mcnemar&#39;s Test P-Value : &lt;0.0000000000000002
##                                              
##             Sensitivity : 0.84622            
##             Specificity : 0.52722            
##          Pos Pred Value : 0.04386            
##          Neg Pred Value : 0.99258            
##              Prevalence : 0.02499            
##          Detection Rate : 0.02115            
##    Detection Prevalence : 0.48211            
##       Balanced Accuracy : 0.68672            
##                                              
##        &#39;Positive&#39; Class : yes                
## </code></pre>
<p>Compute ROC.</p>
<pre class="r"><code>roc_rpart &lt;- 
  pROC::roc(response = temp, 
            predictor = model_rpart_pred[, &quot;yes&quot;])

roc_rpart</code></pre>
<pre><code>## 
## Call:
## roc.default(response = temp, predictor = model_rpart_pred[, &quot;yes&quot;])
## 
## Data: model_rpart_pred[, &quot;yes&quot;] in 173032 controls (temp no) &lt; 4435 cases (temp yes).
## Area under the curve: 0.7463</code></pre>
<p>The ROCAUC is ~74% which is nearly the same as for cross-validated training set. We plot the curve comparing it against the ROC from earlier models.</p>
<pre class="r"><code>pROC::plot.roc(x = roc_glm_1, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
               col = &quot;green&quot;)

pROC::plot.roc(x = roc_glm_2, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
               add = TRUE, col = &quot;blue&quot;)

pROC::plot.roc(x = roc_rpart, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
               add = TRUE, col = &quot;orange&quot;)


legend(x = &quot;bottomright&quot;, legend=c(&quot;glm_1 AUC = 0.664&quot;, &quot;glm_2 AUC = 0.7&quot;,
                                   &quot;rpart AUC = 0.74&quot;), 
       col = c(&quot;green&quot;, &quot;blue&quot;, &quot;orange&quot;), lty = 1, cex = 1.0)</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-94-1.png" width="672" /></p>
</div>
<div id="random-forest-via-randomforest-in-caret" class="section level3">
<h3><span class="header-section-number">12.11.2</span> Random forest via randomForest in caret</h3>
<p>For random forests, the library <a href="https://cran.r-project.org/web/packages/randomForest/index.html">randomForest</a> is used. Tuning parameter is <code>mtry</code> which is the number of variables randomly sampled as candidates at each split. For computationally reasons, we limit the number of trees via <code>caret::trainControl(ntree))</code>, the number of folds via <code>caret::trainControl(number))</code> and the resampling iterations via <code>caret::trainControl(repeats))</code>.</p>
<pre class="r"><code># library(randomForest)

ctrl &lt;- 
  trainControl(method = &quot;repeatedcv&quot;, 
               number = 5,
               repeats = 1,
               classProbs = TRUE,
               summaryFunction = twoClassSummary,
               verboseIter = FALSE,
               allowParallel = TRUE)

model_rf &lt;-
  train_down %&gt;%
  select(model_vars) %&gt;%
  mutate(default = as.factor(ifelse(default == TRUE, &quot;yes&quot;, &quot;no&quot;))) %&gt;%
  filter(complete.cases(.)) %&gt;%
  train(default ~ .,
        data = .,
        method = &#39;rf&#39;,
        ntree = 10,
        metric = &quot;ROC&quot;,
        preProc = c(&quot;center&quot;, &quot;scale&quot;),
        trControl = ctrl)

model_rf</code></pre>
<pre><code>## Random Forest 
## 
## 35488 samples
##     9 predictor
##     2 classes: &#39;no&#39;, &#39;yes&#39; 
## 
## Pre-processing: centered (39), scaled (39) 
## Resampling: Cross-Validated (5 fold, repeated 1 times) 
## Summary of sample sizes: 28391, 28390, 28390, 28391, 28390 
## Resampling results across tuning parameters:
## 
##   mtry  ROC        Sens       Spec     
##    2    0.6877044  0.5636517  0.7134646
##   20    0.7041407  0.6133559  0.6806064
##   39    0.6982071  0.6123415  0.6655027
## 
## ROC was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 20.</code></pre>
<p>We can see that ROCAUC is ~ 70% at mtry = 20. We again visualize but since it is a combination of many trees, the plot will look different.</p>
<pre class="r"><code>plot(model_rf$finalModel)</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-96-1.png" width="672" /></p>
<p>The error would most likely go further down if we had allowed more trees.</p>
<pre class="r"><code>model_rf_pred &lt;- 
  predict(model_rf, 
          newdata = test %&gt;% 
            select(model_vars) %&gt;%
            mutate(default = as.factor(ifelse(default == TRUE, 
                                              &quot;yes&quot;, &quot;no&quot;))) %&gt;%
            filter(complete.cases(.)), 
          type = &quot;prob&quot;)

caret::confusionMatrix(
  data = ifelse(model_rf_pred[, &quot;yes&quot;] &gt; 0.5, &quot;yes&quot;, &quot;no&quot;), 
  reference = as.factor(ifelse(test[complete.cases(test[, model_vars]), 
                                    &quot;default&quot;] == TRUE, &quot;yes&quot;, &quot;no&quot;)),
  positive = &quot;yes&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction     no    yes
##        no  114693   1723
##        yes  58339   2712
##                                              
##                Accuracy : 0.6616             
##                  95% CI : (0.6594, 0.6638)   
##     No Information Rate : 0.975              
##     P-Value [Acc &gt; NIR] : 1                  
##                                              
##                   Kappa : 0.038              
##  Mcnemar&#39;s Test P-Value : &lt;0.0000000000000002
##                                              
##             Sensitivity : 0.61150            
##             Specificity : 0.66284            
##          Pos Pred Value : 0.04442            
##          Neg Pred Value : 0.98520            
##              Prevalence : 0.02499            
##          Detection Rate : 0.01528            
##    Detection Prevalence : 0.34401            
##       Balanced Accuracy : 0.63717            
##                                              
##        &#39;Positive&#39; Class : yes                
## </code></pre>
<p>Compute ROC.</p>
<pre class="r"><code>roc_rf &lt;- 
  pROC::roc(response = temp, 
            predictor = model_rf_pred[, &quot;yes&quot;])

roc_rf</code></pre>
<pre><code>## 
## Call:
## roc.default(response = temp, predictor = model_rf_pred[, &quot;yes&quot;])
## 
## Data: model_rf_pred[, &quot;yes&quot;] in 173032 controls (temp no) &lt; 4435 cases (temp yes).
## Area under the curve: 0.7021</code></pre>
<p>As the ROCAUC is not better than before, we will not plot it against earlier models.</p>
</div>
<div id="stochastic-gradient-boosting-via-gbm-in-caret" class="section level3">
<h3><span class="header-section-number">12.11.3</span> Stochastic Gradient Boosting via gbm in caret</h3>
<p>For Stochastic Gradient Boosting, the library <a href="https://cran.r-project.org/web/packages/gbm/index.html">gbm</a> is used. Tuning parameters are</p>
<ul>
<li>n.trees: total number of trees to fit</li>
<li>interaction.depth: maximum depth of variable interactions</li>
<li>shrinkage: shrinkage parameter applied to each tree in the expansion</li>
<li>n.minobsinnode: minimum number of observations in the trees terminal nodes</li>
</ul>
<p>For some background, see e.g.</p>
<ul>
<li><a href="http://statweb.stanford.edu/~jhf/ftp/stobst.pdf">Stochastic Gradient Boosting, Jerome Friedman</a></li>
<li><a href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/">A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning</a></li>
</ul>
<pre class="r"><code># library(gbm)

ctrl &lt;- 
  trainControl(method = &quot;repeatedcv&quot;, 
               number = 5,
               repeats = 1,
               classProbs = TRUE,
               summaryFunction = twoClassSummary,
               verboseIter = FALSE,
               allowParallel = TRUE)

model_gbm_1 &lt;- 
   train_down %&gt;%
   select(model_vars) %&gt;%
   mutate(default = as.factor(ifelse(default == TRUE, &quot;yes&quot;, &quot;no&quot;))) %&gt;%
   filter(complete.cases(.)) %&gt;%
   train(default ~ ., 
         data = ., 
         method = &quot;gbm&quot;,
         metric = &quot;ROC&quot;,
         trControl = ctrl,
         preProc = c(&quot;center&quot;, &quot;scale&quot;),
         ## This last option is actually one
         ## for gbm() that passes through
         verbose = FALSE)

model_gbm_1</code></pre>
<pre><code>## Stochastic Gradient Boosting 
## 
## 35488 samples
##     9 predictor
##     2 classes: &#39;no&#39;, &#39;yes&#39; 
## 
## Pre-processing: centered (39), scaled (39) 
## Resampling: Cross-Validated (5 fold, repeated 1 times) 
## Summary of sample sizes: 28390, 28391, 28390, 28390, 28391 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  ROC        Sens       Spec     
##   1                   50      0.7218075  0.4989575  0.7923127
##   1                  100      0.7347236  0.5438152  0.7776594
##   1                  150      0.7376056  0.5922232  0.7565241
##   2                   50      0.7351958  0.5467456  0.7754611
##   2                  100      0.7416798  0.6105945  0.7484081
##   2                  150      0.7480446  0.6190476  0.7499302
##   3                   50      0.7387262  0.6076078  0.7507755
##   3                  100      0.7488759  0.6164553  0.7538746
##   3                  150      0.7515752  0.6082840  0.7630048
## 
## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1
## 
## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10
## ROC was used to select the optimal model using  the largest value.
## The final values used for the model were n.trees = 150,
##  interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10.</code></pre>
<p>Plotting the boosting iterations.</p>
<pre class="r"><code>ggplot(model_gbm_1)</code></pre>
<pre><code>## Warning: Ignoring unknown aesthetics: shape</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-100-1.png" width="672" /></p>
<p>Predict.</p>
<pre class="r"><code>model_gbm_1_pred &lt;- 
  predict(model_gbm_1, 
          newdata = test %&gt;% 
            select(model_vars) %&gt;%
            mutate(default = as.factor(ifelse(default == TRUE, 
                                              &quot;yes&quot;, &quot;no&quot;))) %&gt;%
            filter(complete.cases(.)), 
          type = &quot;prob&quot;)

caret::confusionMatrix(
  data = ifelse(model_gbm_1_pred[, &quot;yes&quot;] &gt; 0.5, &quot;yes&quot;, &quot;no&quot;), 
  reference = as.factor(ifelse(test[complete.cases(test[, model_vars]), 
                                    &quot;default&quot;] == TRUE, &quot;yes&quot;, &quot;no&quot;)),
  positive = &quot;yes&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction     no    yes
##        no  104238   1033
##        yes  68794   3402
##                                              
##                Accuracy : 0.6065             
##                  95% CI : (0.6043, 0.6088)   
##     No Information Rate : 0.975              
##     P-Value [Acc &gt; NIR] : 1                  
##                                              
##                   Kappa : 0.0438             
##  Mcnemar&#39;s Test P-Value : &lt;0.0000000000000002
##                                              
##             Sensitivity : 0.76708            
##             Specificity : 0.60242            
##          Pos Pred Value : 0.04712            
##          Neg Pred Value : 0.99019            
##              Prevalence : 0.02499            
##          Detection Rate : 0.01917            
##    Detection Prevalence : 0.40681            
##       Balanced Accuracy : 0.68475            
##                                              
##        &#39;Positive&#39; Class : yes                
## </code></pre>
<pre class="r"><code>roc_gbm_1 &lt;- 
  pROC::roc(response = temp, 
            predictor = model_gbm_1_pred[, &quot;yes&quot;])

roc_gbm_1</code></pre>
<pre><code>## 
## Call:
## roc.default(response = temp, predictor = model_gbm_1_pred[, &quot;yes&quot;])
## 
## Data: model_gbm_1_pred[, &quot;yes&quot;] in 173032 controls (temp no) &lt; 4435 cases (temp yes).
## Area under the curve: 0.7516</code></pre>
<p>The ROCAUC is ~75% which is nearly the same as for cross-validated training set. We plot the curve comparing it against the ROC from the simple model.</p>
<pre class="r"><code>pROC::plot.roc(x = roc_glm_1, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
               col = &quot;green&quot;)

pROC::plot.roc(x = roc_glm_2, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
               add = TRUE, col = &quot;blue&quot;)

pROC::plot.roc(x = roc_rpart, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
               add = TRUE, col = &quot;orange&quot;)

pROC::plot.roc(x = roc_gbm_1, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
               add = TRUE, col = &quot;brown&quot;)

legend(x = &quot;bottomright&quot;, legend=c(&quot;glm_1 AUC = 0.664&quot;, &quot;glm_2 AUC = 0.7&quot;,
                                   &quot;rpart AUC = 0.74&quot;, &quot;gbm AUC = 0.753&quot;), 
       col = c(&quot;green&quot;, &quot;blue&quot;, &quot;orange&quot;, &quot;brown&quot;), lty = 1, cex = 1.0)</code></pre>
<p><img src="/project/2017-10-23-lending-club-loan-data-in-r_files/figure-html/unnamed-chunk-103-1.png" width="672" /></p>
<p></p>
</div>
</div>
</div>
<div id="further-resources" class="section level1">
<h1><span class="header-section-number">13</span> Further Resources</h1>
<ul>
<li><a href="https://www.analyticsvidhya.com/blog/2017/08/skilltest-logistic-regression/">30 Questions to test your understanding of Logistic Regression</a></li>
<li><a href="http://dpmartin42.github.io/blogposts/r/imbalanced-classes-part-1">Handling Class Imbalance with R and Caret - An Introduction</a></li>
<li><a href="https://stats.stackexchange.com/questions/136040/stepwise-model-selection-in-logistic-regression-in-r">Stepwise Model Selection in Logistic Regression in R</a></li>
<li><a href="https://topepo.github.io/caret/index.html">The caret Package</a></li>
<li><a href="https://amunategui.github.io/binary-outcome-modeling/">Modeling 101 - Predicting Binary Outcomes with R, gbm, glmnet, and {caret}</a></li>
<li><a href="https://shiring.github.io/machine_learning/2017/01/15/rfe_ga_post">Feature Selection in Machine Learning (Breast Cancer Datasets)</a></li>
<li><a href="https://shiring.github.io/machine_learning/2017/03/31/webinar_code">Building meaningful machine learning models for disease prediction</a></li>
<li><a href="https://rpubs.com/ryankelly/reg">Linear Model Selection &amp; Regularization</a></li>
<li><a href="http://www.lendingmemo.com/lending-club-prosper-default-rates/">Default Rates at Lending Club &amp; Prosper: When Loans Go Bad</a></li>
<li><a href="http://nbviewer.jupyter.org/gist/odubno/0b767a47f75adb382246">Loan Data (2007-2011) From Lending Club</a></li>
<li><a href="https://rstudio-pubs-static.s3.amazonaws.com/115829_32417d32dbce41eab3eeaf608a0eef9d.html">JFdarre Project 1: Lending Clubâs data</a></li>
<li><a href="https://rstudio-pubs-static.s3.amazonaws.com/203258_d20c1a34bc094151a0a1e4f4180c5f6f.html">Predict LendingClubâs Loan Data</a></li>
<li><a href="http://eriqande.github.io/rep-res-web/lectures/making-maps-with-R.html">Making Maps with R</a></li>
<li><a href="https://shuhelicopter.github.io/Data_Exploratory_Analysis_of_LC.html">Exploratory Data Analysis of Lending Club Issued Loans</a></li>
<li><a href="http://cs229.stanford.edu/proj2015/199_report.pdf">Predicting Default Risk of Lending Club Loans</a></li>
</ul>
</div>

    </div>

  </div>
</article>

<div class="container">
  <nav>
  <ul class="pager">
    

    
  </ul>
</nav>

</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017 Alexander Wagner &middot; 

      Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic
      theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>

    <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js"></script>
    <script src="/js/jquery-1.12.3.min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/isotope.pkgd.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.1/imagesloaded.pkgd.min.js"></script>
    <script src="/js/hugo-academic.js"></script>
    

    
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-104080060-1', 'auto');
        ga('send', 'pageview');

         
        var links = document.querySelectorAll('a');
        Array.prototype.map.call(links, function(item) {
            if (item.host != document.location.host) {
                item.addEventListener('click', function() {
                    var action = item.getAttribute('data-action') || 'follow';
                    ga('send', 'event', 'outbound', action, item.href);
                });
            }
        });
    </script>
    

    
    
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js"></script>

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    

  </body>
</html>

