<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pyspark on suspiciously datalicious</title>
    <link>https://triamus.github.io/tags/pyspark/</link>
    <description>Recent content in Pyspark on suspiciously datalicious</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Alexander Wagner</copyright>
    <lastBuildDate>Fri, 22 Sep 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/pyspark/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Install Spark on Windows</title>
      <link>https://triamus.github.io/post/install-spark-on-windows/</link>
      <pubDate>Fri, 22 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://triamus.github.io/post/install-spark-on-windows/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#Overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#System requirements&#34;&gt;System requirements&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#A general note of caution for Bash on Windows&#34;&gt;A general note of caution for Bash on Windows&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Install Java&#34;&gt;Install Java&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Install Python&#34;&gt;Install Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Example output for system&#34;&gt;Example output for system&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Install Scala&#34;&gt;Install Scala&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Setting environment variables&#34;&gt;Setting environment variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Download/clone windows utilities for corresponding Hadoop version&#34;&gt;Download/clone windows utilities for corresponding Hadoop version&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Make /tmp/hive writable&#34;&gt;Make /tmp/hive writable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Run Spark shell&#34;&gt;Run Spark shell&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#Run PySpark&#34;&gt;Run PySpark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;This is a quick tutorial how to install Spark on Windows via the pre-built on Apache Hadoop. It is assumed that the reader has basic knowledge of the Windows command line (CMD) and, if Bash should be used, a basic familiarity with Bash on Ubuntu on Windows (part of Linux Subsystem on Windows). We will demonstrate a basic example how to use Scala and Python (via PySpark) from the command line.&lt;/p&gt;

&lt;h2 id=&#34;system-requirements&#34;&gt;System requirements&lt;/h2&gt;

&lt;p&gt;We show how to install Spark on Windows 10 (see exact system info below) but generally it should also work on Windows 7. You will require&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Windows 10 (prior versions, i.e. Windows &amp;gt;= 7, may work but not tested)&lt;/li&gt;
&lt;li&gt;Bash on Ubuntu on Windows (Linux subsystem on Windows) in case Bash should be used (not required per se)&lt;/li&gt;
&lt;li&gt;Java runtime environment (JRE) or Java Development Kit (JDK) version &amp;gt;= 8&lt;/li&gt;
&lt;li&gt;Scala&lt;/li&gt;
&lt;li&gt;Python (in case you want to use PySpark)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;a-general-note-of-caution-for-bash-on-windows&#34;&gt;A general note of caution for Bash on Windows&lt;/h3&gt;

&lt;p&gt;If you plan to set up Bash on Windows to work with Spark etc. you will eventually edit configuration files of the Linux on Windows subsystem. It is &lt;strong&gt;very important&lt;/strong&gt; that you do not use any Windows apps/tools to perform any operation on Linux files as these files may get corrupted. This includes using any Windows editor to edit Linux files (e.g. Notepad/++). For more details, see &lt;a href=&#34;https://blogs.msdn.microsoft.com/commandline/2016/11/17/do-not-change-linux-files-using-windows-apps-and-tools/&#34; target=&#34;_blank&#34;&gt;Do not change Linux files using Windows apps and tools&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;install-java&#34;&gt;Install Java&lt;/h3&gt;

&lt;p&gt;Binary installers for Java can be downloaded from &lt;a href=&#34;http://www.oracle.com/technetwork/java/javase/downloads/index.html&#34; target=&#34;_blank&#34;&gt;Oracle Java SE&lt;/a&gt;. To run Spark, you only require a Java runtime environment (JRE) but you may also download the Java development kit (JDK) which includes the JRE. Note that the path to the JRE may be different when using a JDK.&lt;/p&gt;

&lt;p&gt;You can check which version of Java you are running on the CMD via&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;java -version
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In case you like to use Bash, you will need to install Java on the Linux subsystem for Windows separately as it is a distinct OS. You can do so by following instructions from @rs6g10 on &lt;a href=&#34;https://github.com/Microsoft/BashOnWindows/issues/49&#34; target=&#34;_blank&#34;&gt;Microsoft/BashOnWindows/Unable to install Java JDK or Runtime #49&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;install-python&#34;&gt;Install Python&lt;/h3&gt;

&lt;p&gt;If you like to use the Spark Python API via PySpark, you will need to install Python. A frequently recommended distribution is often that of &lt;a href=&#34;https://www.anaconda.com/download/&#34; target=&#34;_blank&#34;&gt;Anaconda&lt;/a&gt; as it ships with a lot of useful libraries and has binary installers for Windows. Note that Bash on Windows already comes with Python 2.7 pre-installed so in case you like to work with Python3, you will have to install it using standard Bash workflow.&lt;/p&gt;

&lt;h3 id=&#34;example-output-for-system&#34;&gt;Example output for system&lt;/h3&gt;

&lt;p&gt;We show how our system is set up (your&amp;rsquo;s will probably vary). Type below commands in CMD (in succession)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systeminfo | findstr /B /C:&amp;quot;OS Name&amp;quot; /C:&amp;quot;OS Version&amp;quot;
java -version
python --version
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://triamus.github.io/post/img/2017-09-22-install-spark-on-windows/cmd_system_and_version_info.PNG&#34; alt=&#34;CMD System Info&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can do the same for Bash (note the default version of Python)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://triamus.github.io/post/img/2017-09-22-install-spark-on-windows/bash_system_and_version_info.PNG&#34; alt=&#34;Bash System Info&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;install-scala&#34;&gt;Install Scala&lt;/h2&gt;

&lt;p&gt;Generally, you can find Scala downloads &lt;a href=&#34;https://scala-lang.org/download/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. Find the Windows installer (in my case clicking on scala-2.12.3.msi under section Archive) downloads &lt;a href=&#34;https://downloads.lightbend.com/scala/2.12.3/scala-2.12.3.msi&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt;. Follow installation wizard.&lt;/p&gt;

&lt;p&gt;Once the installation is finished, open a new CMD and type&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Your output should look like this&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://triamus.github.io/post/img/2017-09-22-install-spark-on-windows/cmd_scala_check_after_install.PNG&#34; alt=&#34;CMD Scala Check&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;install-spark&#34;&gt;Install Spark&lt;/h2&gt;

&lt;p&gt;Find the latest stable version of Spark on &lt;a href=&#34;http://spark.apache.org/downloads.html&#34; target=&#34;_blank&#34;&gt;Apache Spark Downloads&lt;/a&gt; and use pre-built on Hadoop and direct download.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://triamus.github.io/post/img/2017-09-22-install-spark-on-windows/download_spark.PNG&#34; alt=&#34;Spark Download Page&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Copy/save the .tgz file to/at the location where you like Spark to reside and extract it from some Linux shell (e.g. Bash, Cygwin) via below command (your file name may vary). Alternatively, you can use some extraction software such as 7zip.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# in case tar program is not installed, install e.g. via sudo apt-get install tar
tar xzfv spark-2.2.0-bin-hadoop2.7.tgz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://triamus.github.io/post/img/2017-09-22-install-spark-on-windows/bash_extract_spark_tgz.PNG&#34; alt=&#34;Bash extract Spark&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After some time, your Spark install (extraction) should be done and a couple of folders being created at your chosen location.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://triamus.github.io/post/img/2017-09-22-install-spark-on-windows/spark_directory.PNG&#34; alt=&#34;Spark Directory&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;setting-environment-variables&#34;&gt;Setting environment variables&lt;/h2&gt;

&lt;p&gt;In order to make the system aware of all programs, we will be setting some environment variables. This can be done by simply using Windows search function in start menu and typing something like &lt;code&gt;environment variables&lt;/code&gt; and clicking the program &lt;code&gt;Edit the system environment variables&lt;/code&gt;. We will set the following variables (your location may vary)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;JAVA_HOME: C:\Program Files\Java\jre1.8.0_144&lt;/li&gt;
&lt;li&gt;SPARK_HOME: D:\spark\spark-2.2.0-bin-hadoop2.7&lt;/li&gt;
&lt;li&gt;HADOOP_HOME: D:\spark\spark-2.2.0-bin-hadoop2.7\bin&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Moreover, we will expand the system path with&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;C:\Program Files\Java\jre1.8.0_144\bin&lt;/li&gt;
&lt;li&gt;D:\spark\spark-2.2.0-bin-hadoop2.7\bin&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that the system environment variables are not necessarily shared with the Linux subsystem for Windows (although PATH seems to be shared). Any environment variable would thus have to be set in respective Linux configuration file.&lt;/p&gt;

&lt;p&gt;Your Windows environment variables may look something like this now&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://triamus.github.io/post/img/2017-09-22-install-spark-on-windows/windows_environment_variables.PNG&#34; alt=&#34;Windows Environment Variables&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;download-clone-windows-utilities-for-corresponding-hadoop-version&#34;&gt;Download/clone windows utilities for corresponding Hadoop version&lt;/h2&gt;

&lt;p&gt;Go to &lt;a href=&#34;https://github.com/steveloughran/winutils&#34; target=&#34;_blank&#34;&gt;Github/steveloughran/winutils&lt;/a&gt; and clone/copy the Windows utilities for your Hadoop version to your local Hadoop directory. Copy \winutils-master\winutils-master\hadoop-2.7.1\bin and paste into your Spark install. For us this would be D:\spark\spark-2.2.0-bin-hadoop2.7\bin.&lt;/p&gt;

&lt;h2 id=&#34;make-tmp-hive-writable&#34;&gt;Make /tmp/hive writable&lt;/h2&gt;

&lt;p&gt;To avoid a common error (see e.g. &lt;a href=&#34;https://stackoverflow.com/questions/34196302/the-root-scratch-dir-tmp-hive-on-hdfs-should-be-writable-current-permissions&#34; target=&#34;_blank&#34;&gt;The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rw-rw-rw- (on Windows)&lt;/a&gt; or  &lt;a href=&#34;https://hernandezpaul.wordpress.com/2016/01/24/apache-spark-installation-on-windows-10/&#34; target=&#34;_blank&#34;&gt;Apache Spark installation on Windows 10&lt;/a&gt; open CMD as administrator and run below against your Spark directory where Winutils should reside (so in our case in D:\spark\spark-2.2.0-bin-hadoop2.7\bin\bin)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;D:\&amp;gt;spark\spark-2.2.0-bin-hadoop2.7\bin\bin\winutils.exe chmod 777 \tmp\hive
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;run-spark-shell&#34;&gt;Run Spark shell&lt;/h2&gt;

&lt;p&gt;Finally, we can test our Spark installation by running the shell&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;spark-shell
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Your output should look something like below&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://triamus.github.io/post/img/2017-09-22-install-spark-on-windows/cmd_spark-shell.PNG&#34; alt=&#34;CMD Initial Spark Shell&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We execute a minimal example from &lt;a href=&#34;http://spark.apache.org/docs/latest/quick-start.html&#34; target=&#34;_blank&#34;&gt;Apache Spark Quick Start&lt;/a&gt; (note that the README file is located one directory up) to test basic functionality.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val textFile = spark.read.textFile(&amp;quot;../README.md&amp;quot;)
textFile.count() // Number of items in this Dataset
textFile.first() // First item in this Dataset
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://triamus.github.io/post/img/2017-09-22-install-spark-on-windows/cmd_scala_initial_test.PNG&#34; alt=&#34;CMD Scala Initial Test&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Since the Linux subsystem for Windows inherits the Path variables from Windows, we can run the same without further adjustments on Bash on Ubuntu on Windows&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://triamus.github.io/post/img/2017-09-22-install-spark-on-windows/bash_spark-shell.PNG&#34; alt=&#34;Bash Spark Shell&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;run-pyspark&#34;&gt;Run PySpark&lt;/h2&gt;

&lt;p&gt;Assuming you also want to use Python with Spark via &lt;a href=&#34;http://spark.apache.org/docs/2.1.0/api/python/pyspark.html&#34; target=&#34;_blank&#34;&gt;PySpark&lt;/a&gt; and have followed above setup instructions, you can open a new CMD and type below to test if PySpark works&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pyspark
# we show it with bin/pyspark but both will work if your environment variables are properly set
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again, we execute a minimal example from &lt;a href=&#34;we execute a minimal example from https://spark.apache.org/examples.html&#34; target=&#34;_blank&#34;&gt;Apache Spark Examples&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text_file = sc.textFile(&amp;quot;README.md&amp;quot;)
counts = text_file.flatMap(lambda line: line.split(&amp;quot; &amp;quot;)) \
             .map(lambda word: (word, 1)) \
             .reduceByKey(lambda a, b: a + b)
print(counts)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://triamus.github.io/post/img/2017-09-22-install-spark-on-windows/cmd_pyspark_initial_test.PNG&#34; alt=&#34;CMD PySpark Initial Test&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As before, the same command works in Bash but note the default Python version used&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://triamus.github.io/post/img/2017-09-22-install-spark-on-windows/bash_pyspark_initial_test.PNG&#34; alt=&#34;Bash PySpark Initial Test&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In case you want to work with a different Python version, adjust respective Bash configuration file to export the environment variable (see line below). In our case, we use Python3.4 and define it in the &lt;code&gt;.profile&lt;/code&gt; file in the Bash home firectory (&lt;code&gt;~&lt;/code&gt;).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export PYSPARK_PYTHON=python3.4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://triamus.github.io/post/img/2017-09-22-install-spark-on-windows/bash_nano_profile.PNG&#34; alt=&#34;Bash nano profile&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The PySpark shell now runs Python 3.4 as default (this obviously needs to be adjusted in case you like to run a different Python version).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://triamus.github.io/post/img/2017-09-22-install-spark-on-windows/bash_pyspark_python3.PNG&#34; alt=&#34;bash_pyspark_python3&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
